{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Blog","text":"<p> <p>Release Notes Support Me</p> <p></p>"},{"location":"about/","title":"About","text":"<p>Hello, my name is SAITEJA IRRINKI, and I work as a Senior DevOps Engineer in Build &amp; Release. I'm experienced in Infrastructure Provisioning, Configuration Management, Version Control, Code Quality, Environment Administration, Defect Tracking, Release Management, Continuous Integration, and Continuous Delivery, Cloud Computing, Scripting for Automation, Static Web Development, Orchestration-Technologies, and Operating Systems- Linux and Windows.</p> <p> +91 9493322788</p> <p> saitejairrinki91@gmail.com</p> <p> https://www.instagram.com/saitejairrinki/ </p> <p> https://saitejairrinki.medium.com/</p>"},{"location":"awsdevops/","title":"DevOps","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization\u2019s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p> <p></p>"},{"location":"blog/","title":"My DevOps Blog","text":"<p>Welcome to my DevOps Blog. Here you will find a collection of articles and tutorials on various DevOps topics. </p> <p>Whether you are just getting started with DevOps or a pro, I hope you find something useful here. </p> <p>This blog is all about my thoughts, experiences, Challanges and lessons learned in the world of DevOps.</p> <p>I have been working in the IT industry and have been involved in a variety of roles including InfraStructure Provisioning, system administration and operations. In recent years I have become very interested in the DevOps movement and how it can help organisations to improve the way they deliver software.</p> <p>I hope you enjoy reading my blog and please feel free to leave any comments or questions</p> <p> saitejairrinki91@gmail.com</p> <p> +91 9493322788</p> <p>The majority of the content in this blog is created by me through research and conducting POC, while only a small number of items are sourced from the internet, such as definitions,icons,pictures and etc used for reference.</p> <p>Thanks for reading!</p>"},{"location":"fund/","title":"Fuel the Continuous Learning Journey!","text":"<p>\ud83d\ude80 Support Our DevOps Blog - Fueling Insights, One Post at a Time! \ud83d\udee0\ufe0f </p> <p>Hey there, fellow DevOps enthusiasts,</p> <p>I hope our DevOps blog has been a helpful resource on our journey to mastering all things DevOps. Your support can take this learning adventure even further!</p> <p>Why Support?</p> <p>Maintaining a blog takes dedication, time, and resources. As much as I'm fueled by my passion for DevOps, a little support goes a long way in enabling us to continue producing high-quality, well-researched, and practical content. Your contributions will help cover expenses like hosting, domain registration, and the tools necessary to experiment with the latest DevOps technologies. With your support, we can dedicate more time to writing, testing new concepts, and diving deeper into the ever-evolving world of DevOps.</p> <p>How Can You Help?</p> <p>Contribution: A small contribution can go a long way in keeping the blog alive and thriving. </p> <p>Spread the Word: Share the blog with friends and colleagues who share our passion.</p> <p>Your support means the world and keeps the DevOps knowledge flowing. Let's continue this journey together!</p> <p>\ud83d\udd17 Support Now via this UPI link on mobile.</p> <p>Cheers,  </p> <p>SAITEJA IRRINKI </p>"},{"location":"job_roles/","title":"DevOps ENGINEER ROLES","text":""},{"location":"job_roles/#aws-tasks","title":"AWS Tasks:","text":"<p>Managing Ec2 Instances, EIP, Network Interfaces, Security Groups &amp; Key Pairs  Managing EBS Volumes, AMI &amp; Snapshots (Backup &amp; Restore, Migration, etc.) Setup &amp; Managing Elastic Load Balancers, ACM &amp; Autoscaling Groups Setting &amp; Managing Cloudwatch Alarms on metrics from Ec2, ELB &amp; RDS  Creating &amp; Managing RDS Instances, RDS Snapshots, Updating Parameters Groups  AWS CLI for any AWS Tasks</p>"},{"location":"job_roles/#cloud-migration-using-lift-shift-strategy-on-aws-services-using","title":"Cloud Migration using Lift &amp; Shift Strategy on AWS, Services using","text":"<ul> <li> <p>VPC, Ec2, S3, Application Load Balancer, Route53 </p> </li> <li> <p>IAM to give secure access to AWS account using MFA</p> </li> <li> <p>Tightly controlled Security Group for firewall rules of EC2</p> </li> <li> <p>EBS volume for storage on Ec2, Snapshot for Backups of EBS</p> </li> <li> <p>Autoscaling for Automatic scaling of Ec2 instance based on CPU usage</p> </li> <li> <p>Modernization on AWS Cloud, Services used</p> </li> <li> <p>Beanstalk for PAAS for Tomcat Web App</p> </li> <li> <p>RDS for MySQL Database</p> </li> <li> <p>Object storage S3 to store and retrieve files</p> </li> <li> <p>Route53 for Private &amp; Public Hosted zones/Records</p> </li> <li> <p>Amazon MQ for fully managed RabbitMQ</p> </li> <li> <p>ElastiCache for in-memory datastore in cloud</p> </li> <li> <p>Monitoring with CloudWatch, Grafana</p> </li> <li> <p>Notification using SNS</p> </li> </ul>"},{"location":"job_roles/#aws-cloud-automation-using","title":"AWS Cloud Automation using","text":"<ul> <li>Terraform</li> <li>Ansible</li> <li>CloudFormation (Stacks)</li> </ul>"},{"location":"job_roles/#securities","title":"Securities","text":"<ul> <li>IAM management</li> <li>Active Directory</li> </ul>"},{"location":"job_roles/#azure-azure-devops","title":"Azure &amp; Azure DevOps","text":"<ul> <li>Automated the provisioning of Azure resources (App Registration, Blob, Back-up Center, Cosmos DB, Key Vault, Azure VMs) using PowerShell and Templates.</li> <li>Created a Build/Release pipeline with automated build and Continuous-Integration using Azure DevOps</li> </ul>"},{"location":"job_roles/#continuous-delivery-of-java-web-application","title":"Continuous Delivery of Java Web Application","text":"<ul> <li> <p>CICD Pipeline using Jenkins, Git, Maven, Nexus, S3 &amp; SonarQube</p> </li> <li> <p>Deploying Artifact to Beanstack</p> </li> <li> <p>Jenkins Pipeline As A Code for CICD</p> </li> <li> <p>Website Automation through Jenkins whenever there is a GIT Commit</p> </li> </ul>"},{"location":"job_roles/#configuration-management-using-ansible","title":"Configuration Management using Ansible","text":"<ul> <li>Ansible AdHoc commands to execute remote tasks</li> <li>Ansible playbook for Service/Server Deployments</li> <li>Ansible playbook to setup VPC &amp; Bastion Host on AWS</li> <li>Writing our own configuration file (ansible.cfg)</li> <li>Ansible Roles for modular &amp; reusable automation framework</li> </ul>"},{"location":"job_roles/#docker-containers","title":"Docker Containers","text":"<ul> <li>Building customized docker images using Dockerfile</li> <li>Docker-compose to define &amp; run MultiContainer Docker Application</li> </ul>"},{"location":"job_roles/#kubernetes","title":"Kubernetes","text":"<p>Experience in Kubernetes and Helm for Container Orchestration</p> <ul> <li>Creating production-grade Kubernetes clusters using Kops and Kubeadm.</li> <li>Hosting containerized applications on Kubernetes clusters using Pods, Services, Replication Controllers, Deployments, Secrets, and ConfigMaps.</li> <li>Implementing Horizontal Pod Autoscaling (HPA) to ensure application reliability and performance.</li> <li>Utilizing Kubernetes Probes (liveness and readiness) for monitoring and maintaining application health.</li> <li>Managing Kubernetes DaemonSets to ensure that a copy of a pod runs on all (or some) nodes in the cluster.</li> <li>Using <code>kubectl logs</code> with labels and selectors for efficient log management and troubleshooting.</li> <li>Integrating Metric Server for resource usage monitoring and scaling decisions.</li> <li>Packaging Kubernetes applications as Helm charts for simplified deployment and version management.</li> </ul>"},{"location":"job_roles/#mkdocs","title":"MkDocs","text":"<ul> <li>Designing and Building a Static Website using MkDocs (The Current Website you're watching) </li> </ul>"},{"location":"profile_summary/","title":"PROFESSIONAL SUMMARY","text":"<p>Experienced Senior DevOps Engineer with over 4 years in CI/CD pipeline development, automating infrastructure, container management, and cloud platform integration. Microsoft-certified DevOps Engineer (AZ-400) with expertise in tools such as Terraform, Kubernetes, Helm, and Azure DevOps. Recognized for enhancing deployment processes, reducing downtime, and efficiently managing cloud resources, achieving up to an 80% improvement in deployment efficiency.</p>"},{"location":"profile_summary/#academic-details","title":"ACADEMIC DETAILS","text":"<p>B.Tech - 2020</p>"},{"location":"profile_summary/#professional-experience","title":"PROFESSIONAL EXPERIENCE","text":"<p>DevOps Engineer November 2020 - Present Responsible for implementing and maintaining CI/CD pipelines, automating infrastructure provisioning, and deploying applications across cloud platforms. Specialize in managing containerized environments and optimizing deployment processes.</p>"},{"location":"profile_summary/#achievements","title":"ACHIEVEMENTS","text":"<ul> <li>Regularly received written email appreciations from stakeholders, clients, and managers for consistently delivering high-quality results.</li> <li>Awarded a performance bonus by a client for the timely delivery of a project and significant cost reduction in a major initiative.</li> </ul>"},{"location":"project/","title":"Projects","text":""},{"location":"project/#automating-code-deployment-for-app-services-and-azure-data-factory-adf","title":"Automating Code Deployment for App Services and Azure Data Factory (ADF)","text":"<p>Sep 2024 - Present Associated with Tiger Analytics</p> <p>The project involves deploying code changes to Azure App Services and Azure Data Factory (ADF) using a multistage build pipeline with approvals and quality checks leveraging Git Flow Branching Strategy. By implementing robust CI/CD pipelines, 80% of manual tasks previously handled by developers have been automated, resulting in streamlined processes and error-free deployments. This approach enhances efficiency, minimizes human error, and ensures consistent delivery across environments.  </p> <p>Skills: Azure DevOps \u00b7 Microsoft Azure \u00b7 GitFlow  </p>"},{"location":"project/#one-time-setup-for-application-deployment-on-gke-using-github-actions-and-helm","title":"One-Time Setup for Application Deployment on GKE using GitHub Actions and Helm","text":"<p>Jul 2024 - Sep 2024 Associated with Tiger Analytics</p> <p>This project involved a one-time requirement from the client to deploy applications on Google Kubernetes Engine (GKE) within Google Cloud Platform (GCP). GitHub Actions was used for CI/CD with multistage pipelines to automate the build and deployment process. Google Artifact Repository was utilized for artifact storage, and Helm charts were developed to manage environment-specific configurations for Development and Production, ensuring streamlined and consistent deployments across multiple environments. After completing the automation, the setup was handed over to the development team for ongoing management and maintenance.  </p> <p>Skills: Helm \u00b7 Google Kubernetes Engine (GKE) \u00b7 Google Cloud Platform (GCP) \u00b7 GitHub Actions  </p>"},{"location":"project/#efficient-deployment-and-scaling-of-microservices-in-aks-with-helm-and-azure-devops","title":"Efficient Deployment and Scaling of Microservices in AKS with Helm and Azure DevOps","text":"<p>Oct 2023 - Jun 2024 Associated with Tiger Analytics</p> <p>This project demonstrates the successful deployment of a robust microservices application in Azure Kubernetes Service (AKS) using Helm charts and Azure DevOps pipelines. Key accomplishments include developing a multistage build pipeline for creating and pushing Docker images to Azure Container Registry (ACR) and implementing Azure File Share to facilitate real-time data synchronization across pods. Innovative optimizations, such as eliminating redundant sidecar containers by integrating Azure File Share directly, reduced resource consumption and streamlined operations. Application stability was enhanced with readiness and liveness probes, while enabling Horizontal Pod Autoscaling (HPA) improved scalability and reduced idle resource usage by 20%, ensuring dynamic response to workload demands.  </p> <p>Skills: Azure Kubernetes Service (AKS) \u00b7 Helm \u00b7 Azure DevOps \u00b7 Microsoft Azure \u00b7 Linux \u00b7 Docker \u00b7 Kubernetes  </p>"},{"location":"project/#product-deployment-and-automation","title":"Product Deployment and Automation","text":"<p>Aug 2022 - Sep 2023 Associated with Zelar</p> <p>This project involved deploying a product-based application for a pharmaceutical company, where medical data is visualized through Power BI dashboards. The DevOps team automated the entire deployment pipeline using various tools, ensuring seamless and secure code management across different environments. When a pull request (PR) is raised, SonarCloud runs a quality scan to ensure the code meets standards. With necessary approvals, the code is pushed through development, UAT, and production environments, each secured with approval gates and quality checks. The code is then built into a Docker image, pushed to Azure Container Registry (ACR), and deployed to Azure Kubernetes Service (AKS) using Helm charts for efficient and scalable application management.  </p> <p>Skills: Azure Kubernetes Service (AKS) \u00b7 ACR \u00b7 Azure DevOps \u00b7 Sonarqube \u00b7 Powershell \u00b7 Microsoft Azure  </p>"},{"location":"project/#infrastructure-provisioning-using-terraform","title":"Infrastructure Provisioning Using Terraform","text":"<p>Mar 2022 - Aug 2022 Associated with Zelar</p> <p>In this project, I created infrastructure for multiple environments in AWS Cloud and automated the entire process using Jenkins freestyle pipelines. This made it easier to deploy and manage resources across different environments, like development, staging, and production. The automation ensured faster and more consistent deployments with less manual work and fewer errors.  </p> <p>Skills: Terraform \u00b7 Linux \u00b7 Amazon Web Services (AWS) \u00b7 AWS Command Line Interface (CLI) \u00b7 Jenkins  </p>"},{"location":"project/#techbricks-edu-appwebsite","title":"Techbricks Edu App/Website","text":"<p>Mar 2021 - Feb 2022 Associated with VisualpathIT</p> <p>Managed CI/CD pipelines integrating Git, Nexus, and SonarQube, decreasing build failures by 30%. Built Docker images for Kubernetes deployments, optimizing resource usage by 25%. Resolved deployment issues efficiently, ensuring 99.9% application availability.  </p> <p>Skills: Amazon Web Services (AWS) \u00b7 Sonarqube \u00b7 Jenkins \u00b7 Git \u00b7 Docker \u00b7 Kubernetes \u00b7 Amazon S3  </p>"},{"location":"release_notes/","title":"Release Notes \ud83d\udcda","text":"<p>As an individual content provider, I'm excited to share that you can discover the latest updates and changes right here in these release notes. I'm dedicated to improving and refining the content regularly to offer you the best learning experience possible. Keep an eye on this space for weekly updates and enhancements. Your commitment to learning is appreciated, and I look forward to continually enhancing our journey!</p>"},{"location":"release_notes/#feedback","title":"Feedback \ud83d\udd8b\ufe0f","text":"<p>Your input matters to me! Please take a moment to share your thoughts and suggestions with me. Your feedback helps us continually improve my content and services. Thank you for being a part of my journey! </p>"},{"location":"release_notes/#release-date-28-04-2025","title":"Release Date: 28-04-2025 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>Getting Started with Azure DevOps Pipelines: From Hello World to Real-World Deployments. </li> </ul>"},{"location":"release_notes/#release-date-09-10-2024","title":"Release Date: 09-10-2024 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>Mastering Terraform Import: Bringing Existing Infrastructure Under Terraform Control</li> </ul>"},{"location":"release_notes/#release-date-30-03-2024","title":"Release Date: 30-03-2024 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>Boosting Efficiency: Simplifying Workflows with Reusable Pipeline Templates on Azure DevOps! \ud83d\udee0\ufe0f</li> <li>GitHub Folder: Easy Azure DevOps Pipelines</li> </ul>"},{"location":"release_notes/#release-date-01-03-2024","title":"Release Date: 01-03-2024 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>In this Helm repository, I have added Horizontal Pod Autoscaling (HPA) and Kubernetes probes to ensure high availability and improved reliability.</li> </ul> <p>Updated Content:</p> <ul> <li>EKS Configuration Revised with Extra Commands Included</li> </ul>"},{"location":"release_notes/#release-date-09-02-2024","title":"Release Date: 09-02-2024 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>Helm Delve into the latest Helm features to enhance the efficiency of Kubernetes deployments, complete with a sample project to help you get started.</li> </ul>"},{"location":"release_notes/#release-date-19-09-2023","title":"Release Date: 19-09-2023 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>Docker Compose files added for different scenarios with various examples. Explore these new additions to streamline your Docker container orchestration.</li> </ul> <p>Updated Content:</p> <ul> <li>Dive into the enhanced Docker experience with updated Docker commands and Docker files. We've included code-level comments and additional content to help you master Docker.</li> </ul>"},{"location":"release_notes/#release-date-29-09-2023","title":"Release Date: 29-09-2023 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>We're excited to introduce a new addition to our knowledge base: Kubernetes Architecture Concepts. Dive deep into the inner workings of Kubernetes and gain a comprehensive understanding of its architecture.</li> </ul> <p>Updated Content:</p> <ul> <li>Our Kubernetes Introduction has been revamped with improved definitions and enriched examples. We believe this enhancement will provide you with a more insightful learning experience.</li> </ul>"},{"location":"release_notes/#release-date-05-10-2023","title":"Release Date: 05-10-2023 \ud83d\udcc5","text":"<p>New Content:</p> <ul> <li>Added a Simple guide on the Terraform Workspace concept on GitHub. Check it out to get a better understanding of how it works.</li> </ul> <p>Updated Content:</p> <ul> <li>Improved the Terraform and Docker files by adding titles for the code blocks, making it easier to find what you need.</li> </ul>"},{"location":"resume/","title":"Resume","text":"<p>To View My Resume please click here  Resume</p>"},{"location":"technical_skills/","title":"TECHNICAL SKILLS","text":"Category Tools Technologies &amp; Softwares Operating Systems - Linux   Windows  Virtualization - Vagrant Cloud Platform - AWS    AZURE   GCP  Scripting Languages - Shell Scripting  PowerShell  Configuration Management Tool - Ansible  Infrastructure as Code - Terraform   Cloud Formation  Version Control Tool - GIT  Build Software - Maven Build &amp; Release/CICD - Azure DevOps  Jenkins  Containerization Tools - Docker   Kubernetes  Helm  Web/App Server - Apache Tomcat  Ticketing Tool - Freshdesk  Static Web Development - MkDocs Repository Manager - Nexus Repository Monitoring - CloudWatch  Grafana Code Quality Analysis - SonarQube/Cloud"},{"location":"CICD/00.CICD_Introduction/","title":"Continuous Integration and Continuous Deployment (CI/CD)","text":""},{"location":"CICD/00.CICD_Introduction/#what-is-cicd","title":"What is CI/CD","text":"<p>CI/CD, which stands for Continuous Integration and Continuous Deployment, is a set of best practices and automation techniques used in software development to streamline the process of building, testing, and deploying applications. CI/CD helps development teams deliver high-quality software more rapidly and reliably by automating repetitive tasks, reducing manual errors, and promoting collaboration among team members.</p> <p>\ud83d\udd27 Key Components of CI/CD: - Continuous Integration (CI): The practice of automatically integrating code changes into a shared repository multiple times a day, ensuring that new code does not break the existing codebase. - Continuous Deployment (CD): The process of automatically deploying code changes to production or staging environments after passing automated tests.</p>"},{"location":"CICD/00.CICD_Introduction/#difference-between-agile-and-cicd","title":"Difference Between Agile and CI/CD","text":"<p>While Agile and CI/CD share some similarities, they address different aspects of the software development lifecycle (SDLC):</p> <ul> <li> <p>Agile: Agile is a software development methodology that emphasizes iterative development, customer collaboration, and responding to change. It focuses on delivering small, incremental updates to software based on customer feedback. Agile provides a framework for project management and team collaboration.</p> </li> <li> <p>CI/CD: CI/CD, on the other hand, is a set of practices and automation tools that focus on the technical aspects of software development. It aims to automate and accelerate the processes of building, testing, and deploying code changes. CI/CD complements Agile by ensuring that code changes are continuously integrated and deployed in a reliable and automated manner.</p> </li> </ul>"},{"location":"CICD/00.CICD_Introduction/#famous-cicd-tools","title":"Famous CI/CD Tools","text":"<p>There are several popular CI/CD tools available, each with its own strengths and features:</p> <ol> <li> <p>Jenkins \ud83c\udfd7\ufe0f: An open-source automation server with a vast plugin ecosystem, Jenkins is known for its flexibility and extensibility, making it a popular choice for CI/CD pipelines.</p> </li> <li> <p>GitHub Actions :octocat:: GitHub's built-in CI/CD solution, tightly integrated with repositories, allows you to automate workflows and build, test, and deploy code changes directly from your GitHub repositories.</p> </li> <li> <p>Azure DevOps : Microsoft's comprehensive DevOps platform offers a complete CI/CD solution, including pipelines, repositories, and project management tools.</p> </li> <li> <p>Travis CI \ud83d\ude80: A cloud-based CI/CD service that integrates seamlessly with GitHub repositories, making it easy to set up and manage CI/CD pipelines.</p> </li> <li> <p>CircleCI \ud83d\udd04: A cloud-based CI/CD platform that provides fast and flexible CI/CD configuration, allowing teams to build, test, and deploy code efficiently.</p> </li> </ol>"},{"location":"CICD/00.CICD_Introduction/#how-cicd-makes-sdlc-smooth","title":"How CI/CD Makes SDLC Smooth","text":"<p>CI/CD plays a crucial role in making the Software Development Lifecycle (SDLC) smoother by addressing common challenges and delivering multiple benefits:</p> <ol> <li> <p>Faster Development Cycles \u23e9: CI/CD automates the build and test process, reducing the time required to identify and fix issues, resulting in faster development cycles.</p> </li> <li> <p>Reduced Manual Errors : Automation minimizes human error in repetitive tasks, leading to higher software quality and reliability.</p> </li> <li> <p>Continuous Feedback Loop : CI/CD encourages a culture of continuous improvement by providing rapid feedback on code changes, enabling teams to iterate and enhance their software continually.</p> </li> <li> <p>Enhanced Collaboration : CI/CD promotes collaboration among development, testing, and operations teams, breaking down silos and fostering a DevOps culture.</p> </li> <li> <p>Efficient Scaling : As projects grow, CI/CD scales effortlessly, handling increased complexity and workload without a proportional increase in effort.</p> </li> </ol>"},{"location":"CICD/00.CICD_Introduction/#real-time-examples-and-use-cases","title":"Real-Time Examples and Use Cases","text":""},{"location":"CICD/00.CICD_Introduction/#1-e-commerce-website-deployment","title":"1. E-commerce Website Deployment","text":"<p>Use Case: An e-commerce company regularly updates its website to add new products and features. Using CI/CD, they automate the testing and deployment process, ensuring that any changes are thoroughly tested and deployed to production with minimal manual intervention.</p>"},{"location":"CICD/00.CICD_Introduction/#2-mobile-app-development","title":"2. Mobile App Development","text":"<p>Use Case: A mobile app development team uses CI/CD to streamline the build and release process. Whenever a developer commits code changes to the repository, the CI/CD pipeline automatically builds the app, runs tests, and deploys it to app stores, allowing for rapid and reliable updates.</p>"},{"location":"CICD/00.CICD_Introduction/#conclusion","title":"Conclusion","text":"<p>CI/CD is a crucial practice in modern software development that accelerates the delivery of high-quality software, enhances collaboration, and ensures a smooth and efficient Software Development Lifecycle. By automating key processes, CI/CD tools empower development teams to focus on innovation and delivering value to their users.</p>"},{"location":"CICD/01.CICD_with_GitHub/","title":"Continuous Integration and Continuous Deployment (CI/CD) with GitHub Actions","text":"<p>GitHub Actions is a powerful automation and CI/CD platform integrated with GitHub repositories. It allows you to build, test, and deploy your code directly from your GitHub repository.</p>"},{"location":"CICD/01.CICD_with_GitHub/#components-of-github-actions","title":"Components of GitHub Actions:","text":"<ul> <li>Workflows \ud83d\udd04</li> <li>Jobs \ud83d\udee0\ufe0f</li> <li>Runners \ud83c\udfc3</li> <li>Actions \ud83e\udd16</li> <li>Artifacts \ud83d\udce6</li> </ul>"},{"location":"CICD/01.CICD_with_GitHub/#real-time-use-case","title":"Real-Time Use Case:","text":"<p>GitHub Actions is best suited for developers and teams using GitHub for version control. It seamlessly integrates with your repositories and is an excellent choice for projects hosted on GitHub.</p>"},{"location":"CICD/01.CICD_with_GitHub/#mostly-used-for","title":"Mostly Used For:","text":"<ul> <li>Continuous Integration (CI)</li> <li>Continuous Deployment (CD)</li> <li>Automated Testing</li> <li>Workflow Automation</li> </ul>"},{"location":"CICD/01.CICD_with_GitHub/#understanding-github-actions-workflows","title":"Understanding GitHub Actions Workflows","text":"<p>Workflows in GitHub Actions are defined YAML files that specify the automation process for your project. A workflow can include one or more jobs, each consisting of a series of steps and actions to execute.</p>"},{"location":"CICD/01.CICD_with_GitHub/#github-actions-runner-types","title":"GitHub Actions Runner Types","text":"<p>GitHub Actions allows you to use different types of runners to execute your workflows:</p> <ul> <li>GitHub-hosted runners are provided by GitHub and offer a variety of pre-configured environments.</li> <li>Self-hosted runners are runners you can set up and maintain in your own infrastructure, giving you more control over the execution environment.</li> </ul>"},{"location":"CICD/01.CICD_with_GitHub/#installation-process","title":"Installation Process:","text":"<ol> <li>Access your GitHub repository.</li> <li>Navigate to the \"Actions\" tab.</li> <li>Create a new workflow or choose from existing templates.</li> <li>Define your workflow steps, including build and deployment tasks.</li> <li>Save your workflow configuration in a YAML file.</li> <li>Trigger your workflow manually or automatically based on events like code pushes or pull requests.</li> </ol>"},{"location":"CICD/01.CICD_with_GitHub/#creating-a-basic-github-actions-workflow","title":"Creating a Basic GitHub Actions Workflow","text":""},{"location":"CICD/01.CICD_with_GitHub/#yaml-method","title":"YAML Method:","text":"<pre><code>name: CI/CD with GitHub Actions\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout Repository\n      uses: actions/checkout@v2\n\n    - name: Print Hello World\n      run: echo 'Hello, World!'\n</code></pre> <p>To create a basic GitHub Actions workflow using YAML configuration:</p> <ol> <li>Access your GitHub repository.</li> <li>Navigate to the \"Actions\" tab.</li> <li>Click on \"Set up a workflow yourself\" to create a new YAML file.</li> <li>Copy and paste the provided YAML configuration.</li> <li>Save the file as <code>.github/workflows/main.yml</code>.</li> <li>Trigger your workflow manually or by pushing code changes.</li> </ol> <p>This workflow will execute and print \"Hello, World!\" when triggered by a code push.</p>"},{"location":"CICD/01.CICD_with_GitHub/#github-actions-marketplace-and-custom-actions","title":"GitHub Actions Marketplace and Custom Actions","text":"<p>The GitHub Actions ecosystem includes a marketplace of pre-built actions that you can use to extend your workflows. You can also create custom actions tailored to your specific automation needs.</p>"},{"location":"CICD/01.CICD_with_GitHub/#real-time-use-case-for-github-actions","title":"Real-Time Use Case for GitHub Actions","text":"<p>Use Case: A software development team manages their project on GitHub and uses GitHub Actions for CI/CD. Whenever a developer pushes code changes to the repository, GitHub Actions automatically triggers a workflow that builds the application, runs tests, and deploys it to a staging environment for testing. Once testing is successful, another workflow deploys the application to production.</p> <p>By leveraging GitHub Actions, the team achieves automated testing and deployment, reduces manual intervention, and ensures consistent and reliable releases.</p> <p>GitHub Actions seamlessly integrates with their GitHub repository, making it a natural choice for their CI/CD needs.</p>"},{"location":"CICD/01.CICD_with_GitHub/#tool-comparisons","title":"Tool Comparisons:","text":"<p>GitHub Actions is an excellent choice for projects hosted on GitHub, providing tight integration with the platform. It offers a wide range of pre-configured environments and actions, making it easy to get started with CI/CD. However, it may have limited flexibility compared to Jenkins and Azure DevOps for highly customized automation tasks.</p>"},{"location":"CICD/02.CICD_with_Jenkins/","title":"Continuous Integration and Continuous Deployment (CI/CD) with Jenkins","text":"<p>Jenkins is a widely used open-source automation server that supports building, deploying, and automating projects. It is known for its extensibility through plugins and is a popular choice for CI/CD pipelines.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#components-of-jenkins","title":"Components of Jenkins:","text":"<ul> <li>Jobs \ud83d\udee0\ufe0f</li> <li>Builds \ud83d\ude80</li> <li>Plugins \ud83e\udde9</li> <li>Nodes \ud83d\udda5\ufe0f</li> <li>Artifacts \ud83d\udcc2</li> </ul>"},{"location":"CICD/02.CICD_with_Jenkins/#real-time-use-case","title":"Real-Time Use Case:","text":"<p>Jenkins is versatile and can be used for various scenarios, from building and testing code to deploying applications. It's an excellent choice for organizations with diverse automation needs.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#mostly-used-for","title":"Mostly Used For:","text":"<ul> <li>Continuous Integration (CI)</li> <li>Continuous Deployment (CD)</li> <li>Automated Testing</li> <li>Scheduled Jobs and Tasks</li> </ul>"},{"location":"CICD/02.CICD_with_Jenkins/#installation-process","title":"Installation Process:","text":"<ol> <li>Download and install Jenkins on your server or machine.</li> <li>Access the Jenkins web interface in your browser.</li> <li>Install necessary plugins to extend Jenkins' functionality.</li> <li>Create Jenkins jobs or pipelines to define your CI/CD processes.</li> <li>Configure job triggers, source code management, and build steps.</li> <li>Save and run your Jenkins jobs.</li> </ol>"},{"location":"CICD/02.CICD_with_Jenkins/#understanding-jenkins-artifacts-and-its-use-cases","title":"Understanding Jenkins Artifacts and Its Use Cases","text":""},{"location":"CICD/02.CICD_with_Jenkins/#what-are-jenkins-artifacts","title":"What are Jenkins Artifacts","text":"<p>Jenkins Artifacts are files or outputs generated during the build and deployment processes. These artifacts can include compiled binaries, reports, logs, or any other files produced by Jenkins jobs. They are essential for tracking and sharing build results.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#pipeline-artifacts-vs-jenkins-artifacts-differences-and-best-practices","title":"Pipeline Artifacts vs Jenkins Artifacts: Differences and Best Practices","text":"<p>When working with Jenkins, it's essential to understand the difference between Pipeline Artifacts and Jenkins Artifacts and when to use each effectively.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#pipeline-artifacts","title":"Pipeline Artifacts","text":"<p>Pipeline Artifacts refer to files generated during the execution of a Jenkins pipeline. They can include build artifacts, test results, and any other files produced as part of the pipeline's workflow. Pipeline Artifacts are often temporary and used for a specific job or task.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#jenkins-artifacts","title":"Jenkins Artifacts","text":"<p>Jenkins Artifacts, on the other hand, encompass a broader range of files and outputs generated by Jenkins jobs, not just limited to pipelines. They can include build artifacts, archived logs, test reports, and more. Jenkins Artifacts are typically stored for historical reference and can be accessed across different jobs and builds.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#when-to-use-each","title":"When to Use Each","text":"<ul> <li> <p>Pipeline Artifacts: Utilize Pipeline Artifacts when you need to store and manage files generated during the execution of a specific Jenkins pipeline. These artifacts are often temporary and are useful for debugging or further processing within the same pipeline.</p> </li> <li> <p>Jenkins Artifacts: Opt for Jenkins Artifacts when you want to retain a broader set of files and outputs generated by Jenkins jobs, not limited to pipelines. These artifacts serve as a historical record and can be accessed and shared across different jobs and builds.</p> </li> </ul>"},{"location":"CICD/02.CICD_with_Jenkins/#creating-and-managing-pipelines-in-jenkins","title":"Creating and Managing Pipelines in Jenkins","text":"<p>Jenkins provides robust support for creating and managing pipelines. You can define pipelines using the built-in pipeline DSL, scripted pipelines, or declarative pipelines.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#freestyle-projects","title":"Freestyle Projects","text":"<p>Freestyle Projects offer a graphical interface for creating Jenkins jobs and pipelines. They are suitable for simple automation tasks and require minimal scripting.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#creating-1st-basic-pipeline-to-run-hello-world","title":"Creating 1<sup>st</sup> Basic Pipeline to Run Hello World:","text":"<ol> <li>Open Jenkins and navigate to the \"New Item\" menu.</li> <li>Select \"Pipeline\" and provide a name for your project.</li> <li>Configure the pipeline using a Jenkinsfile or the visual editor.</li> <li>Define stages, steps, and agent configurations.</li> <li>Save your pipeline configuration.</li> <li>Trigger your pipeline manually or by code commits.</li> </ol>"},{"location":"CICD/02.CICD_with_Jenkins/#declarative-pipelines","title":"Declarative Pipelines","text":"<p>Declarative Pipelines provide a structured way to define pipelines using a simplified, human-readable syntax. They are a good choice for straightforward CI/CD workflows.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#creating-a-hello-world-declarative-pipeline-using-groovy","title":"Creating a \"Hello, World!\" Declarative Pipeline using Groovy","text":"<ol> <li>In your Jenkins dashboard, click on \"New Item\" to create a new project.</li> <li>Enter a name for your project, select \"Pipeline,\" and click \"OK.\"</li> <li>In the pipeline configuration, scroll down to the \"Pipeline\" section.</li> <li>Select \"Pipeline script\" from the \"Definition\" dropdown.</li> <li>In the \"Pipeline script\" box, enter the following Groovy script to print \"Hello, World!\":    <pre><code>pipeline {\n    agent any\n    stages {\n        stage('Hello') {\n            steps {\n                echo 'Hello, World!'\n            }\n        }\n    }\n}\n</code></pre></li> <li>Save your project configuration.</li> </ol> <p>Now, when you build this project, it will execute the \"Hello, World!\" script defined in the Groovy pipeline.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#scripted-pipelines","title":"Scripted Pipelines","text":"<ul> <li>Scripted Pipelines offer full flexibility and power through Groovy scripting. They are ideal for complex, customized automation tasks.</li> </ul>"},{"location":"CICD/02.CICD_with_Jenkins/#real-time-use-case-for-jenkins-pipelines","title":"Real-Time Use Case for Jenkins Pipelines","text":"<p>Use Case: Consider a software development team that uses Jenkins for CI/CD. They have a declarative pipeline that builds, tests, and deploys their application to multiple environments automatically. Jenkins pipelines help the team ensure consistent, automated deployments, reducing manual errors and speeding up release cycles.</p> <p>By incorporating Jenkins pipelines, the team streamlines their development process and ensures reliable, repeatable deployments.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#jenkins-plugin-integration","title":"Jenkins Plugin Integration:","text":"<p>Jenkins supports a vast ecosystem of plugins that extend its functionality. You can integrate Jenkins with various tools and services using plugins, making it highly adaptable to your specific requirements.</p>"},{"location":"CICD/02.CICD_with_Jenkins/#setting-up-jenkins-agents","title":"Setting Up Jenkins Agents","text":"<p>Jenkins allows you to set up agents for executing build and deployment jobs. You can choose between Jenkins-managed agents and agents on your own infrastructure.</p> <ul> <li> <p>Jenkins-managed agents are hosted by Jenkins and are suitable for most scenarios. They provide a wide range of pre-installed tools and environments.</p> </li> <li> <p>Self-hosted agents are agents that you set up and maintain in your own infrastructure. This option offers more control over the agent environment.</p> </li> </ul>"},{"location":"CICD/02.CICD_with_Jenkins/#tool-comparisons","title":"Tool Comparisons:","text":"<p>Jenkins is a highly customizable and extensible automation server, making it a popular choice for organizations with diverse CI/CD needs. It offers a wide range of plugins and integrations, but it may require more initial setup and configuration compared to Azure DevOps.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/","title":"Continuous Integration and Continuous Deployment (CI/CD) with Azure DevOps","text":"<p>Azure DevOps is a comprehensive cloud-based platform by Microsoft that offers end-to-end CI/CD capabilities integrated with Azure services.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#components-for-azure-devops","title":"Components for Azure DevOps:","text":"<ul> <li>Boards \ud83d\udcca</li> <li>Repositories \ud83d\udcc2</li> <li>Pipelines \ud83c\udf10</li> <li>Artifacts \ud83d\udce6</li> <li>Test Plans \ud83e\uddea</li> </ul>"},{"location":"CICD/03.CICD_with_AzureDevOps/#real-time-use-case","title":"Real-Time Use Case:","text":"<p>Azure DevOps is best suited for enterprises using Microsoft technologies and Azure cloud services, as it seamlessly integrates with these tools.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#mostly-used-for","title":"Mostly Used For:","text":"<ul> <li>Full-Stack Application Development</li> <li>Automated Testing and Deployment</li> <li>Project Management and Collaboration</li> </ul>"},{"location":"CICD/03.CICD_with_AzureDevOps/#microsoft-hosted-vs-self-hosted","title":"Microsoft Hosted Vs Self Hosted","text":"<p>Azure DevOps allows you to choose between Microsoft-hosted and self-hosted agents for your CI/CD pipelines. </p> <ul> <li> <p>Microsoft-hosted agents are provided and maintained by Microsoft. They are suitable for most scenarios and offer a wide variety of pre-installed tools and environments.</p> </li> <li> <p>Self-hosted agents are agents that you set up and maintain in your own infrastructure. This option provides more flexibility and control, allowing you to use specific tools and environments tailored to your needs.</p> </li> </ul>"},{"location":"CICD/03.CICD_with_AzureDevOps/#understanding-azure-artifacts-and-its-use-cases","title":"Understanding Azure Artifacts and Its Use Cases","text":""},{"location":"CICD/03.CICD_with_AzureDevOps/#what-are-azure-artifacts","title":"What are Azure Artifacts","text":"<p>Azure Artifacts is a vital component of Azure DevOps services that facilitates the storage, publication, and sharing of various package types, such as NuGet, npm, Maven, and more. It serves as a centralized repository for managing and distributing packages used in software development projects.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#pipeline-artifacts-vs-azure-artifacts-differences-and-best-practices","title":"Pipeline Artifacts vs Azure Artifacts: Differences and Best Practices","text":"<p>When working with Azure DevOps, it's essential to distinguish between Pipeline Artifacts and Azure Artifacts and understand when to leverage each of them effectively.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#pipeline-artifacts","title":"Pipeline Artifacts","text":"<p>Pipeline Artifacts are primarily focused on compiling and packaging source code into artifacts. These pipelines are typically triggered on code commits and serve purposes like compilation, unit testing, and artifact creation. They are best suited for scenarios where you need to create and store build-specific artifacts generated during the build process.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#azure-artifacts","title":"Azure Artifacts","text":"<p>Azure Artifacts, in contrast, offer a more comprehensive package management solution. They are designed to store and manage packages and dependencies used in your software development projects. Azure Artifacts support various package types like NuGet, npm, Maven, and more. They are ideal for managing libraries, dependencies, and third-party packages that your projects depend on.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#when-to-use-each","title":"When to Use Each","text":"<ul> <li> <p>Pipeline Artifacts: Utilize Pipeline Artifacts when you need to create and store build-specific artifacts like compiled binaries, libraries, or any output generated during the build process. These artifacts are often specific to a particular build and are used for deployment or further testing.</p> </li> <li> <p>Azure Artifacts: Opt for Azure Artifacts when you need to manage packages and dependencies across multiple projects or teams. They provide a centralized repository for storing and sharing packages, streamlining consistency and version control across your organization's software development efforts.</p> </li> </ul>"},{"location":"CICD/03.CICD_with_AzureDevOps/#build-and-release-pipelines-in-azure-devops","title":"Build and Release Pipelines in Azure DevOps","text":"<p>Azure DevOps offers two distinct types of pipelines: Build Pipelines and Release Pipelines.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#build-pipelines","title":"Build Pipelines","text":"<p>Build Pipelines are dedicated to compiling and packaging source code into artifacts. They are typically triggered on code commits and are employed for tasks such as compilation, unit testing, and artifact creation.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#release-pipelines","title":"Release Pipelines","text":"<p>Release Pipelines take charge of the deployment and release of applications. They utilize artifacts created by build pipelines and deploy them to various environments, such as development, testing, and production.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#real-time-use-case-for-build-and-release-pipelines","title":"Real-Time Use Case for Build and Release Pipelines","text":"<p>Use Case: Consider an e-commerce company that develops a web application. They rely on Azure DevOps Build Pipelines to compile their code, run automated tests, and create deployable artifacts. Once the artifacts are ready, Release Pipelines come into play, deploying the application to multiple environments. This ensures that each deployment is consistent and automated.</p> <p>By strategically incorporating Build and Release Pipelines in Azure DevOps, the company streamlines their development process, minimizes manual errors, and ensures that their application is continuously delivered and updated with minimal human intervention.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#installation-process","title":"Installation Process:","text":"<ol> <li>Sign in to Azure DevOps or create an account.</li> <li>Create a new project within Azure DevOps.</li> <li>Set up repositories and define your code version control strategy.</li> <li>Configure CI/CD pipelines using YAML or the visual designer.</li> <li>Link your pipelines to source code repositories.</li> <li>Trigger your pipelines manually or automatically on code changes.</li> </ol>"},{"location":"CICD/03.CICD_with_AzureDevOps/#creating-a-basic-pipeline-to-run-hello-world","title":"Creating a Basic Pipeline to Run Hello World","text":""},{"location":"CICD/03.CICD_with_AzureDevOps/#classic-visual-designer-method","title":"Classic (Visual Designer) Method:","text":"<ol> <li>In your Azure DevOps project, navigate to \"Pipelines.\"</li> <li>Click on \"New Pipeline\" to create a new pipeline.</li> <li>Connect to your source code repository.</li> <li>Choose the \"Classic Editor\" option to use the visual designer.</li> <li>Follow these steps to create a simple \"Hello, World!\" pipeline:</li> </ol> <p>a. Agent Pool: Select an agent pool (e.g., \"Azure Pipelines\").</p> <p>b. Agent Specification: Choose an agent specification (e.g., \"ubuntu-latest\").</p> <p>c. Tasks: Click on the \"+\" icon to add a new task.</p> <p>d. Agent Job: Select the \"Agent job\" task from the list.</p> <p>e. Display Name: Set the display name to \"Print Hello World.\"</p> <p>f. Script: In the script box, enter the command to print \"Hello, World!\" (e.g., <code>echo 'Hello, World!'</code>).</p> <p></p> <ol> <li>Save and run your pipeline.</li> </ol>"},{"location":"CICD/03.CICD_with_AzureDevOps/#yaml-method","title":"YAML Method:","text":"<pre><code>trigger:\n- main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nsteps:\n- script: echo 'Hello, World!'\n  displayName: 'Print Hello World'\n</code></pre> <p>To create a pipeline using YAML configuration:</p> <ol> <li>In your Azure DevOps project, navigate to \"Pipelines.\"</li> <li>Click on \"New Pipeline\" to create a new pipeline.</li> <li>Connect to your source code repository.</li> <li>Choose the \"YAML\" option to define your pipeline in code.</li> <li>Copy and paste the provided YAML configuration.</li> <li>Save and run your pipeline.</li> </ol> <p>Both methods will create a basic pipeline that prints \"Hello, World!\" when executed. You can choose the method that best suits your preference and workflow.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#how-to-integrate-other-cloud-or-tools-using-service-connections","title":"How to Integrate Other Cloud or Tools Using Service Connections","text":"<p>Azure DevOps allows you to integrate with other cloud services and tools through service connections. Examples of service connections include connecting to AWS for cloud deployment or integrating with Jira for issue tracking.</p> <p>To set up service connections: 1. In your Azure DevOps project, go to \"Project Settings.\" 2. Under \"Service connections,\" create a new connection. 3. Select the type of service or tool you want to connect to. 4. Follow the setup instructions provided for the specific service or tool.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#what-are-variable-groups","title":"What Are Variable Groups","text":"<p>Variable groups in Azure DevOps allow you to define and manage sets of variables that you can use across multiple pipelines. This helps maintain consistency and simplifies configuration management.</p> <p>Example: You can create a variable group for database connection strings and use it in multiple pipelines to ensure uniform database settings.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#what-are-task-groups","title":"What Are Task Groups","text":"<p>Task groups in Azure DevOps allow you to encapsulate a sequence of tasks into a single reusable entity. This simplifies the management of complex pipelines by promoting reusability and maintaining consistency.</p> <p>Example: You can create a task group for deploying microservices and reuse it across different deployment pipelines for various microservices in your application.</p>"},{"location":"CICD/03.CICD_with_AzureDevOps/#tool-comparisons","title":"Tool Comparisons:","text":"<p>Azure DevOps provides a seamless integration of CI/CD with Azure services, making it an excellent choice for organizations invested in the Microsoft ecosystem. It offers robust project management features but may have a steeper learning curve compared to GitHub Actions or Jenkins.</p>"},{"location":"azure/aks_deployment/","title":"Implementing Effective CI/CD Pipeline using \ud83c\udf29\ufe0f Azure DevOps, \ud83d\udc33 Docker, \u2638\ufe0f Kubernetes, and \ud83d\udee0\ufe0f Terraform","text":"<p>The project employs a robust Continuous Integration and Continuous Deployment (CI/CD) approach utilizing Azure DevOps, Docker, Kubernetes, SonarCloud, Terraform, and PowerShell. The key Steps of the project implementation are outlined below:</p> <p></p>"},{"location":"azure/aks_deployment/#step-1-infrastructure-as-code-iaac-with-azure-devops","title":"Step 1: Infrastructure as Code (IAAC) with Azure DevOps","text":"<p>An Azure-based build pipeline is initiated to create the requisite infrastructure, including storage accounts, Azure Key Vault, and virtual machines. This process ensures efficient and scalable resource provisioning.</p> <p>Example</p> <p>Terraform, ARM, Bicep</p>"},{"location":"azure/aks_deployment/#step-2-installation-quality-check-pipeline","title":"Step 2: Installation Quality Check Pipeline","text":"<p>Following the successful execution of the infrastructure build, an installation quality pipeline is activated to assess the provisioning state and pre-configurations of the Azure resources. This validation Step ensures the accurate completion of the installation process.</p> <p>Example</p> <p>PowerShell using Azure CLI</p>"},{"location":"azure/aks_deployment/#step-3-implementation-of-gitflow-branching-strategy","title":"Step 3: Implementation of GitFlow Branching Strategy","text":"<p>The project adheres to the GitFlow branching strategy, which includes development, staging, QA, pre-production, and production branches. Multiple feature branches are created and merged based on successful pull requests, ensuring an organized and efficient development workflow. For example, \ud83c\udf3f feature/new-feature-branch is created from \ud83c\udf3f development for implementing specific features.</p>"},{"location":"azure/aks_deployment/#step-4-configuring-separate-aks-clusters-for-different-environments","title":"Step 4: Configuring Separate AKS Clusters for Different Environments","text":"<p>Distinct AKS clusters are set up for deploying code changes across various environments, including development, staging, pre-production, and production. These clusters are configured in subsequent release pipelines for seamless deployment management.</p>"},{"location":"azure/aks_deployment/#step-5-docker-image-building-process","title":"Step 5: Docker Image Building Process","text":"<p>The build pipeline automatically triggers upon code changes, testing the code, and converting it into Docker images. These images are then pushed to the Docker repository, facilitating streamlined code deployment and management. For instance, \ud83d\udc33 Docker images like \"myapp:latest\" are generated from the code changes.</p> <p>Example</p> <p>DockerHub &amp; ACR</p>"},{"location":"azure/aks_deployment/#step-6-streamlined-release-pipelines","title":"Step 6: Streamlined Release Pipelines","text":"<p>Upon successful Docker image publication, the release pipeline initiates four distinct stages: development, QA, staging, and production. Each stage allows for comprehensive testing and verification before moving on to the next deployment phase.</p>"},{"location":"azure/aks_deployment/#step-7-dev-deployment-triggering","title":"Step 7: Dev Deployment Triggering","text":"<p>The Dev release pipeline is triggered post-Docker image build, deploying the image into the designated Dev Kubernetes AKS cluster. For subsequent environments, deployment waits for QA verification and testing before further progression.</p>"},{"location":"azure/aks_deployment/#step-8-deployment-in-higher-environments","title":"Step 8: Deployment in Higher Environments","text":"<p>Following thorough quality assurance checks, including SonarCloud Quality Gates, the staging deployment is approved, triggering the deployment process in the staging environment. Code merge from development to staging is facilitated with proper approvals and resolution of any pertinent comments.</p>"},{"location":"azure/aks_deployment/#step-9-replicating-the-workflow-across-environments","title":"Step 9: Replicating the Workflow Across Environments","text":"<p>With staging serving as a stable environment, the project ensures the smooth deployment of pre-production and production environments, adhering to the established approval procedures. This uniform approach guarantees consistency and reliability across all deployment stages.</p> <p></p> <p>The seamless integration of \ud83c\udf29\ufe0f Azure DevOps, \ud83d\udc33 Docker, \u2638\ufe0f Kubernetes, \ud83d\udd0d SonarCloud, \ud83d\udee0\ufe0f Terraform, and \ud83d\udcbb PowerShell in the CI/CD pipeline ensures the efficient management and deployment of code changes, allowing for a smooth transition across development, testing, and production environments. \ud83d\ude80</p>"},{"location":"azure/branchingstrategy/","title":"Branching strategies","text":"<p>Azure DevOps supports several branching strategies that teams can use to manage source code and collaborate on development. Here are some of the most common branching strategies:</p> <p>Mainline/Branching: In this strategy, all developers work on a single branch, usually called \"master\" or \"main\". Developers make changes directly to this branch, and all changes are integrated and tested continuously. This strategy is best for small teams or small projects where the development cycle is short.</p> <p>Feature Branching: In this strategy, each new feature is developed on a separate branch, which is later merged into the main branch when the feature is complete. This strategy allows multiple features to be developed simultaneously without impacting the main branch. It is a common strategy for large projects with multiple teams working on different features.</p> <p>Release Branching: In this strategy, a separate branch is created for each release. Development work is done on the main branch, and when it's time for a release, a new branch is created from the main branch. Any bug fixes or changes are made on this branch and then merged back into the main branch after the release is completed. This strategy is best for projects with frequent releases.</p> <p>Gitflow: This is a variation of the feature branching strategy. It involves two long-lived branches: \"master\" and \"develop\", and multiple short-lived branches for feature development and hotfixes. Features are developed on separate branches and then merged into the \"develop\" branch for integration testing. Once the feature is complete, it is merged into the \"master\" branch for release.</p> <p></p> <p>Note</p> <p>These are just a few of the branching strategies that can be used with Azure DevOps. The best strategy for a team depends on the project requirements, team size, and development cycle. It is essential to choose a strategy that aligns with the team's workflow and ensures smooth collaboration and code management.</p>"},{"location":"azure/comparison/","title":"Azure DevOps vs Jenkins","text":"<p>Azure DevOps and Jenkins are two popular tools for continuous integration and delivery (CI/CD) that help teams to automate the software development and delivery process. </p>"},{"location":"azure/comparison/#here-are-some-key-differences-between-azure-devops-and-jenkins","title":"Here are some key differences between Azure DevOps and Jenkins:","text":"<p>Hosted vs. Self-hosted: Azure DevOps is a cloud-based service, while Jenkins is a self-hosted solution that can be installed on-premises or on a virtual machine. This means that Azure DevOps offers the convenience of not having to worry about maintaining servers, while Jenkins offers more control over the environment.</p> <p>Integration with Microsoft Tools: Azure DevOps is tightly integrated with Microsoft's development tools, including Visual Studio and Azure. This integration allows for seamless collaboration and provides a more cohesive solution for teams that use Microsoft's tools. Jenkins, on the other hand, has a wider range of integrations with various tools and platforms.</p> <p>Ease of use: Azure DevOps provides a user-friendly interface that is easy to navigate and use, making it an excellent option for small to medium-sized teams. Jenkins, however, can be more complex and requires more technical knowledge to set up and configure.</p> <p>Security: Azure DevOps provides enterprise-grade security and compliance features, such as multi-factor authentication, encryption, and compliance with industry standards. Jenkins, on the other hand, requires additional plugins and configurations to provide the same level of security.</p> <p>Cost: Azure DevOps offers a range of pricing options, including a free tier for small teams. Jenkins, on the other hand, is open-source and free to use, but may require additional costs for hosting and maintenance.</p> <p>Conclusion</p> <p>Overall, Azure DevOps is a more integrated and user-friendly solution that is better suited for small to medium-sized teams or those using Microsoft tools. Jenkins, on the other hand, offers more flexibility and control for larger teams and organizations that require a more customized CI/CD solution.</p>"},{"location":"azure/installation/","title":"A Step-by-Step Guide to Installing Azure DevOps \ud83d\udce5","text":"<p>Are you ready to dive into the world of Azure DevOps and streamline your software development projects? Azure DevOps is a powerful set of tools and services designed to facilitate collaboration, automation, and continuous delivery in your development workflow. In this guide, we'll walk you through the installation process, ensuring you're set up for success right from the beginning. \ud83d\udee0\ufe0f</p>"},{"location":"azure/installation/#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<p>Before we begin, ensure that you have the following prerequisites in place:</p> <ul> <li>Microsoft Account \ud83d\udc64: You'll need a Microsoft account to access Azure DevOps. If you don't have one, you can create it here.</li> </ul> <p>Now, let's get started with the installation.</p>"},{"location":"azure/installation/#1-azure-devops-account-setup","title":"1. Azure DevOps Account Setup \ud83c\udfd7\ufe0f","text":"Account Creation <p>Setting up your first project in Azure DevOps</p> <p>The first step is to set up your Azure DevOps account. Follow these steps:</p> <ul> <li>Visit Azure DevOps and sign in with your Microsoft account.</li> <li>Click on the \"New Project\" button to create your first project.</li> <li>Give your project a name and choose the appropriate visibility settings (public or private). You can also select a template based on your project type.</li> <li>Click \"Create\" to set up your project.</li> </ul>"},{"location":"azure/installation/#2-run-a-basic-hello-world-pipeline","title":"2. Run a Basic Hello World Pipeline \ud83d\ude80","text":"First Pipeline <p>creating and running your first pipeline in Azure DevOps</p> <p>With your project set up, it's time to create and run your first pipeline. A pipeline automates your software build, test, and deployment processes. Here's how to set up a basic \"Hello World\" pipeline:</p> <ul> <li>In your Azure DevOps project, navigate to the \"Pipelines\" section.</li> <li>Click on the \"New Pipeline\" button.</li> <li>Follow the guided steps to configure your pipeline. You'll define your source code repository (e.g., GitHub, Azure Repos), choose a template (e.g., Starter pipeline), and define the build and deployment steps.</li> <li>Save and run your pipeline.</li> </ul> <p>Enable/Disable Classic Pipelines</p> <p>If you're unable to locate the classic pipeline creation option, it suggests that classic pipeline creation has been turned off in either your organization or project settings. To activate this capability, please refer to the provided Document link</p>"},{"location":"azure/installation/#additional-tips","title":"Additional Tips \ud83d\udcda","text":"<ul> <li> <p>Integration \ud83d\udd04: Azure DevOps integrates seamlessly with popular development tools, version control systems, and cloud platforms. Explore the integrations that best suit your project's needs.</p> </li> <li> <p>Extensions \ud83e\udde9: Azure DevOps offers a marketplace of extensions that can enhance your workflow. Browse and install extensions that can automate tasks, add reporting capabilities, and more.</p> </li> <li> <p>Security \ud83d\udd10: Ensure that your Azure DevOps environment is secure by setting up proper access controls, authentication, and continuous security monitoring.</p> </li> <li> <p>Documentation \ud83d\udcd6: Azure DevOps provides extensive documentation and tutorials. Don't hesitate to consult the official documentation for in-depth information on specific features and best practices.</p> </li> </ul> <p>By following these steps and utilizing the resources available, you'll be well on your way to harnessing the power of Azure DevOps for efficient and collaborative software development. Good luck with your projects! \ud83c\udf1f</p>"},{"location":"azure/intro/","title":"Introduction","text":"<p>Azure DevOps is a comprehensive set of development tools that provides an integrated and collaborative environment for software development teams. It offers a suite of services that includes version control, agile project management, continuous integration and delivery, testing and release management capabilities.</p> <p>Azure DevOps was previously known as Visual Studio Team Services (VSTS) and provides cloud-hosted services that can be accessed from anywhere with an internet connection. It is a cloud-based solution that allows teams to collaborate on projects, track progress, manage code repositories, build and test applications, and deploy applications to production.</p>"},{"location":"azure/intro/#some-of-the-key-features-of-azure-devops-include","title":"Some of the key features of Azure DevOps include:","text":"<p>Azure Repos: This is a cloud-based version control system that allows teams to manage and share code securely across the organization. It supports both Git and Team Foundation Version Control (TFVC) repositories.</p> <p>Azure Boards: This is a project management tool that supports agile methodologies such as Scrum and Kanban. It allows teams to plan and track work, create backlogs, and manage sprints and iterations.</p> <p>Azure Pipelines: This is a continuous integration and delivery (CI/CD) tool that allows teams to build, test, and deploy applications to multiple platforms and environments, including cloud platforms like Azure and AWS.</p> <p>Azure Test Plans: This is a testing tool that enables teams to test applications across different platforms and devices. It includes features like exploratory testing, test case management, and automated testing.</p> <p>Azure Artifacts: This is a package management tool that allows teams to manage and share packages, such as code libraries and dependencies, across the organization.</p>"},{"location":"azure/intro/#azure-devops-basic-workflow-digaram","title":"Azure DevOps Basic Workflow Digaram","text":"<p>Conclusion</p> <p>Overall, Azure DevOps provides a comprehensive suite of tools that enables software development teams to collaborate effectively and deliver high-quality applications at a faster pace</p>"},{"location":"azure/playwright/","title":"Automation Testing using Playwright","text":"<p>Playwright is a popular open-source testing framework for web applications that allows developers to write end-to-end tests in a simple and concise manner. With Azure DevOps, you can automate your Playwright tests and integrate them into your CI/CD pipeline for continuous testing and deployment.</p> <p>Here are the steps to set up Playwright automation with Azure DevOps:</p> <ul> <li> <p>Create a new project in Azure DevOps: Navigate to the Azure DevOps portal and create a new project.</p> </li> <li> <p>Set up a build pipeline: In the project, create a new build pipeline and configure it to use the appropriate build agent. Then, add a task to install the necessary dependencies for running Playwright tests, such as Node.js and Playwright.</p> </li> </ul> <pre><code>npm init playwright@latest\n</code></pre> <ul> <li>Configure the Playwright test task: Add a new task to the build pipeline for running Playwright tests. This task should run the command to execute your Playwright tests, such as \"npx playwright test\" or \"npm test\". You can also configure this task to specify the browser and environment in which the tests should run.</li> </ul> <pre><code>npx playwright test\n\nnpm test\n</code></pre> <ul> <li> <p>Set up test reporting: To view the results of your Playwright tests in Azure DevOps, you can add a test reporting task to the build pipeline. This task should publish the test results to Azure DevOps so that you can view them in the test summary.</p> </li> <li> <p>Set up continuous testing: Once your build pipeline is set up, you can configure it to trigger automatically whenever changes are made to your code repository. This will enable continuous testing and allow you to catch any issues early in the development process.</p> </li> </ul> <p>By following these steps, you can set up Playwright automation with Azure DevOps and integrate it into your development workflow for continuous testing and deployment.</p> <p>Info</p> <p>You can automate some DevOps operations that lack automation or API support using PlayWright by recording and playing, however this is not a recommended procedure as it may fail if there are any changes to the UI or platform side.</p> Official Document <p>Please see the official document for further details.</p> <p>https://playwright.dev/docs/intro</p>"},{"location":"azure/serviceprinciple/","title":"How to Create Service Principles in\u00a0Azure","text":"<p>To use service principles while running Azure commands in Azure DevOps pipelines, you will need to first create a service principle and grant it the necessary permissions in Azure. Then, you can use the service principle's credentials to authenticate and authorize the Azure commands that you want to run in your Azure DevOps pipelines.</p> <p></p>"},{"location":"azure/serviceprinciple/#here-is-an-example-of-how-you-can-create-a-service-principle-and-use-it-in-an-azure-devops-pipeline","title":"Here is an example of how you can create a service principle and use it in an Azure DevOps pipeline:","text":"<ul> <li> <p>Sign in to the Azure portal and navigate to the Azure Active Directory page.</p> </li> <li> <p>Click on \"App registrations\" and then click on the \"New registration\" button.</p> </li> <li> <p>Give your service principle a name and select \"Accounts in this organizational directory only\" as the supported account type. Click on the \"Register\" button to create the service principle.</p> </li> <li> <p>Click on the service principle that you just created, and then click on the \"Certificates &amp; secrets\" tab.</p> </li> <li> <p>Click on the \"New client secret\" button, give the secret a description, and select an expiration time. Click on the \"Add\" button to create the secret.</p> </li> <li> <p>Copy the secret value, as you will need it later or Store it in a Key-Vault</p> </li> <li> <p>Navigate to the resource or resources that you want to grant the service principle access to.</p> </li> <li> <p>Click on the \"Access control (IAM)\" tab, and then click on the \"Add role assignment\" button.</p> </li> <li> <p>Select the role that you want to assign to the service principle, and then type in the name of the service principle in the \"Select\" field. Click on the \"Save\" button to assign the role to the service principle.</p> </li> <li> <p>To use the service principle in your Azure DevOps pipeline, you will need to pass the service principle's client ID and client secret as environment variables. Here is an example of how you can do this:</p> </li> <li> <p>In your Azure DevOps project, navigate to the \"Pipelines\" page and click on the pipeline that you want to edit.</p> </li> <li> <p>Click on the \"Variables\" tab, and then click on the \"Add\" button.</p> </li> <li> <p>Add two new variables with the names \"AZURE_CLIENT_ID\" and \"AZURE_CLIENT_SECRET\", and set the values to the service principle's client ID and client secret, respectively.</p> </li> <li> <p>In your pipeline tasks, use the service principle's client ID and client secret to authenticate and authorize the Azure commands that you want to run. For example, you can use the following command to authenticate using the service principle:</p> </li> </ul> <pre><code>az login\u200a-\u200aservice-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET\u200a-\u200atenant $TENANT_ID\n</code></pre> <p>Note</p> <ul> <li> <p>Replace TENANT_ID with the ID of your Azure Active Directory tenant.</p> </li> <li> <p>Don't forget to store AZURE_CLIENT_ID &amp; AZURE_CLIENT_SECRET in Key-Vault for Future Use.</p> </li> </ul>"},{"location":"azure/techchallenges/","title":"Azure DevOps Technical Challanges","text":"<p>Here are some common technical challenges that can arise when using Azure DevOps:</p> ChallengesTips to Overcome <p>Integration challenges: Integrating Azure DevOps with other tools in your tech stack can be challenging. This includes integrating with other Azure services, as well as non-Azure tools like GitHub or Jira.</p> <p>Pipeline complexity: Azure DevOps provides powerful tools for building and deploying pipelines, but creating complex pipelines can be challenging. For example, setting up continuous delivery with multiple environments can require configuring many moving parts.</p> <p>Security and compliance: Ensuring security and compliance across your pipelines and repositories can be challenging. Azure DevOps provides security features like RBAC, but implementing and enforcing security policies can require careful planning and configuration.</p> <p>Release management: Managing the release of software to multiple environments can be challenging. This includes configuring release gates and approvals, rolling back releases, and tracking deployments across environments.</p> <p>Performance and scalability: As your pipeline and repository size grows, you may encounter performance and scalability issues. This can include slow build times, long queue times, and issues with concurrent builds.</p> <p>Version control: Using Git for version control in Azure DevOps can be challenging for teams not familiar with Git. It requires a different workflow than centralized version control systems, and can be difficult to manage if multiple teams are working on the same codebase.</p> <p>Continuous testing: Setting up and maintaining continuous testing with Azure DevOps can be challenging. This includes configuring test plans, integrating with test automation tools, and ensuring that tests run reliably and accurately. </p> <p>Integration challenges: To overcome integration challenges, it's important to understand the capabilities of Azure DevOps and the tools you want to integrate with. Azure DevOps provides a range of APIs and extensions that can help with integration. It's also important to plan integration in advance and test it thoroughly before deployment.</p> <p>Pipeline complexity: To simplify complex pipelines, it's important to break them down into smaller, more manageable pieces. Use templates and variables to reduce duplication and improve consistency. It's also important to test pipelines regularly and optimize for speed and reliability.</p> <p>Security and compliance: To ensure security and compliance, it's important to understand the security features of Azure DevOps and how to configure them properly. You should also establish clear security policies and ensure that all users are aware of them. Regular audits and monitoring can also help ensure compliance.</p> <p>Release management: To manage releases effectively, it's important to establish clear release policies and automate as much of the release process as possible. Use release gates and approvals to control the release process and ensure that releases are tested thoroughly before deployment. Monitor releases closely and be prepared to roll back if necessary.</p> <p>Performance and scalability: To improve performance and scalability, it's important to optimize pipelines for speed and reduce queue times. This can involve using parallelism, caching, and distributed builds. It's also important to monitor performance regularly and scale resources as necessary.</p> <p>Version control: To overcome version control challenges, it's important to establish clear version control policies and train all users in the use of Git. Use branching and merging effectively to manage code changes, and ensure that code reviews are conducted regularly to maintain code quality.</p> <p>Continuous testing: To set up and maintain continuous testing, it's important to establish clear testing policies and automate as much of the testing process as possible. Use test plans and test suites to organize and manage tests, and integrate with test automation tools for better efficiency. Monitor test results closely and be prepared to adjust your testing strategy as necessary. </p>"},{"location":"azure/techchallenges/#skipping-ci-for-git-push","title":"Skipping CI for Git Push","text":"<p>Moreover, you may instruct Azure Pipelines to forego starting a pipeline that a push would typically start. To prevent Azure Pipelines from performing CI for this push, just put [skip ci] in the message or description of any of the commits that are a part of a push. Any of the following modifications are also acceptable.</p> <ul> <li> <p>[skip ci] or [ci skip] </p> </li> <li> <p><code>***NO_CI***</code></p> </li> </ul> <p>Example</p> <p>git commit -m \" Commit Message ***No_CI*** \"</p>"},{"location":"azure/techchallenges/#syncs-changes-back-to-the-same-branch-in-azure-devops","title":"syncs changes back to the same branch in Azure DevOps:","text":"<p>The script provided below is a PowerShell script that is used in Azure DevOps to sync changes back to the same branch in the code repository. This script assumes that you have already cloned the repository in the pipeline and are working in the same branch.</p> Pre-Requsites <p>Enable OAuth Token in Agent: The agent running the script should have an OAuth token enabled to access the code repository. This can be done by following the steps in the Azure DevOps documentation.</p> <p>Now let's take a closer look at the script and what each line does:</p> <p>git pull origin $(Build.SourceBranch):  This command pulls the latest changes from the source branch into the local repository.</p> <p>git checkout $(Build.SourceBranch):  This command switches to the source branch.</p> <p>git commit -m \"Syncing updates with the code repository NO_CI\":  This  command commits the changes made to the repository and adds a commit message. The NO_CI flag in the commit message disables the continuous integration (CI) trigger for subsequent commits. This means that if there are further changes made to the code repository, the pipeline won't trigger a build, preventing the pipeline from running in a loop.</p> <p>git push origin HEAD:$(Build.SourceBranch):  This command pushes the changes to the same branch in the remote repository.</p> <p>The next block of code is an if statement that checks if the previous command executed successfully:</p> <p><pre><code>if ($?) {Write-Host \"Syncing Changes Back to Repo\"}\nelse {Write-Host \"Pushing Changes Anyway\"\ngit push -f origin HEAD:$(Build.SourceBranch)}\n</code></pre> If the previous command was successful, it displays \"Syncing Changes Back to Repo\". If the command failed, it displays \"Pushing Changes Anyway\" and then forces a push to the same branch in the remote repository.</p> <p>Overall, this script automates the process of syncing changes back to the same branch in a code repository using Azure DevOps.</p> PowerShellShell <pre><code>git pull origin $(Build.SourceBranch)\ngit checkout $(Build.SourceBranch)\ngit merge %sourceBranch% -m \"Merge to $(Build.SourceBranch)\"\ngit config --global user.email \"$(Build.RequestedForEmail)\"\ngit config --global user.name \"$(Build.RequestedFor)\"\ngit add .\ngit status\ngit commit -m \"Syncing updates with the code repository ***NO_CI***\"\ngit push origin HEAD:$(Build.SourceBranch)\n\nif ($?) {\n    Write-Host \"Syncing Changes Back to Repo\"\n} \nelse {\n    Write-Host \"Pushing Changes Anyway\"\ngit push -f origin HEAD:$(Build.SourceBranch)\n}\n</code></pre> <pre><code>git pull origin $(Build.SourceBranch)\ngit checkout $(Build.SourceBranch)\ngit merge %sourceBranch% -m \"Merge to $(Build.SourceBranch)\"\ngit config --global user.email \"$(Build.RequestedForEmail)\"\ngit config --global user.name \"$(Build.RequestedFor)\"\ngit add .\ngit status\ngit commit -m \"Syncing updates with the code repository ***NO_CI***\"\ngit push origin HEAD:$(Build.SourceBranch)\n\nif [ $? -eq 0 ]\nthen\n  echo \"Syncing Changes Back to Repo\"\nelse\n  echo \"Pulling before pushing Changes Back to Repo\"\n  git pull origin $BUILD_SOURCEBRANCH\n  git push origin HEAD:$BUILD_SOURCEBRANCH\nfi\n</code></pre>"},{"location":"azure/techchallenges/#pre-merge-validation-from-variable-source-branches-to-fixed-target-branch","title":"Pre-Merge Validation from Variable Source Branches to Fixed Target Branch","text":"<p>When there is a pull request from a variable feature branch, such as <code>Feature-A</code>, <code>Feature-B</code>, or <code>Feature-C</code> to the <code>Development</code> branch, we must determine whether the code in the feature branch is buildable or not. When the feature branch is good, we must only merge it with the development branch. However, in Azure Classic pipelines, we are unable to automatically change the source branch name. The script below will get the variable feature branch name whenever there is a PR and\u00a0 it will run the Build Validation pipeline by changing the branch name.</p> Pre-Merge Validation from Feature-Dev Branch <pre><code>echo \"Login to Azure \"\n\naz login -u $(username) -p $(password)\n\n\necho \"Fetching Current Feature Branch \"\n\n$PR = az repos pr list --repository $(Repo) --target-branch $(targetbranch) --organization $(URL) --project $(project) | grep sourceRefName | awk '{print $3}'\n$Branchname = $PR.Split(\"/\")[2] | sed 's/\"\"/\\ /g' | sed 's/,/\\ /g' | sed -e 's/^[ \\t]*//' | Foreach {$_.TrimEnd()}\n\n\necho \"The Current PR Received from Branch: $Branchname \"\ngit switch $Branchname\n\necho \"Commands to Execute\"\n\nnpm i #Example Commands\nnpm run build #Example Commands\n</code></pre>"},{"location":"azure/pipelines/acr_deployment/","title":"Azure DevOps ACR Deployment","text":""},{"location":"azure/pipelines/acr_deployment/#overview","title":"Overview","text":"<p>This is a basic Azure DevOps pipeline written in YAML for deploying a generic Sample application using Azure Container Registry (ACR). The pipeline is designed as a starter template that you can customize to fit the specific needs of your project. It follows a Continuous Integration/Continuous Deployment (CI/CD) approach, automating the build and deployment process.</p>"},{"location":"azure/pipelines/acr_deployment/#pipeline-structure","title":"Pipeline Structure","text":""},{"location":"azure/pipelines/acr_deployment/#trigger","title":"Trigger","text":"<p>The pipeline is triggered to run whenever changes are pushed to the 'main' branch.</p> <pre><code>trigger:\n  - main\n</code></pre>"},{"location":"azure/pipelines/acr_deployment/#build-agent","title":"Build Agent","text":"<p>The pipeline uses an Ubuntu-based build agent.</p> <pre><code>pool:\n  vmImage: ubuntu-latest\n</code></pre>"},{"location":"azure/pipelines/acr_deployment/#stages","title":"Stages","text":"<p>The pipeline consists of two stages - \"Dev\" and \"HigherEnv\" - representing development and higher environments (Higher_Env).</p>"},{"location":"azure/pipelines/acr_deployment/#dev-stage","title":"Dev Stage","text":"<p>The Dev stage is configured to use a variable group named \"DevEnvironment.\"</p> <pre><code>- stage: Dev\n  variables:\n    - group: DevEnvironment\n  displayName: 'Dev'\n  jobs:\n    - deployment: Dev\n      displayName: 'CI/CD Stage for Generic Sample image'\n      environment: DevEnvironment\n      strategy:\n        runOnce:\n          deploy:\n            steps:\n              # Steps for building and pushing the Docker image\n</code></pre>"},{"location":"azure/pipelines/acr_deployment/#higherenv-stage","title":"HigherEnv Stage","text":"<p>The HigherEnv stage is configured to use a variable group named \"HigherEnvVariables.\"</p> <pre><code>- stage: HigherEnv\n  variables:\n    - group: HigherEnvVariables\n  displayName: 'Higher_Env'\n  jobs:\n    - deployment: Higher_Env\n      displayName: 'CI/CD Stage for Generic Sample image'\n      environment: Higher_Env\n      strategy:\n        runOnce:\n          deploy:\n            steps:\n              # Steps for building and pushing the Docker image\n</code></pre>"},{"location":"azure/pipelines/acr_deployment/#steps","title":"Steps","text":"<p>Both stages follow a similar structure, with steps that include checking out the source code and building/pushing a Docker image.</p> <pre><code>steps:\n  - checkout: self\n  - task: Docker@2\n    displayName: 'Build &amp; Push docker image for generic Sample Application'\n    inputs:\n      repository: '$(GenericSampleImage)'\n      command: buildAndPush\n      DockerFIle: 'Dockerfile'\n      containerRegistry: '$(ACRServiceConnection)'\n      tags: |\n        $(Build.BuildId)\n        latest\n</code></pre>"},{"location":"azure/pipelines/acr_deployment/#customization","title":"Customization","text":"<p>To adapt this pipeline for your project, you can modify variables, stages, and steps according to your specific build and deployment requirements. For more customization options, refer to the official Azure DevOps YAML documentation.</p>"},{"location":"azure/pipelines/acr_deployment/#full-pipeline-code","title":"Full Pipeline Code","text":"<pre><code># Define the trigger for the pipeline to run when changes are pushed to the 'main' branch\n\ntrigger:\n  - main\n\n# Define the VM image to use for the build agent\npool:\n  vmImage: ubuntu-latest\n\n# Define the stages of the pipeline\nstages:\n  # Stage for development environment\n  - stage: Dev\n    variables:\n      # Define variable group for generic Dev environment\n      - group: DevEnvironment\n    displayName: 'Dev'\n    jobs:\n      # Define a deployment job for the Dev stage\n      - deployment: Dev\n        displayName: 'CI/CD Stage for Generic Sample image'\n        environment: DevEnvironment\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n                # Check out the source code\n                - checkout: self\n                # Build and push the Docker image for the generic Sample Application\n                - task: Docker@2\n                  displayName: 'Build &amp; Push docker image for generic Sample Application'\n                  inputs:\n                    repository: '$(GenericSampleImage)'\n                    command: buildAndPush\n                    DockerFIle: 'Dockerfile'\n                    containerRegistry: '$(ACRServiceConnection)'\n                    tags: |\n                      $(Build.BuildId)\n                      latest\n\n  # Stage for higher environments (Higher_Env)\n  - stage: HigherEnv\n    variables:\n      # Define variable group for generic Higher_Env environments\n      - group: HigherEnvVariables\n    displayName: 'Higher_Env'\n    jobs:\n      # Define a deployment job for the Higher_Env stage\n      - deployment: Higher_Env\n        displayName: 'CI/CD Stage for Generic Sample image'\n        environment: Higher_Env\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n                # Check out the source code\n                - checkout: self\n                # Build and push the Docker image for generic Sample Application\n                - task: Docker@2\n                  displayName: 'Build &amp; Push docker image for generic Sample Application'\n                  inputs:\n                    repository: '$(GenericSampleImage)'\n                    command: buildAndPush\n                    DockerFIle: 'Dockerfile'\n                    containerRegistry: '$(ACRServiceConnection)'\n                    tags: |\n                      $(Build.BuildId)\n                      latest\n</code></pre>"},{"location":"azure/pipelines/quickstart/","title":"Getting Started with Azure DevOps Pipelines: From Hello World to Real-World Deployments","text":"<p>Learn how to build and deploy Azure DevOps Pipelines, from a simple \"Hello World\" to real-world multi-job deployments.</p>"},{"location":"azure/pipelines/quickstart/#hello-world-pipeline","title":"Hello-World Pipeline","text":"<p>Azure DevOps YAML Pipeline to Print \"Hello World\"</p> azure-pipelines.yml<pre><code># Basic Azure DevOps Pipeline YAML to print \"Hello World\"\n\n# 'trigger' defines which branch will trigger the pipeline automatically.\ntrigger:\n  - main  # (1)!\n\n# 'pool' specifies the agent (virtual machine) where the pipeline will run.\npool:\n  vmImage: ubuntu-latest  # (2)!\n\n# 'steps' define the tasks the pipeline will execute.\nsteps:\n  - script: echo \"Hello World from Azure DevOps!\" # (3)!\n    displayName: 'Print Hello World'  \n</code></pre> <ol> <li> <p>trigger \u2014 Start the Pipeline Automatically. The trigger field specifies the branch that kicks off the pipeline.</p> <ul> <li> <p>main</p> </li> <li> <p>development</p> </li> <li> <p>feature1</p> </li> <li> <p>feature2</p> </li> </ul> </li> <li> <p>pool \u2014 Choose an Agent. The pool defines where the pipeline runs.</p> <ul> <li> <p>ubuntu-latest uses a ready-to-go Ubuntu machine provided by Microsoft.</p> </li> <li> <p>windows-latest</p> </li> <li> <p>macos-latest</p> </li> </ul> </li> <li> <p>steps \u2014 Define the Tasks. steps list the actions the agent should perform.</p> <ul> <li> <p>script: echo \"Hello World...\" simply runs a command to print a message in the pipeline logs.</p> </li> <li> <p>script: run main.py #Run Python Script</p> </li> <li> <p>script: Any other \"Commands Here\"</p> </li> </ul> </li> </ol> <p>\ud83d\udccc Summary:</p> Concept Description trigger Tells when to run (based on branch). pool Tells where to run (which agent image). steps Tells what to run (the actual commands)."},{"location":"azure/pipelines/quickstart/#variables","title":"Variables","text":"<p>Inline variables inside YAML</p> azure-pipelines.yml<pre><code># Azure DevOps Pipeline to print Hello World using variables\n\ntrigger:\n  - main\n\nvariables:\n  greeting: \"Hello World from Azure DevOps with Variables!\"\n\npool:\n  vmImage: ubuntu-latest\n\nsteps:\n  - script: echo $(greeting) #Calling Variable\n    displayName: 'Print Hello World using Variable'\n</code></pre> <p>Using External Variables in Azure DevOps Pipeline</p> <p>Earlier, we learned how to define variables inside the YAML file itself. Now, let's take the next step and separate variables cleanly.</p> <ul> <li> <p>Version 1: Using a file to store variables.</p> </li> <li> <p>Version 2: Using a Variable Group from Azure DevOps Library.</p> </li> </ul> Version 1Version 2 <p>Loading Variable from a File (for example, variables/vars.txt) and use it inside our pipeline.</p> <ul> <li>Create a file in your Repo in this location <code>variables/vars.txt</code></li> </ul> <p><pre><code>mkdir -p variables/vars.txt\n</code></pre> * Paste the below content azure-pipelines.yml<pre><code>variables:\ngreeting: \"Hello World from Template!\"\n</code></pre></p> <ul> <li>Create the Main Pipeline YAML</li> </ul> azure-pipelines.yml<pre><code>trigger:\n- main\n\n# Load variables from an external YAML file\nvariables:\n- template: variables/vars.yml  # Load greeting variable from variables/vars.yml\n\npool:\nvmImage: ubuntu-latest\n\nsteps:\n- script: echo $(greeting)\n    displayName: 'Print Hello World from Template Variable'\n</code></pre> <p>This time, we will create a Variable Group from the Azure DevOps UI and use it inside the pipeline.</p> <ul> <li> <p>Create a Variable Group</p> </li> <li> <p>Go to Azure DevOps Portal \u2192 Pipelines \u2192 Library \u2192 + Variable group</p> </li> <li>Create a group named (for example): <code>hello-world-vars</code></li> <li>Add a variable:</li> <li>Name: greeting</li> <li>Value: Hello World from Variable Group!</li> </ul> <p>\u2705 Save it.</p> <ul> <li>Create the Main Pipeline YAML</li> </ul> azure-pipelines.yml<pre><code>trigger:\n- main\n# Load variables from Azure DevOps Variable Groups\nvariables:\n- group: hello-world-vars \n\npool:\nvmImage: ubuntu-latest\n\nsteps:\n- script: echo $(greeting)\n    displayName: 'Print Hello World from Variable Group'\n</code></pre>"},{"location":"azure/pipelines/quickstart/#pre-defined-variables","title":"Pre-defined Variables","text":"<p>Pre-defined variables in Azure DevOps are automatically available during pipeline execution. These variables provide useful information about the pipeline's execution environment, build system, and process details. These variables can be used in pipeline definitions to access dynamic data, such as paths, build numbers, and agent details.</p> <p>Working Directory Variables</p> Variable Name Description Example Value <code>$(Build.SourcesDirectory)</code> The directory where the source code is checked out. <code>/home/vsts/work/1/s</code> <code>$(Build.ArtifactStagingDirectory)</code> The directory where build artifacts are staged before being published. <code>/home/vsts/work/1/a</code> <code>$(Build.BinariesDirectory)</code> The directory where build output (binaries) are stored. <code>/home/vsts/work/1/b</code> <code>$(Build.DropDirectory)</code> The directory for the final drop of artifacts or outputs. <code>/home/vsts/work/1/d</code> <p>Artifact Variables</p> Variable Name Description Example Value <code>$(Build.ArtifactStagingDirectory)</code> The directory where the build artifacts are staged before being published. <code>/home/vsts/work/1/a</code> <code>$(Build.ArtifactName)</code> The name of the artifact being published. <code>myArtifact.zip</code> <code>$(Build.BuildId)</code> A unique ID of the current build. <code>12345</code> <code>$(Build.DefinitionName)</code> The name of the build definition. <code>BuildPipeline</code> <p>Agent/System Variables</p> Variable Name Description Example Value <code>$(Agent.HomeDirectory)</code> The home directory of the agent on the machine. <code>/home/vsts/agent</code> <code>$(Agent.OS)</code> The operating system of the agent. <code>Linux</code> <code>$(Agent.WorkFolder)</code> The directory used by the agent to store temporary files. <code>/home/vsts/work</code> <code>$(Agent.TempDirectory)</code> The temporary directory for the agent during builds. <code>/home/vsts/temp</code> <code>$(Pipeline.Workspace)</code> The workspace directory where pipeline data is stored. <code>/home/vsts/work/1/s</code> <code>$(Pipeline.BuildId)</code> The unique identifier of the current build. <code>12345</code> <p>Build and Release Variables</p> Variable Name Description Example Value <code>$(Build.BuildId)</code> A unique ID for the current build. <code>12345</code> <code>$(Build.DefinitionName)</code> The name of the build pipeline definition. <code>CI-CD Pipeline</code> <code>$(Build.SourceBranch)</code> The source branch for the current build. <code>refs/heads/main</code> <code>$(Build.Repository.Name)</code> The name of the repository associated with the build. <code>MyRepo</code> <code>$(Release.ReleaseId)</code> The unique identifier of the release. <code>98765</code> <code>$(Release.EnvironmentName)</code> The name of the environment in the release pipeline. <code>Production</code> <p>Note</p> <p>For more details on available variables in Azure DevOps, please refer to the official Microsoft documentation.</p> <p>Agent Workspace Path: <code>/home/vsts/work/s/a</code> Breakdown</p> Full Path Component Description Example Value <code>/home/vsts/</code> The base directory for the agent. <code>/home/vsts/</code> <code>/work/</code> The working directory where the agent works. <code>/home/vsts/work/</code> <code>/1/</code> A unique number identifying a specific build. <code>/home/vsts/work/1/</code> <code>/s/</code> The source code directory where your repository is checked out. <code>/home/vsts/work/1/s/</code> <p>Sample Pipeline</p> azure-pipelines.yml<pre><code>trigger:\n  - main\n\npool:\n  vmImage: ubuntu-latest\n\nsteps:\n  - checkout: self  # Checkout the code from the main branch of the Azure Repo\n\n  - script: |\n      echo \"Listing files in $(Build.SourcesDirectory):\"\n      ls $(Build.SourcesDirectory)  # List all files in the source directory\n    displayName: 'List Files from Repo'\n</code></pre>"},{"location":"azure/pipelines/quickstart/#environment","title":"Environment","text":"<p>Environment in Azure DevOps is a way to define a target place (like Dev, QA, Prod servers) where you deploy your application and control approvals, checks, history, etc.</p> <p>Creating Environment</p> <ul> <li>Go to Azure DevOps \u2192 Pipelines \u2192 Environments.</li> <li>Create a New Environment (example: Dev-Environment).</li> <li>Inside that Environment, go to Approvals and Checks.</li> <li>Add an Approval \u2192 Example: Add yourself or your team to approve.</li> </ul> <p>\u26a1 Super Simple Flow:</p> <pre><code>Code Push \u2794 Pipeline Trigger \u2794 Environment Approval Needed \u2794 Approver Clicks \"Approve\" \u2794 Pipeline Continues\n</code></pre> azure-pipelines.yml<pre><code>trigger:\n  - main\n\nvariables:\n  - template: variables/vars.yml  # Load greeting variable from variables/vars.yml\n\npool:\n  vmImage: ubuntu-latest\n\n# Link this pipeline to an Environment\nenvironment: Dev-Environment   # Specify the environment name \n\n# Define the steps\nsteps:\n  - script: echo $(greeting)\n    displayName: 'Print Hello World from Template Variable'\n</code></pre> <p>Info</p> <p>\u2705 You can attach your pipeline to an Environment, and then configure approvals (like someone must manually approve) before the pipeline can proceed.</p>"},{"location":"azure/pipelines/quickstart/#jobs","title":"jobs","text":"<ul> <li>A job is a collection of steps that run together on the same agent.  </li> <li>Each job gets its own clean machine (agent).  </li> <li>If you have multiple jobs, they can run parallel or sequential based on how you design.</li> <li>Jobs help organize tasks like \"Build\", \"Test\", \"Deploy\" into separate logical blocks.</li> </ul> <p>\u26a1 Super Simple Flow:</p> <pre><code>Job Start \u2794 Step 1 \u2794 Step 2 \u2794 Step \"n\" \u2794 Job End\n</code></pre> Single Job PipelineMulti-Job Pipeline (Single Dependencies)Multi-Job Pipeline (Multiple Dependencies) azure-pipelines.yml<pre><code>trigger:\n  - main\n\nvariables:\n  - template: variables/vars.yml  # Load greeting variable from external file\n\npool:\n  vmImage: ubuntu-latest\n\njobs:\n  - job: PrintJob  # Define a Job named 'PrintJob'\n    displayName: 'Print Greeting Job'\n    environment: Dev-Environment  # Attach Environment directly to Job \u2705\n\n    steps:  # Steps inside the Job\n      - script: echo $(greeting)\n        displayName: 'Print Hello World from Template Variable'\n</code></pre> azure-pipelines.yml<pre><code>trigger:\n  - main\n\nvariables:\n  - template: variables/vars.yml  # Load variables from external template\n\npool:\n  vmImage: ubuntu-latest\n\njobs:\n  - job: BuildJob\n    displayName: 'Build Job'\n    environment: Dev-Environment  # Attach Environment directly to the Build job\n    steps:\n      - script: |\n          echo $(greeting)  # Use the greeting variable from the template\n          echo \"Building the project...\"\n        displayName: 'Run Build Steps'\n\n  - job: DeployJob\n    displayName: 'Deploy Job'\n    dependsOn: BuildJob  # This job depends on 'BuildJob'\n    environment: Dev-Environment  # Attach the same environment to Deploy job (can be different too)\n    steps:\n      - script: |\n          echo $(greeting)  # Use the greeting variable again\n          echo \"Deploying the project...\"\n        displayName: 'Run Deployment Steps'\n</code></pre> <p>!!! \"Note\"     If you have only a single dependency (example: dependsOn a single job like dependsOn: BuildJob), you can skip square brackets and just write it directly without [ ].</p> <p>azure-pipelines.yml<pre><code>trigger:\n  - main\n\nvariables:\n  - template: variables/vars.yml  # Load variables from external template\n\npool:\n  vmImage: ubuntu-latest\n\njobs:\n  - job: BuildJob\n    displayName: 'Build Job'\n    environment: Dev-Environment  # Attach Environment directly to the Build job\n    steps:\n      - script: |\n          echo $(greeting)  # Use the greeting variable from the template\n          echo \"Building the project...\"\n        displayName: 'Run Build Steps'\n\n  - job: TestJob\n    displayName: 'Test Job'\n    dependsOn: BuildJob  # This job depends on 'BuildJob'\n    environment: Test-Environment  # Attach a different environment to the Test job\n    steps:\n      - script: |\n          echo $(greeting)  # Use the greeting variable again\n          echo \"Running tests...\"\n        displayName: 'Run Test Steps'\n\n  - job: DeployJob\n    displayName: 'Deploy Job'\n    dependsOn: [BuildJob, TestJob]  # This job depends on both 'BuildJob' and 'TestJob'\n    environment: Test-Environment  # Attach the Test environment to Deploy job\n    steps:\n      - script: |\n          echo $(greeting)  # Use the greeting variable again\n          echo \"Deploying the project...\"\n        displayName: 'Run Deployment Steps'\n</code></pre> !!! \"Note\"     When you have multiple dependencies (example: dependsOn multiple jobs like dependsOn: [BuildJob, TestJob]), you must use square brackets [ ] to list them.</p> <p>\ud83d\udccc Summary:</p> Concept Description Steps Small tasks (scripts, copy files, etc.) Job A collection of steps Environment Can be attached at the Job level too if needed dependsOn Usage Use [ ] when depending on multiple jobs, no [ ] needed for single job"},{"location":"azure/pipelines/quickstart/#stages","title":"Stages","text":"<p>A stage in Azure DevOps is like a big chapter inside your pipeline.</p> <p>Each stage groups related steps together. Example: - One stage for building your code (<code>Build</code> stage) - Another stage for deploying your app (<code>Deploy</code> stage)</p> <p>\u2705 Stages help you organize your pipeline. \u2705 You can also control approvals, conditions, and flows between stages.</p> <p>In Simple Terms</p> <p>If Build succeeds \u2794 then go to Deploy after approval.</p> <p>\u26a1 Super Simple Flow:</p> <pre><code>Trigger \u2794 Build Stage \u2794 Deploy Stage (Approval) \u2794 Done\n</code></pre> azure-pipelines.yml<pre><code>trigger:\n- main\n\n# Load variables from external file\nvariables:\n- template: variables/vars.yml\n\n# Use Ubuntu agent\npool:\n  vmImage: ubuntu-latest\n\n# Define stages\nstages:\n- stage: Build\n  displayName: 'Build Stage'\n  jobs:\n  - job: BuildJob\n    displayName: 'Run Build Job'\n    steps:\n    - script: echo \"Building project...\"\n      displayName: 'Build Step'\n    - script: echo $(greeting)\n      displayName: 'Print Greeting Message'\n\n- stage: Deploy\n  displayName: 'Deploy Stage'\n  dependsOn: Build # wait for build to complete\n  jobs:\n  - job: DeployJob\n    displayName: 'Run Deploy Job'\n    environment: Dev-Environment  # Add Approval Here \u2705\n    steps:\n    - script: echo \"Deploying to server...\"\n      displayName: 'Deployment Step'\n</code></pre> <p>\ud83d\udccc Summary:</p> Concept Description Steps Small tasks (scripts, copy files, etc.) Job A collection of steps Stages A collection of jobs, representing a larger unit of work Environment Can be attached at the Job level too if needed"},{"location":"azure/pipelines/template/","title":"\ud83d\ude80 Boosting Efficiency: Simplifying Workflows with Reusable Pipeline Templates on Azure DevOps! \ud83d\udee0\ufe0f","text":"<p>\ud83d\udca1 Learn how to make your work smoother by using reusable pipeline templates on Azure DevOps. With these templates, you can save time, avoid mistakes, and work more effectively. Dive into our GitHub folder to see exactly how it's done:</p> <p>\ud83d\udcc1 GitHub Folder: Easy Azure DevOps Pipelines</p>"},{"location":"devops/ansible/","title":"Ansible \ud83e\udd16","text":"<p>Ansible is an open-source IT Configuration Management, Deployment &amp; Orchestration tool. It aims to provide large productivity gains to a wide variety of automation challenges. This tool is very simple to use yet powerful enough to automate complex multi-tier IT application environments.</p>"},{"location":"devops/ansible/#installing-ansible","title":"Installing Ansible \ud83d\udce5","text":"Ubuntu \ud83d\udc27CentOS \ud83d\udc27 <p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install software-properties-common -y\n</code></pre> <pre><code>sudo add-apt-repository --yes --update ppa:ansible/ansible\n</code></pre> <pre><code>sudo apt install ansible -y\n</code></pre> <pre><code>ansible --version\n</code></pre></p> <p><pre><code>sudo yum install epel-release -y\n</code></pre> <pre><code>sudo yum install ansible -y\n</code></pre> <pre><code>ansible --version\n</code></pre></p>"},{"location":"devops/ansible/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like Ec2 Instances, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., Ec2 Full Access, S3 Full Access). Note down the user's access key ID and secret access key. Reference Document Link</p> </li> </ol> <p>Warning</p> <p>Utilize Role-Based Authentication when working with Terraform on AWS Instances or Services, as it offers a higher level of security compared to using access keys.</p>"},{"location":"devops/ansible/#scp-commands","title":"SCP Commands \ud83d\udee1\ufe0f","text":"<p>To Copy files From Localmachine to Remote machine <pre><code>scp -i &lt;private-key-file-path&gt; &lt;files to copy&gt; &lt;user&gt;@&lt;IP&gt;:&lt;Remote-machine-path&gt;\n</code></pre></p> <p>Example</p> <p>scp -i Downloads/control.pem sample.txt ubuntu@54.165.128.104:/home/ubuntu/</p> <p>To Copy Files from Remotemachine to Local machine <pre><code>scp -i &lt;private-key-file-path&gt; &lt;user&gt;@&lt;IP&gt;:&lt;remote-files-path&gt; &lt;Localmachine-path&gt;\n</code></pre></p> <p>Example</p> <p>scp -i Downloads/control.pem ubuntu@54.165.128.104:/home/ubuntu/sample.txt Desktop/</p>"},{"location":"devops/ansible/#sample-inventory-file","title":"Sample Inventory File \ud83d\udccb","text":"Inventory inventory<pre><code>##Host Level\n\nwebserver01    ansible_host=&lt;Private IP&gt;\nwebserver02    ansible_host=&lt;Private IP&gt;\nwebserver03    ansible_host=&lt;Private IP&gt;      \ndbserver01     ansible_host=&lt;Private IP&gt;\ndbserver02     ansible_host=&lt;Private IP&gt;      ansible_user=ubuntu\n\n##Group Level \n\n[Group1]\nwebserverserver01\nwebserverserver02\nwebserverserver03\n\n[Group2]\ndbserver01\ndbserver02\n\n##Parent Level\n\n[dc_mumbai:children] \nwebservergrp\ndbsrvgrp\n\n##Variables\n\n[dc_mumbai:vars]\nansible_user=&lt;user&gt;\nansible_ssh_private_key_file=&lt;key-path&gt;\n</code></pre> <p>Info</p> <p>Host level has the highest priority, If you mention anything like username or Keyfile etc. It will take only, which are mentioned at the host level.</p>"},{"location":"devops/ansible/#ansible-commands","title":"Ansible Commands \ud83d\udee0\ufe0f","text":"<p>To test the connection of a particular Remote Machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping &lt;hostname&gt;\n</code></pre> To test the connection of a particular Group of Remote Machines <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping &lt;Groupname&gt;\n</code></pre> To test the connection of All Remote Machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping all\n</code></pre> To see details about the machine  <pre><code>ansible -i &lt;Inventoryfile path&gt; -m setup &lt;hostname&gt;\n</code></pre></p>"},{"location":"devops/ansible/#some-example-ad-hoc-commands","title":"Some Example Ad hoc Commands \ud83d\udce1","text":"Ad hoc Commands <p>Copy files to Remote machines name start with web Command Line<pre><code>ansible -i &lt;Inventoryfile path&gt; -m copy -a \"src=index.html dest=/var/www/html/index.html\" 'web*' --become\n</code></pre> Installing httpd in centos Remote machine  Command Line<pre><code>ansible -i &lt;Inventoryfile path&gt; -m yum -a \"name=httpd state=present\" websrvgrp --become\n</code></pre> Start &amp; Enable httpd in centos Remote machine Command Line<pre><code>ansible -i &lt;Inventoryfile path&gt; -m service -a \"name=httpd state=started enabled=yes\" websrvgrp --become\n</code></pre></p> <p>Info</p> <p>Ansible Playbooks should be with .yml or .yaml Extension for example vim sample.yml</p>"},{"location":"devops/ansible/#playbook-for-creating-files-directories","title":"Playbook For Creating Files &amp; Directories \ud83d\udcda","text":"Sample Ansible Playbooks <p>1st_playbook.yaml<pre><code>- name: Creating Files &amp; Directories\n  hosts: &lt;host&gt;\n  become: yes\n  tasks:\n     - name: Creating a Directory\n         file:\n           path: /tmp/welcome\n           state: directory\n\n     - name: Creating a File\n         file:\n           path: /tmp/sample.txt\n           state: touch\n</code></pre> To Execute the playbook <pre><code>ansible-playbook -i &lt;Inventory file path&gt; sample.yml\n</code></pre></p>"},{"location":"devops/ansible/#writing-playbook-for-installing-httpd-service-in-remote-machines-with-start-and-enable-and-copying-indexhtml-files-from-local-machine-to-the-remote-machine","title":"Writing Playbook For Installing Httpd service in remote machines with start and enable and copying index.html files from local machine to the remote machine \ud83c\udf10","text":"installation_service.yaml<pre><code>- name: Install httpd and start the service\n  hosts: all\n  tasks:\n     - name: Installing the Apache package\n       yum:\n         name: httpd\n         state: present\n\n\n     - name: Starting service\n       service:\n         name: httpd\n         state: started\n         enabled: yes\n\n     - name: Copy file with owner and permissions\n       copy:\n         src: ./index.html\n         dest: /var/www/html/index.html\n\n     - name: Restarting service\n       service:\n         name: httpd\n         state: restarted\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine","title":"Writing Playbook for Setting Up Website in Remote Machine \ud83c\udf10","text":"website.yaml<pre><code>- name: Setting up Website\n  hosts: websrv\n  gather_facts: False\n  become: True\n\n  tasks:\n    - name: Installing Packages in CentOS\n      yum:\n        name: \"{{item}}\"\n        state: present\n      when: ansible_distribution == \"CentOS\"\n      loop:\n        - httpd\n        - wget\n        - unzip\n\n    - name: Start &amp; Enable httpd\n      service:\n        name: httpd\n        state: started\n        enabled: yes   \n\n    - name: Downloading Source code\n      get_url:\n        url: https://www.tooplate.com/zip-templates/2114_pixie.zip\n        dest: /opt\n\n    - name: Unarchive a file that is already on the remote machine\n      unarchive:\n        src: /opt/2114_pixie.zip\n        dest: /opt\n        remote_src: yes\n\n    - name : Deploy Website\n      copy:\n        src: /opt/2114_pixie/\n        dest: /var/www/html/\n        remote_src: yes\n\n\n    - name: Restarting httpd service\n      service:\n        name: httpd\n        state: restarted\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine-with-conditions-handlers","title":"Writing Playbook for Setting Up Website in Remote Machine with Conditions &amp; Handlers.\ud83c\udf10","text":"loops_conditions_handlers.yaml<pre><code>- name: Writing playbook for loops and conditions\n  hosts: all\n  tasks:\n    - name: Install packages on centos\n      yum:\n        name: \"{{item}}\"\n        state: present\n      when: ansible_distribution == \"CentOS\"\n      loop:\n        - httpd\n        - wget\n        - unzip\n        - zip\n        - git           \n\n    - name: Install packages on Ubuntu \n      apt:\n        name: \"{{item}}\"\n        state: present\n        update_cache: yes\n      when: ansible_distribution == \"Ubuntu\"\n      loop:\n        - apache2\n        - wget\n        - unzip\n        - zip\n        - git\n\n\n    - name: Start &amp; enable service on CentOS\n      service:\n       name: httpd\n       state: started\n       enabled: yes\n      when: ansible_distribution == \"CentOS\"\n\n    - name: Start &amp; enable service on Ubuntu\n      service:\n       name: apache2\n       state: started\n       enabled: yes\n      when: ansible_distribution == \"Ubuntu\"\n\n\n    - name: Push index.html on centos\n      copy:\n        src: index.html\n        dest: /var/www/html/\n        backup: yes\n      when: ansible_distribution == \"CentOS\"\n      notify:\n        - Restart service on CentOS\n\n    - name: Push index.html on ubuntu\n      copy:\n        src: index.html\n        dest: /var/www/html/\n        backup: yes\n      when: ansible_distribution == \"Ubuntu\"\n      notify:\n        - Restart service on Ubuntu\n\n  handlers:\n    - name: Restart service on CentOS\n      service:\n        name: httpd\n        state: restarted\n        enabled: yes\n      when: ansible_distribution == \"CentOS\"\n\n    - name: Restart service on Ubuntu\n      service:\n        name: apache2\n        state: restarted\n        enabled: yes\n      when: ansible_distribution == \"Ubuntu\"\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-to-create-vpc-in-aws-cloud-and-including-variables-from-different-file","title":"Writing Playbook to Create VPC in AWS Cloud and Including Variables from different File \u2601\ufe0f","text":"Requirements <ul> <li> <p>python &gt;= 3.6</p> </li> <li> <p>boto3 &gt;= 1.15.0</p> </li> <li> <p>botocore &gt;= 1.18.0</p> </li> </ul> <p><pre><code>apt install python3-pip\n</code></pre> <pre><code>pip install boto\n</code></pre> <pre><code>pip install boto3\n</code></pre> <pre><code>pip install botocore\n</code></pre> Creating File to store Variables <pre><code>vim vpc_setup.txt\n</code></pre> <pre><code>vpc_name: \"Vprofile-vpc\"\n\n#Vpc-range\nvpcrange: '172.21.0.0/16'\n\n#subnet range\npubip1: '172.21.1.0/24'\npubip2: '172.21.2.0/24'\npubip3: '172.21.3.0/24'\npvtip1: '172.21.4.0/24'\npvtip2: '172.21.5.0/24'\npvtip3: '172.21.6.0/24'\n\n#region\nregion: 'us-east-2'\n\n#zone names\nzone1: us-east-2a\nzone2: us-east-2b\nzone3: us-east-2c\n\n\nstate: present\n</code></pre></p> Playbook \ud83d\udcda aws_vpc.yaml<pre><code>- hosts: localhost\n  connection: local\n  gather_facts: False\n  tasks:\n    - name: Import vpc variables\n      include_vars: /path/vpc_setup\n\n    - name: create vpc\n      ec2_vpc_net:\n          name: \"{{vpc_name}}\"\n          cidr_block: \"{{vpcrange}}\"\n          region: \"{{region}}\"\n          dns_support: yes\n          dns_hostnames: yes\n          tenancy: default\n          state: \"{{state}}\"\n      register: vpcout\n\n\n    - name: Create a public subnet for zone1\n      ec2_vpc_subnet:\n          vpc_id: \"{{vpcout.vpc.id}}\"\n          region: \"{{region}}\"\n          az: \"{{zone1}}\"\n          state: \"{{state}}\"\n          cidr: \"{{pubip1}}\"\n          map_public: yes\n          tags:\n            Name: vprofile_pubsub1\n      register: pubsub1_out\n\n    - name: Create a public subnet for zone2\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone2}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pubip2}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pubsub2\n      register: pubsub2_out\n\n    - name: Create a public subnet for zone3\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone3}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pubip3}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pubsub3\n      register: pubsub3_out\n\n    - name: Create a private subnet for zone1\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone1}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pvtip1}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pvtsub1\n      register: pvtsub1_out\n\n    - name: Create a private subnet for zone2\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone2}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pvtip2}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pvtsub2\n      register: pvtsub2_out\n\n    - name: Create a private subnet for zone3\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone3}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pvtip3}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pvtsub3\n      register: pvtsub3_out\n\n    - name: Internet gateway setup\n      ec2_vpc_igw:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         state: \"{{state}}\"\n         tags:\n           Name: vprofile_IGW\n      register: igw_out\n\n\n\n    - name: public subnet route table\n      ec2_vpc_route_table:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         tags:\n           Name: vprofile_Public\n         subnets:\n           - \"{{pubsub1_out.subnet.id}}\"\n           - \"{{pubsub2_out.subnet.id}}\"\n           - \"{{pubsub3_out.subnet.id}}\"\n         routes:\n           - dest: 0.0.0.0/0\n             gateway_id: \"{{ igw_out.gateway_id }}\"\n      register: public_route_table\n\n\n    - name: Create a new nat gateway and allocate a new EIP if a nat gateway does not yet exist in the subnet.\n      ec2_vpc_nat_gateway:\n         state: \"{{state}}\"\n         subnet_id: \"{{pubsub1_out.subnet.id}}\"\n         wait: true\n         region: \"{{region}}\"\n         if_exist_do_not_create: true\n      register: nat_out\n\n    - name: private subnet route table\n      ec2_vpc_route_table:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         tags:\n           Name: vprofile_Private\n         subnets:\n           - \"{{pvtsub1_out.subnet.id}}\"\n           - \"{{pvtsub2_out.subnet.id}}\"\n           - \"{{pvtsub3_out.subnet.id}}\"\n         routes:\n           - dest: 0.0.0.0/0\n             gateway_id: \"{{nat_out.nat_gateway_id}}\"\n      register: private_route_table\n\n\n    - debug:\n        var: \"{{item}}\"\n      loop:\n         - vpcout.vpc.id\n         - pubsub1_out.subnet.id\n         - pubsub2_out.subnet.id\n         - pubsub3_out.subnet.id\n         - pvtsub1_out.subnet.id\n         - pvtsub2_out.subnet.id\n         - pvtsub3_out.subnet.id\n         - igw_out.gateway_id\n         - public_route_table.route_table.id\n         - nat_out.nat_gateway_id\n         - private_route_table.route_table.id\n\n    - set_fact:\n        vpcid: \"{{vpcout.vpc.id}}\"\n        pubsublid: \"{{ pubsub1_out.subnet.id }}\"\n        pubsub2id: \"{{ pubsub2_out.subnet.id }}\"\n        pubsub3id: \"{{ pubsub3_out.subnet.id }}\"\n        privsublid: \"{{ pvtsub1_out.subnet.id }}\"\n        privsub2id: \"{{ pvtsub2_out.subnet.id }}\"\n        privsub3id: \"{{ pvtsub3_out.subnet.id }}\"\n        igwid: \"{{ igw_out.gateway_id }}\"\n        pubRTid: \"{{ public_route_table.route_table.id }}\"\n        NATGWid: \"{{ nat_out.nat_gateway_id }}\"\n        privRTid: \"{{ private_route_table.route_table.id }}\"\n        cacheable: yes\n\n    - name: creating file for vpc output\n      copy:\n        content: \"vpcid: {{vpcout.vpc.id}}\\n pubsublid: {{ pubsub1_out.subnet.id }}\\npubsub2id: {{ pubsub2_out.subnet.id }}\\npubsub3id: {{ pubsub3_out.subnet.id }}\\nprivsublid: {{ pvtsub1_out.subnet.id }}\\nprivsub2id: {{ pvtsub2_out.subnet.id }}\\nprivsub3id: {{ pvtsub3_out.subnet.id }}\\nigwid: {{ igw_out.gateway_id }}\\npubRTid: {{ public_route_table.route_table.id }}\\nNATGWid: {{ nat_out.nat_gateway_id }}\\nprivRTid: {{ private_route_table.route_table.id }}\"\n        dest: /home/ubuntu/Vprofile/vars/output_vars\n</code></pre>"},{"location":"devops/ansible/#launching-ec2-instance-in-aws-cloud","title":"Launching Ec2 Instance in AWS Cloud \ud83d\ude80","text":"Requirements <ul> <li> <p>python &gt;= 3.6</p> </li> <li> <p>boto3 &gt;= 1.15.0</p> </li> <li> <p>botocore &gt;= 1.18.0</p> </li> </ul> <p><pre><code>apt install python3-pip\n</code></pre> <pre><code>pip install boto\n</code></pre> <pre><code>pip install boto3\n</code></pre> <pre><code>pip install botocore\n</code></pre></p> Launching Instance Playbook aws_ec2.yaml<pre><code>- name: Launching Ec2 Instance\n  hosts: localhost\n  connection: local\n  tasks:\n     - name: Creating Key pair\n       amazon.aws.ec2_key:\n         name: samplekey\n         region: us-west-1\n       register: key\n\n     - debug:\n        var: key\n\n     - name: Storing private key into a file\n       copy:\n          content: \"{{key.key.private_key}}\"\n          dest: \"./sample.pem\"\n          mode: 0600\n       when: key.changed\n\n\n     - name: Creating Security Group\n       amazon.aws.ec2_group:\n         name: mysg\n         description: Allowing 22 and 80\n         vpc_id: vpc-0c8e70cf05b1342ac\n         region: us-west-1\n         rules:\n           - proto: tcp\n             from_port: 22\n             to_port: 22\n             cidr_ip: 0.0.0.0/0\n             rule_desc: allow all on port 80 &amp; 22\n       register: sg_out\n\n     - name: Launching bastion_host\n       ec2:\n          key_name: \"samplekey\"\n          region: us-west-1\n          instance_type: t2.micro\n          image: ami-0573b70afecda915d\n          wait: yes\n          wait_timeout: 300\n          instance_tags:\n            name: \"Ansible Instance\"\n            project: vprofile\n            owner: devops team\n          exact_count: 1\n          count_tag:\n            name: \"Ansible Instance\"\n            project: vprofile\n            owner: devops team\n          group_id: \"{{sg_out.group_id}}\"\n          vpc_subnet_id: subnet-0c4734c845e549cda\n       register: instance\n</code></pre>"},{"location":"devops/ansible/#ansible-configuration-file","title":"Ansible Configuration File \u2699\ufe0f","text":"<p>Note</p> <p>You should save the configuration file with name ansible.cfg</p> <pre><code>vim ansible.cfg\n</code></pre> Ansible Configuration File ansible.cfg<pre><code>[defaults]\nhost_key_checking=False\ninventory=&lt;Inventory File Path&gt;\ntimeout=20\nlog_path=/var/log/ansible_world.log\nremote_port=22\nremote_user=&lt;username&gt;\n\n[privilege_escalation]\nbecome=True\nbecome_method=sudo\nbecome_user=root\nbecome_ask_pass=False\n</code></pre>"},{"location":"devops/aws-cli/","title":"AWS","text":"<p>Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally.</p>"},{"location":"devops/aws-cli/#aws-cli-command-line-interface","title":"AWS CLI (Command Line Interface)","text":""},{"location":"devops/aws-cli/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like Ec2 Instances, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., Ec2 Full Access, S3 Full Access). Note down the user's access key ID and secret access key. Reference Document Link</p> </li> </ol> <p>Warning</p> <p>Utilize Role-Based Authentication when working with Terraform on AWS Instances or Services, as it offers a higher level of security compared to using access keys.</p>"},{"location":"devops/aws-cli/#installing-aws-cli","title":"Installing AWS-CLI","text":"Linux \ud83d\udc27Windows \ud83e\ude9f <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n</code></pre> <pre><code>unzip awscliv2.zip\n</code></pre> <pre><code>sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update\n</code></pre> <pre><code>choco install awscli\n</code></pre> <pre><code>aws --version\n</code></pre>"},{"location":"devops/aws-cli/#configure-aws-cli-with-iam-user-credentials-with-a-specific-region","title":"Configure AWS CLI with IAM user Credentials with a specific Region","text":"<p><pre><code>aws configure\n</code></pre>  Once it is done try some aws cli commands like aws s3 ls If u have any buckets in your s3 it will list</p>"},{"location":"devops/aws-cli/#ec2-elastic-compute-cloud","title":"EC2 \u2013 Elastic Compute Cloud","text":""},{"location":"devops/aws-cli/#create-a-key-pair","title":"Create a key pair","text":"<p><pre><code>aws ec2 create-key-pair --key-name &lt;keypair-Name&gt; --query 'KeyMaterial' --output text &gt; &lt;keypair-Name.pem&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-a-key-pair","title":"Delete a key pair","text":"<p>To delete a key pair, run the aws ec2 delete-key-pair command, substituting MyKeyPair with the name of the pair to delete. <pre><code>aws ec2 delete-key-pair --key-name &lt;keypair-Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-security-group-adding-inbound-rules","title":"Create a Security Group &amp; Adding Inbound rules","text":"<p><pre><code>aws ec2 create-security-group --group-name &lt;security grp Name&gt; --description \"&lt;Description&gt;\"\n</code></pre> <pre><code>curl https://checkip.amazonaws.com\n</code></pre> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;security group Id&gt; --protocol tcp --port &lt;port Number&gt; --cidr &lt;ip address&gt;\n</code></pre> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;security grp Id&gt;--protocol tcp --port 22-8000 --cidr 0.0.0.0/0 \n</code></pre>  To view the initial information for my-sg, run the aws ec2 describe-security-groups command. For an EC2-Classic security group, you can reference it by its name. <pre><code>aws ec2 describe-security-groups --group-names &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-security-group","title":"Delete your security group","text":"<p>The following command example deletes the EC2-Classic security group named. <pre><code>aws ec2 delete-security-group --group-name &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#launch-instance","title":"Launch Instance","text":"<p>You can use the following command to launch a t2.micro instance in EC2-Classic. Replace the italicized parameter values with your own. You can get the AMI IDs from documentation or console for your required Instance. <pre><code> aws ec2 run-instances --image-id &lt;ami-Id&gt; --count 1 --instance-type &lt;type&gt; --key-name &lt;keypair-Name&gt; --security-groups &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#add-a-tag-to-your-instance","title":"Add a tag to your Instance","text":"<p><pre><code>aws ec2 create-tags --resources &lt;Instance-Id&gt;--tags Key=Name,Value=&lt;value&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#terminate-your-instance","title":"Terminate your Instance","text":"<p>To delete an instance, you use the command aws ec2 terminate-instances to delete it. <pre><code>aws ec2 terminate-instances --instance-ids &lt;Instance-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-launch-template","title":"Create Launch Template","text":"<p><pre><code>aws ec2 create-launch-template --launch-template-name &lt;Name&gt;\":[{\"AssociatePublicIpAddress\":true,\"DeviceIndex\":0,\"Ipv6AddressCount\":1,\"SubnetId\":\"pe\":\"&lt;Instance type\",\"TagSpecifications\":[{\"ResourceType\":\"instance\",\" Tags\":[{\"Key\":\"Name\",\"Value\":\"&lt;value&gt;\"}]}]}'\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-launch-template","title":"Delete Launch Template","text":"<p><pre><code>aws ec2 delete-launch-template --launch-template-id &lt; template id&gt;  --region &lt;region&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#creating-auto-scaling-group","title":"Creating Auto-Scaling group","text":"<p><pre><code>aws autoscaling create-auto-scaling-group --auto-scaling-group-name &lt;Name&gt;  --launch-LaunchTemplateId=&lt;template \u2013 id &gt; --min-size 2 --max-size 5 --vpc-zone-identifier \"subnet1-id,subnet2-id,subnet3-id\"\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-auto-scaling-group","title":"Delete your Auto-Scaling Group","text":"<pre><code>aws autoscaling delete-auto-scaling-group --auto-scaling-group-name &lt; Auto -Scaling group Name &gt;\n</code></pre>"},{"location":"devops/aws-cli/#ebs-elastic-block-storage","title":"EBS \u2013 Elastic Block Storage","text":""},{"location":"devops/aws-cli/#create-ebs-volume","title":"Create EBS Volume","text":"<p>To create an empty General Purpose SSD (gp2) volume <pre><code>aws ec2 create-volume --volume-type &lt;volume type&gt; --size &lt;size in number&gt; --availability-zone &lt;zone&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-an-encrypted-volume","title":"To create an encrypted volume","text":"<p><pre><code>aws ec2 create-volume --volume-type &lt;volume type&gt; --size &lt;size in number&gt;  --encrypted --availability-zone &lt;zone&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-a-volume-with-tags","title":"To create a volume with tags","text":"<p><pre><code>aws ec2 create-tags --resources &lt;volume-id&gt; --tags Key=Name,Value=&lt;value&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-delete-a-volume","title":"To Delete a Volume","text":"<pre><code>aws ec2 delete-volume --volume-id &lt;volume Id&gt;\n</code></pre> <p>Output</p> <p>Output: None</p>"},{"location":"devops/aws-cli/#to-create-a-snapshot","title":"To create a snapshot","text":"<p>This example command creates a snapshot of the volume with a volume ID of  and a short description to identify the snapshot. <pre><code>aws ec2 create-snapshot --volume-id &lt;volume Id&gt; --description \"&lt;Description&gt;\"\n</code></pre>"},{"location":"devops/aws-cli/#to-create-a-snapshot-with-tags","title":"To create a snapshot with tags","text":"<p><pre><code>aws ec2 create-snapshot --volume-id &lt;volume Id&gt; --description 'Prod backup' --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=&lt;value&gt;},{Key=Database,Value=Mysql}]'\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-allocate-an-elastic-ip-address-for-ec2-classic","title":"To allocate an Elastic IP address for EC2-Classic","text":"<p>The following allocate-address example allocates an Elastic IP address to use with an instance in EC2-Classic. <pre><code>aws ec2 allocate-address\n</code></pre> </p>"},{"location":"devops/aws-cli/#elb-elastic-load-balancer","title":"ELB \u2013 Elastic Load Balancer","text":""},{"location":"devops/aws-cli/#create-load-balancer","title":"Create-load-balancer","text":""},{"location":"devops/aws-cli/#to-create-an-application-load-balancer","title":"To create an Application load balancer","text":"<p>The below commands to find subnet id &amp; Instance Id  <pre><code>aws ec2 describe-subnets\n</code></pre> <pre><code>aws ec2 describe-instances\n</code></pre> <pre><code>aws elbv2 create-load-balancer --name &lt;Load balancer Name&gt;--type &lt;type&gt; --subnets &lt;subnet-Id&gt; &lt;subnet-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-a-network-load-balancer","title":"To create a Network load balancer","text":"<p><pre><code>aws elbv2 create-load-balancer --name &lt;Load balancer Name&gt;--type &lt;type&gt; --subnets &lt;subnet-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-register-instances-with-a-load-balancer","title":"To register instances with a load balancer","text":"<pre><code>aws elb register-instances-with-load-balancer --load-balancer-name &lt;Load balancer Name&gt; --instances &lt;Instance-Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#to-delete-a-specific-load-balancer","title":"To Delete a Specific Load balancer","text":"<p><pre><code>aws elbv2 delete-load-balancer --load-balancer-arn &lt;arn end point&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#rds-relational-database-service","title":"RDS - Relational Database Service","text":""},{"location":"devops/aws-cli/#create-db-instance","title":"Create-db-Instance","text":"<p><pre><code> aws rds create-db-instance --db-instance-identifier &lt;db - Name&gt; --db-instance-class &lt;db.type&gt; --engine &lt;Database Engine&gt;  --master-username &lt;username&gt; --master-user-password &lt;password&gt; --allocated-storage &lt;storage in numbers&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-delete-your-db-instance","title":"To delete your db-Instance","text":"<p><pre><code>aws rds delete-db-instance --db-instance-identifier &lt;db - Name&gt; --final-db-snapshot-identifier &lt;db - Name&gt;-final-snap\n</code></pre> </p>"},{"location":"devops/aws-cli/#s3-simple-storage-service","title":"S3 \u2013 Simple Storage Service","text":""},{"location":"devops/aws-cli/#list-buckets-objects","title":"List Buckets &amp; Objects","text":"<p>To list your buckets, folders, or objects, use the s3 ls command. Using the command without a target or options lists all buckets. <pre><code>aws s3 ls\n</code></pre> <pre><code>aws s3 ls s3://&lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-bucket","title":"Create a bucket","text":"<p>Use the s3 mb command to make a bucket. Bucket names must be globally unique (unique across all of Amazon S3) and should be DNS compliant. <pre><code>aws s3 mb s3:// &lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#copy-objects","title":"Copy objects","text":"<p>Use the s3 cp command to copy objects from a bucket or a local directory <pre><code>aws s3 cp &lt;file&gt; s3:// &lt;bucket name&gt;\n</code></pre> <pre><code>aws s3 cp s3://&lt;source bucket/file&gt; s3://&lt;destination-bucket&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#move-objects","title":"Move objects","text":"<p>Use the s3 mv command to move objects from a bucket or a local directory. <pre><code>aws s3 mv &lt;local file&gt; s3:// &lt;bucket name&gt; \n</code></pre> <pre><code>aws s3 mv s3:// &lt;source bucket/file&gt; s3://&lt;destination-bucket&gt;\n</code></pre></p>"},{"location":"devops/aws-cli/#sync-objects","title":"Sync Objects","text":"<p><pre><code>aws s3 sync . s3://&lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-objects","title":"Delete Objects","text":"<p><pre><code>aws s3 rm s3://&lt;bucket name/file&gt; --recursive\n</code></pre> </p>"},{"location":"devops/aws-cli/#empty-bucket","title":"Empty Bucket","text":"<pre><code>aws s3 rm s3://&lt;bucket name&gt; --recursive\n</code></pre>"},{"location":"devops/aws-cli/#delete-bucket","title":"Delete Bucket","text":"<pre><code>aws s3 rb s3://&lt;bucket name&gt;\n</code></pre>"},{"location":"devops/aws-cli/#vpc-virtual-private-cloud","title":"VPC \u2013 Virtual Private Cloud","text":""},{"location":"devops/aws-cli/#to-create-a-vpc-and-subnets-using-the-aws-cli","title":"To create a VPC and subnets using the AWS CLI","text":""},{"location":"devops/aws-cli/#create-a-vpc-with-a-1000016-cidr-block-using-the-following-create-vpc-command","title":"Create a VPC with a 10.0.0.0/16 CIDR block using the following create-vpc command.","text":"<p><pre><code>aws ec2 create-vpc --cidr-block &lt;Ip address&gt; --query Vpc.VpcId --output text\n</code></pre> </p>"},{"location":"devops/aws-cli/#using-the-vpc-id-from-the-previous-step-create-a-subnet-with-a-1001024-cidr-block-using-the-following-create-subnet-command","title":"Using the VPC ID from the previous step, create a subnet with a 10.0.1.0/24 CIDR block using the following create-subnet command.","text":"<p><pre><code>aws ec2 create-subnet --vpc-id &lt;vpc - Id&gt;--cidr-block &lt;Ip address&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-second-subnet-in-your-vpc-with-a-1002024-cidr-block","title":"Create a second subnet in your VPC with a 10.0.2.0/24 CIDR block.","text":"<p><pre><code>aws ec2 create-subnet --vpc-id &lt;vpc - Id&gt;--cidr-block &lt;Ip address&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-an-internet-gateway-using-the-following-create-internet-gateway-command","title":"Create an internet gateway using the following create-internet-gateway command.","text":"<p><pre><code>aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text\n</code></pre> </p> <p></p>"},{"location":"devops/aws-cli/#using-the-id-from-the-previous-step-attach-the-internet-gateway-to-your-vpc-using-the-following-attach-internet-gateway-command","title":"Using the ID from the previous step, attach the internet gateway to your VPC using the following attach-internet-gateway command.","text":"<pre><code>aws ec2 attach-internet-gateway --vpc-id &lt;vpc - Id&gt;--internet-gateway-id &lt;IGW - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#create-a-custom-route-table-for-your-vpc-using-the-following-create-route-table-command","title":"Create a custom route table for your VPC using the following create-route-table command.","text":"<p><pre><code>aws ec2 create-route-table --vpc-id &lt;vpc - Id&gt;--query RouteTable.RouteTableId --output text\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-route-in-the-route-table-that-points-all-traffic-00000-to-the-internet-gateway-using-the-following-create-route-command","title":"Create a route in the route table that points all traffic (0.0.0.0/0) to the internet gateway using the following create-route command.","text":"<p><pre><code>aws ec2 create-route --route-table-id &lt;route table - Id&gt;--destination-cidr-block 0.0.0.0/0 --gateway-id &lt;Igw - Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#you-can-describe-the-route-table-using-the-following-describe-route-tables-command","title":"You can describe the route table using the following describe-route-tables command.","text":"<p><pre><code>aws ec2 describe-route-tables --route-table-id &lt;route table - Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#the-route-table-is-currently-not-associated-with-any-subnet-you-need-to-associate-it-with-a-subnet-in-your-vpc-so-that-traffic-from-that-subnet-is-routed-to-the-internet-gateway","title":"The route table is currently not associated with any subnet. You need to associate it with a subnet in your VPC so that traffic from that subnet is routed to the internet gateway.","text":"<p><pre><code>aws ec2 describe-subnets --filters \"Name=vpc-id,Values=&lt;vpc \u2013Id&gt;  --query \"Subnets[*].{ID:SubnetId,CIDR:CidrBlock}\"\n</code></pre> </p>"},{"location":"devops/aws-cli/#you-can-choose-which-subnet-to-associate-with-the-custom-route-table-for-example-subnet-0c312202b3f26703a-and-associate-it-using-the-associate-route-table-command-this-subnet-is-your-public-subnet","title":"You can choose which subnet to associate with the custom route table, for example, subnet-0c312202b3f26703a, and associate it using the associate-route-table command. This subnet is your public subnet.","text":"<pre><code>aws ec2 associate-route-table  --subnet-id &lt;subnet-Id&gt; --route-table-id &lt;route table - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#clean-up","title":"CLEAN UP","text":""},{"location":"devops/aws-cli/#delete-your-custom-route-table","title":"Delete your custom route table:","text":"<pre><code>aws ec2 delete-route-table --route-table-id &lt;route table - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-subnets","title":"Delete your subnets:","text":"<pre><code>aws ec2 delete-subnet --subnet-id &lt;subnet-Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#detach-your-internet-gateway-from-your-vpc","title":"Detach your internet gateway from your VPC:","text":"<pre><code>aws ec2 detach-internet-gateway --internet-gateway-id &lt;Igw -Id&gt; --vpc-id &lt;vpc- Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-internet-gateway","title":"Delete your internet gateway:","text":"<pre><code>aws ec2 delete-internet-gateway --internet-gateway-id &lt;Igw - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-vpc","title":"Delete your VPC:","text":"<pre><code>aws ec2 delete-vpc --vpc-id &lt;vpc- Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#cloud-watch","title":"Cloud Watch","text":""},{"location":"devops/aws-cli/#creating-alarm","title":"Creating Alarm","text":"<p><pre><code>aws cloudwatch put-metric-alarm --alarm-name &lt;Alarm name&gt; --alarm-description \"&lt;Description&gt;\" --metric-name &lt;Metric&gt; --namespace AWS/EC2 --statistic Average --period 300 --threshold &lt;70&gt; --comparison-operator &lt;GreaterThanThreshold&gt;  --dimensions \"Name=InstanceId,Value=&lt;Id&gt;\" --evaluation-periods 2 --alarm-actions &lt;SNS \u2013 arn &gt; --unit Percent \n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-alarm","title":"Delete Your Alarm","text":"<pre><code>aws cloudwatch delete-alarms --alarm-names &lt;Alarm name&gt; \n</code></pre>"},{"location":"devops/aws-cli/#disable-your-alarm","title":"Disable your Alarm","text":"<pre><code>aws cloudwatch disable-alarm-actions --alarm-names &lt;Alarm name&gt;\n</code></pre>"},{"location":"devops/aws-cli/#enable-your-alarm","title":"Enable your Alarm","text":"<pre><code>aws cloudwatch enable-alarm-actions --alarm-names &lt;Alarm name&gt;\n</code></pre>"},{"location":"devops/awsdevops/","title":"DevOps","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increase an organization\u2019s ability to deliver applications and services at high velocity: Evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p>"},{"location":"devops/awsdevops/#learning-paths-with-practical-workshops","title":"Learning Paths with Practical Workshops","text":"<p>Decoding DevOps:This course is designed to provide a comprehensive understanding of the principles and practices of DevOps. It covers topics such as continuous integration and delivery, infrastructure as code, configuration management, and monitoring and logging. The course is suitable for software developers, system administrators, and IT professionals who want to learn about DevOps and how it can be implemented in their organizations. By the end of the course, you should have a good understanding of the DevOps culture, tools, and practices.</p> <p>Real Time DevOps Projects:This course is focused on providing hands-on experience with DevOps tools and practices through real-world projects. You'll work on projects such as setting up a CI/CD pipeline, automating infrastructure deployment, and monitoring and logging applications. The course is designed for learners who have some experience with DevOps and want to gain practical experience working on real-world projects. By the end of the course, you should have a portfolio of completed projects that you can showcase to potential employers. </p> <p></p>"},{"location":"devops/azure/","title":"Azure","text":"<p>Updating...</p>"},{"location":"devops/docker-setup/","title":"Docker \ud83d\udc33","text":"<p>Docker is a powerful containerization tool designed to simplify the process of creating, deploying, and running applications using containers. Containers package applications along with their dependencies, making it easy to deploy them consistently across different environments. Unlike traditional virtualization, containers share the host OS kernel, making them lightweight and efficient.</p>"},{"location":"devops/docker-setup/#installation-of-docker-engine-docker-compose-on-ubuntu-centos","title":"Installation of Docker Engine &amp; Docker Compose on Ubuntu &amp; CentOS \ud83d\udce5","text":"Docker Setup <p>To streamline the installation process, you can use the following script to install Docker Engine and Docker Compose on both Ubuntu and CentOS: <pre><code>vim docker_setup.sh\n</code></pre> Dockerfile<pre><code>#!/bin/bash\napt --help &gt;&gt;/dev/null\nif [ $? -eq 0 ]\n   then \necho \" INSTALLING DOCKER IN UBUNTU\"\necho\nsudo apt update\nsudo apt-get remove docker docker-engine docker.io containerd runc\nsudo apt-get update\nsudo apt-get -y install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \\\n  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io -y  \nsudo docker run hello-world\n    else\necho \" INSTALLING DOCKER IN CENTOS\"\necho\nsudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine\nsudo yum install -y yum-utils\nsudo yum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\nsudo yum install docker-ce docker-ce-cli containerd.io -y   \nsudo systemctl start docker\nsudo docker run hello-world\nfi\necho \" Installing Docker Compose\"\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre> Now, grant execute permission to the script: <pre><code>sudo chmod +x docker_setup.sh\n</code></pre> Finally, run the script to install Docker and Docker Compose: <pre><code>./docker_setup.sh\n</code></pre></p>"},{"location":"devops/docker-setup/#docker-commands","title":"Docker Commands \ud83d\ude80","text":"Docker Images \ud83d\uddbc\ufe0fContainers \ud83d\udce6Share \ud83d\udd04 <p>List All Images \ud83d\udccb</p> <p>To display all locally stored Docker images, use the following command:   <pre><code>docker image ls\n</code></pre></p> <p>Build an Image \ud83d\udee0\ufe0f</p> <p>To create an image from a Dockerfile in the current directory and assign a tag, utilize this command:  <pre><code>docker build -t &lt;imagename&gt;:&lt;tag&gt; \n</code></pre></p> <p>Delete an Image \ud83d\uddd1\ufe0f</p> <p>Remove an image from the local image store using the following command:  <pre><code>docker image rm &lt;imagename&gt;:&lt;tag&gt;\n</code></pre></p> <p>Run a container in an interactive mode:  <pre><code>docker run -it &lt;imagename&gt;:&lt;tag&gt;\n</code></pre></p> <p>Run a container from the Image nginx:latest, name it \"web,\" and expose port 5000 externally, mapped to port 80 inside the container in detached mode: \u25b6\ufe0f <pre><code>docker run --name web -d -p 5000:80 nginx:latest\n</code></pre></p> <p>Run a detached container in a previously created container network: \ud83c\udf10 <pre><code>docker network create &lt;mynetwork&gt;\n</code></pre> <pre><code>docker run --name web -d --net mynetwork -p 5000:80 nginx:latest\n</code></pre></p> <p>Follow the logs of a specific container: <pre><code>docker logs -f &lt;container name or container container-id&gt;\n</code></pre></p> <p>List only active containers \ud83d\udfe2 <pre><code>docker ps\n</code></pre></p> <p>List all containers \ud83d\udfe2\ud83d\udd34 <pre><code>docker ps -a\n</code></pre></p> <p>Stop a container \u23f9\ufe0f <pre><code>docker stop &lt;container name or container container-id&gt;\n</code></pre></p> <p>Stop a container (timeout = 1 second)  <pre><code>docker stop -t1\n</code></pre></p> <p>Remove a stopped container \ud83d\uddd1\ufe0f <pre><code>docker rm &lt;container name or container container-id&gt;\n</code></pre></p> <p>Force stop and remove a container <pre><code>docker rm -f &lt;container name or container container-id&gt;\n</code></pre></p> <p>Remove all containers  <pre><code>docker rm -f $(docker ps-aq)\n</code></pre></p> <p>Remove all stopped containers <pre><code>docker rm $(docker ps -q -f \u201cstatus=exited\u201d)\n</code></pre></p> <p>Execute a new process in an existing container: Execute and access bash inside a container <pre><code>docker exec -it &lt;container name or container-id&gt; bash\n</code></pre> To inspect the container <pre><code>docker inspect &lt;container name or container container-id&gt;\n</code></pre></p> <p>To Establish Connections from Local to Remote. log in with your Dockerhub Credentials. <pre><code>docker login\n</code></pre> Pull an image from a registry</p> <pre><code>docker pull &lt;imagename&gt;:&lt;tag&gt;\n</code></pre> <p>Retag a local image with a new image name and tag <pre><code>docker tag myimage:1.0 myrepo/myimage:2.0\n</code></pre></p> <p>Push an image to a registry. <pre><code>docker push myrepo/myimage:2.0\n</code></pre></p>"},{"location":"devops/docker-setup/#dockerfile","title":"Dockerfile \ud83d\udccb","text":"Docker InstructionsSample DockerfileUsing VariablesUsing Existing zip/tar File <ul> <li> <p>\ud83c\udfc1 FROM - Base Image to run other instructions or commands</p> </li> <li> <p>\ud83c\udfc3\u200d\u2642\ufe0f RUN - Run commands</p> </li> <li> <p>\ud83d\udcc2 WORKDIR - Set working directory in the container</p> </li> <li> <p>\ud83d\udcdc CMD - Command to run within the container</p> </li> <li> <p>\ud83d\udd11 ENTRYPOINT - Configures a command that will run as the container starts, overriding the CMD instruction if both are specified.</p> </li> <li> <p>\ud83d\udce6 VOLUME - Mount a directory from the host to the container</p> </li> <li> <p>\ud83d\udcdd COPY - Copy files or directories to the container</p> </li> <li> <p>\u2795 ADD - Copy files or directories to the container with additional features (like extracting TAR files)</p> </li> <li> <p>\ud83c\udff7\ufe0f LABEL - Add metadata to the image</p> </li> <li> <p>\ud83c\udf10 EXPOSE - Expose a port to enable network access to the container</p> </li> </ul> <p>Sample Dockerfile for Deploying a Static website.</p> <p><pre><code>vim Dockerfile\n</code></pre> <pre><code># Use the CentOS 7 base image\nFROM centos:7\n\n# Set metadata labels\nLABEL \"Author\"=\"saiteja Irrinki\"\nLABEL \"Project\"=\"Wave\"\n\n# Install necessary packages - Apache, wget, and unzip\nRUN cd /etc/yum.repos.d/\nRUN sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*\nRUN sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*\nRUN  yum update -y\nRUN yum install httpd unzip wget -y\n\n# Download the website template\nRUN wget https://www.tooplate.com/zip-templates/2121_wave_cafe.zip\n\n# Unzip the downloaded template\nRUN unzip 2121_wave_cafe.zip\n\n# Copy the contents of the unzipped template to the web server directory\nRUN cp -r 2121_wave_cafe/* /var/www/html/\n\n# Start the Apache web server in the foreground\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]\n\n# Expose port 80 for web traffic\nEXPOSE 80\n\n# Set the working directory to the web server directory\nWORKDIR /var/www/html\n\n# Create a volume for Apache's log files\nVOLUME /var/log/httpd\n</code></pre></p> <p>Using Variables in Dockerfile for Deploying a Static website.</p> <p><pre><code>vim Dockerfile\n</code></pre> <pre><code># Define variables\nARG AUTHOR=\"saiteja Irrinki\"\nARG PROJECT=\"Highway\"\n# Use the variables\nFROM centos:7\nLABEL \"Author\"=\"$AUTHOR\"\nLABEL \"Project\"=\"$PROJECT\"\nRUN yum install httpd wget unzip -y\n\n# Use --no-check-certificate flag to bypass SSL certificate checks\nARG DOWNLOAD_URL=\"https://templatemo.com/tm-zip-files-2020/templatemo_520_highway.zip\"\nRUN wget \"$DOWNLOAD_URL\" --no-check-certificate\nARG FILE=\"templatemo_520_highway\"\nRUN unzip \"$FILE\".zip\nRUN cp -r \"$FILE\"/* /var/www/html/\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]\nEXPOSE 80\nWORKDIR /var/www/html\nVOLUME /var/log/httpd\n\n# Set the image name and optionally a tag\nLABEL \"name\"=\"$PROJECT\"\n</code></pre></p> <p><pre><code>vim Dockerfile\n</code></pre> Adding Existing zip/tar files in Dockerfile</p> <p>Docker expects the wave.tar.gz file to exist in the same directory where you are executing the docker build command. Docker will then copy the contents of that file into the /var/www/html/ directory within the Docker image being built. </p> <pre><code># Use the latest Ubuntu base image\nFROM ubuntu:latest\n\n# Set metadata labels for author and project\nLABEL \"Author\"=\"Saiteja Irrinki\"\nLABEL \"Project\"=\"Wave\"\n\n# Set environment variable to make the installation non-interactive\nENV DEBIAN_FRONTEND=noninteractive\n\n# Update package list and install Apache2 and Git\nRUN apt update &amp;&amp; apt install apache2 git -y\n\n# Add the contents of 'wave.tar.gz' to the Apache web server\n# You can use either 'COPY' or 'ADD'. Here, we're using 'ADD'.\nADD wave.tar.gz /var/www/html/\n\n# Start the Apache2 web server in the foreground\nCMD [\"/usr/sbin/apache2ctl\", \"-D\", \"FOREGROUND\"]\n\n# Expose port 80 for web traffic\nEXPOSE 80\n\n# Set the working directory to the Apache web server's document root\nWORKDIR /var/www/html\n\n# Create a volume for Apache2 log files\nVOLUME /var/log/apache2\n</code></pre> <p>Build the image from the Docker file \ud83d\uddbc\ufe0f</p> <p>Replace saitejairrinki/wavecafe:v1 with your preferred image name and tag.     <pre><code> docker build -t saitejairrinki/wavecafe:v1 . \n</code></pre></p> <p>Run Container From the Image \ud83d\udce6 <pre><code>docker run --name wavecafe -d -p 9699:80 saitejairrinki/wavecafe:v1\n</code></pre> Access it from your browser, ensuring that you have allowed port 9699 (or your chosen port) in your security group if you are using a cloud VM. \ud83c\udf10 <pre><code>Public-IPaddress:9699\n</code></pre></p> <p>Docker Image</p> <p>You can pull my image and start a container from it without needing to create a Dockerfile.</p> <pre><code>docker pull saitejairrinki/wavecafe:v1\n</code></pre> <p><pre><code>docker run --name wavecafe -d -p 9999:80 saitejairrinki/wavecafe:v1\n</code></pre> Now Access From the Browser, Make sure you have to allow the port number in my case 9999 in your security group if you are using cloud VM. <pre><code>Public-IPaddress:9999\n</code></pre></p>"},{"location":"devops/docker-setup/#docker-compose","title":"Docker Compose \ud83d\udcdf","text":"Single Docker File \ud83d\udcc4Multi Docker File \ud83d\udcd1Multiple Dockerfiles in Same Directory \ud83d\udcc2 <p>Creating Docker Compose for a single local Docker File</p> docker-compose.yml<pre><code>version: \"3\"\nservices:\n    Wavecafe:\n        build:\n            context: /Dockerfile_path/\n        ports:\n                - \"5555:80\"\n        container_name: wavecafe\n</code></pre> <p>Creating Docker Compose for a Multi local Docker File</p> docker-compose.yml<pre><code>version: '3'\n\nservices:\n  website1:\n    build:\n      context: /home   # Path to the first Dockerfile directory\n    ports:\n      - \"8081:80\"      # Map port 8081 on the host to port 80 in the container\n\n  website2:\n    build:\n      context: /tmp    # Path to the second Dockerfile directory\n    ports:\n      - \"8082:80\"      # Map port 8082 on the host to port 80 in the container\n</code></pre> <p>If you have multiple Dockerfiles in the same build context directory, you can use the dockerfile option to specify a different Dockerfile name.</p> docker-compose.yml<pre><code>version: '3'\n\nservices:\n  website1:\n    build:\n      context: ./website1\n      dockerfile: Dockerfile1\n    ports:\n      - \"8080:80\"  # Map host port 8080 to container port 80\n\n  website2:\n    build:\n      context: ./website2\n      dockerfile: Dockerfile2\n    ports:\n      - \"8081:80\"  # Map host port 8081 to container port 80\n</code></pre> <p>Creating Docker Compose for DockerHub Images</p> Single ImageMltiple Images docker-compose.yml<pre><code>version: '3'\nservices:\n  website:\n    image: saitejairrinki/wavecafe:v1\n    ports:\n      - \"8085:80\"\n</code></pre> docker-compose.yml<pre><code>version: '3'\nservices:\n  nginx:\n    image: nginx\n    ports:\n      - \"8085:80\"\n  wavecafe:\n    image: saitejairrinki/wavecafe:v1\n    ports:\n      - \"8086:80\"\n</code></pre> <p>Start all containers defined in the docker-compose.yml file <pre><code>docker-compose up\n</code></pre></p> <p>Start containers in the background (detached mode) <pre><code>docker-compose up -d\n</code></pre> Build or rebuild images and Start containers in the background (detached mode)  <pre><code>docker-compose up -d --build\n</code></pre></p> <p>Stop and remove all containers defined in the docker-compose.yml file <pre><code>docker-compose down\n</code></pre> Stop, remove containers, and remove volumes <pre><code>docker-compose down -v\n</code></pre></p> <p>List containers and their current status <pre><code>docker-compose ps\n</code></pre></p> <p>View logs of all containers defined in the docker-compose.yml file <pre><code>docker-compose logs\n</code></pre> Follow logs in real-time <pre><code>docker-compose logs -f\n</code></pre></p> <p>Build or rebuild Docker images for all services <pre><code>docker-compose build\n</code></pre></p> <p>Pull the latest images for all services <pre><code>docker-compose pull\n</code></pre></p> <p>Restart all containers <pre><code>docker-compose restart\n</code></pre></p> <p>Execute a command in a running container <pre><code>docker-compose exec &lt;service-name&gt; &lt;command&gt;\n</code></pre></p> <p>Stop all containers without removing them <pre><code>docker-compose stop\n</code></pre></p> <p>Start stopped containers <pre><code>docker-compose start\n</code></pre></p> <p>Stop and remove containers, networks, volumes, and associated Docker images <pre><code>docker-compose down --rmi all\n</code></pre></p> <p>Remove containers for services not defined in the docker-compose.yml file <pre><code>docker-compose down --remove-orphans\n</code></pre></p> Docker Volume \ud83d\udcbd <p>Creating a Separate Directory to Store Container data</p> <p><pre><code>mkdir mountbind\n</code></pre> Now link your Directory while running the container <pre><code>docker run --name db01 -e MYSQL_ROOT_PASSWORD=secret123 -p 3300:3306 -v /root/mountbind:/var/lib/mysql -d mysql:5.7\n</code></pre> Now do ls to the Directory there you can see the containers data <pre><code>ls mountbind\n</code></pre> Creating docker Volume, use the below command to see all the available options of docker volume <pre><code>docker volume --help\n</code></pre> Creating a new docker volume with name datadb <pre><code>docker volume create datadb\n</code></pre> Command removes all Docker volumes that are present on your system. <pre><code>docker volume rm $(docker volume ls -q)\n</code></pre> Now run your container with that volume <pre><code>docker run --name db02 -e MYSQL_ROOT_PASSWORD=secret123 -p 3301:3306 -v datadb:/var/lib/mysql -d mysql:5.7\n</code></pre> Now check  <pre><code>ls /var/lib/docker/volumes/datadb/_data/\n</code></pre> Now for testing Create any file with any name of your choice, here I'm creating a file with the name Milkyway <pre><code>touch /var/lib/docker/volumes/datadb/_data/milkyway\n</code></pre> Now log in into the container and Verify your file. <pre><code>docker exec -it db02 /bin/bash\n</code></pre> <pre><code>ls /var/lib/mysql/\n</code></pre> Now exit from the container <pre><code>exit\n</code></pre> If you want to access the MySQL database with a MySQL client then follow the below steps</p> <p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install mysql-client\n</code></pre> Now fetch the container IP by doing Docker Inspect <pre><code>docker inspect db02 |grep -i ipaddress\n</code></pre> Now Connect with that IP <pre><code>mysql -h 172.17.0.4 -u root -psecret123\n</code></pre></p>"},{"location":"devops/git/","title":"Git and GitHub: Empowering Collaborative Software Development","text":"<p>In the fast-paced world of software development, effective version control and collaboration are paramount. Enter Git and GitHub, the dynamic duo that has revolutionized how teams manage, track, and collaborate on code. In this blog, we'll explore the fundamentals of Git, the power of GitHub, and how they work together to empower collaborative software development.</p>"},{"location":"devops/git/#understanding-git","title":"Understanding Git","text":"<p>Git, created by Linus Torvalds in 2005, is a distributed version control system. It allows developers to track changes in their codebase, collaborate seamlessly, and easily manage different versions of their software.</p>"},{"location":"devops/git/#key-concepts","title":"Key Concepts","text":"<p>Repository (Repo): A Git repository is like a project's folder, containing all the files, history, and configurations related to the project. It can exist locally on your machine or remotely on a server.</p> <p>Commit : A commit is a snapshot of your code at a specific point in time. It represents a set of changes that you want to save.</p> <p>Branch: Branches are parallel lines of development. They allow you to work on different features or bug fixes independently.</p> <p>Merge: Merging combines the changes from one branch into another. It's often used to integrate completed features or fixes back into the main codebase.</p> <p>Pull Request (PR): A pull request is a request to merge changes from one branch into another. It's a common practice for code review and collaboration in open source and team projects.</p>"},{"location":"devops/git/#basic-git-workflow","title":"Basic Git Workflow","text":"<p>Here's a simplified Git workflow:</p> <ol> <li> <p>Initialize a Repository: Start a Git repository in your project folder with <code>git init</code>.</p> </li> <li> <p>Stage Changes: Use <code>git add</code> to stage changes you want to commit.</p> </li> <li> <p>Commit Changes: Commit your staged changes with <code>git commit -m \"Your commit message\"</code>.</p> </li> <li> <p>Create Branches: Create branches with <code>git branch branch-name</code>.</p> </li> <li> <p>Switch Branches: Move between branches with <code>git checkout branch-name</code>.</p> </li> <li> <p>Merge Branches: Merge branches with <code>git merge branch-name</code>.</p> </li> <li> <p>Collaborate: Push your changes to a remote repository and collaborate with others.</p> </li> </ol>"},{"location":"devops/git/#git-command-reference","title":"Git Command Reference","text":"<p>While we've covered the basics of Git here, you can dive deeper by referring to the Atlassian Git Cheatsheet. This resource provides a comprehensive list of Git commands and their usage.</p>"},{"location":"devops/git/#securely-adding-ssh-keys-to-github","title":"Securely Adding SSH Keys to GitHub \ud83d\udee1\ufe0f","text":""},{"location":"devops/git/#adding-ssh-keys-to-your-github-account","title":"Adding SSH Keys to Your GitHub Account\ud83c\udf10","text":"<ul> <li> <p>Description: GitHub allows SSH key authentication for secure repository access. Adding your public key to your GitHub account grants you secure access to repositories.</p> </li> <li> <p>Steps:</p> </li> <li>Generate an SSH key pair using <code>ssh-keygen</code> if you haven't already.</li> <li>Copy your public key to the clipboard:      <pre><code>cat ~/.ssh/id_rsa.pub | xclip -selection clipboard   # On Linux\n</code></pre></li> <li>Log in to your GitHub account.</li> <li>Go to Settings &gt; SSH and GPG keys.</li> <li>Click New SSH key.</li> <li>Paste your public key into the Key field and give it a meaningful title.</li> <li> <p>Click Add SSH key.</p> </li> <li> <p>Use Case: Adding SSH keys to GitHub simplifies secure access to your repositories without the need for a password.</p> </li> </ul>"},{"location":"devops/git/#continuous-integration-and-continuous-deployment-cicd-with-github-actions","title":"Continuous Integration and Continuous Deployment (CI/CD) with GitHub Actions","text":"<p>GitHub Actions is a powerful automation and CI/CD platform integrated with GitHub repositories. It allows you to build, test, and deploy your code directly from your GitHub repository.</p>"},{"location":"devops/git/#components-of-github-actions","title":"Components of GitHub Actions:","text":"<ul> <li>Workflows \ud83d\udd04</li> <li>Jobs \ud83d\udee0\ufe0f</li> <li>Runners \ud83c\udfc3</li> <li>Actions \ud83e\udd16</li> <li>Artifacts \ud83d\udce6</li> </ul>"},{"location":"devops/git/#real-time-use-case","title":"Real-Time Use Case:","text":"<p>GitHub Actions is best suited for developers and teams using GitHub for version control. It seamlessly integrates with your repositories and is an excellent choice for projects hosted on GitHub.</p>"},{"location":"devops/git/#mostly-used-for","title":"Mostly Used For:","text":"<ul> <li>Continuous Integration (CI)</li> <li>Continuous Deployment (CD)</li> <li>Automated Testing</li> <li>Workflow Automation</li> </ul>"},{"location":"devops/git/#understanding-github-actions-workflows","title":"Understanding GitHub Actions Workflows","text":"<p>Workflows in GitHub Actions are defined YAML files that specify the automation process for your project. A workflow can include one or more jobs, each consisting of a series of steps and actions to execute.</p>"},{"location":"devops/git/#github-actions-runner-types","title":"GitHub Actions Runner Types","text":"<p>GitHub Actions allows you to use different types of runners to execute your workflows:</p> <ul> <li>GitHub-hosted runners are provided by GitHub and offer a variety of pre-configured environments.</li> <li>Self-hosted runners are runners you can set up and maintain in your own infrastructure, giving you more control over the execution environment.</li> </ul>"},{"location":"devops/git/#installation-process","title":"Installation Process:","text":"<ol> <li>Access your GitHub repository.</li> <li>Navigate to the \"Actions\" tab.</li> <li>Create a new workflow or choose from existing templates.</li> <li>Define your workflow steps, including build and deployment tasks.</li> <li>Save your workflow configuration in a YAML file.</li> <li>Trigger your workflow manually or automatically based on events like code pushes or pull requests.</li> </ol>"},{"location":"devops/git/#creating-a-basic-github-actions-workflow","title":"Creating a Basic GitHub Actions Workflow","text":""},{"location":"devops/git/#yaml-method","title":"YAML Method:","text":"<pre><code>name: CI/CD with GitHub Actions\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout Repository\n      uses: actions/checkout@v2\n\n    - name: Print Hello World\n      run: echo 'Hello, World!'\n</code></pre> <p>To create a basic GitHub Actions workflow using YAML configuration:</p> <ol> <li>Access your GitHub repository.</li> <li>Navigate to the \"Actions\" tab.</li> <li>Click on \"Set up a workflow yourself\" to create a new YAML file.</li> <li>Copy and paste the provided YAML configuration.</li> <li>Save the file as <code>.github/workflows/main.yml</code>.</li> <li>Trigger your workflow manually or by pushing code changes.</li> </ol> <p>This workflow will execute and print \"Hello, World!\" when triggered by a code push.</p>"},{"location":"devops/git/#more-workflows","title":"More Workflows","text":"GitHub Auto-Push Workflow <p>This GitHub Actions workflow is designed to automate the process of pushing changes to a GitHub repository's main branch</p> <pre><code>name: Push Changes to GitHub\n\n# Define the trigger for this workflow: it runs when there are pushes to the specified branch (main).\non:\n  push:\n    branches:\n      - main  # Replace with the branch you want to trigger this on\n\njobs:\n  build:\n    # Specify that this job runs on a self-hosted runner.\n    runs-on: self-hosted\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        # Disable persisting credentials to use a personal access token (PAC) instead of the GITHUB_TOKEN.\n        persist-credentials: false\n\n        # Set the fetch depth to 0 to ensure the full history is fetched, avoiding errors when pushing refs.\n        fetch-depth: 0\n\n    - name: Create local changes\n      run: |\n        # Execute the 'free -m' command and redirect the output to a file named 'memory.txt'.\n        free -m &gt; memory.txt\n\n    - name: Commit files\n      run: |\n        # Add all changes to the Git staging area.\n        git add .\n\n        # Configure the user email and name for this local Git session.\n        git config --local user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n        git config --local user.name \"github-actions[bot]\"\n\n        # Commit all changes with the message \"Add changes [skip ci]\".\n        git commit -a -m \"Add changes [skip ci]\"\n\n    - name: Push changes\n      uses: ad-m/github-push-action@master\n      with:\n        # Provide the GitHub token (PAC) stored in GitHub secrets for authentication.\n        github_token: ${{ secrets.PAC }}\n\n        # Specify the target branch for pushing the changes (main).\n        branch: main\n</code></pre>"},{"location":"devops/git/#automated-github-website-deployment","title":"Automated GitHub Website Deployment","text":"<p>This script automates the process of setting up a website by downloading and extracting files from a URL, adding them to a Git repository, committing the changes, and pushing them to a remote Git repository. It also includes package installation and cleanup steps in a Linux Ubuntu OS.</p> <pre><code>#!/bin/bash\n# Website setup script\n\n# Define variables for URLs, file names, and repository information.\nURL=https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nFILE=2118_chilling_cafe\nGIT_URL=git@github.com:username/username.github.io.git\nGIT_REPO=username.github.io #use your github username \nPKG=apt\nEMAIL=admin123@gmail.com\nUSER=admin\n\n# Install necessary packages (e.g., git, wget, unzip) using the package manager specified in PKG.\nsudo $PKG install git wget unzip -y\n\n# Create a directory structure for Git and navigate to it.\nmkdir -p ~/git\ncd ~/git\n\n# Clone the specified Git repository.\ngit clone $GIT_URL\n\n# Remove the contents of the Git repository directory.\nrm -rf $GIT_REPO/*\n\n# Download a file from the specified URL and unzip it.\nwget $URL\nunzip $FILE.zip\n\n# Copy the unzipped files to the Git repository directory.\ncp -r $FILE/* $GIT_REPO/\n\n# Navigate to the Git repository directory.\ncd $GIT_REPO\n\n# Configure Git user email and name globally.\ngit config --global user.email \"$EMAIL\"\ngit config --global user.name \"$USER\"\n\n# Add all changes, commit with the current date, and push to the remote repository.\ngit add .\ngit commit -m \"$(date)\"\ngit push\n\n# Remove the temporary git directory.\nrm -rf ~/git\n</code></pre>"},{"location":"devops/helm/","title":"Helm","text":"<p>\u2699 Introducing Helm: Sailing Smoothly Through Kubernetes Deployments \ud83d\ude80</p> <p>Are you diving into the world of Kubernetes deployments and feeling a bit lost at sea? Don't worry, Helm is here to rescue you! Let's sail through Helm and make your Kubernetes journey smoother.</p>"},{"location":"devops/helm/#what-is-helm","title":"\ud83e\udd14 What is Helm?","text":"<p>Helm is a package manager for Kubernetes applications, designed to simplify the process of deploying, managing, and scaling containerized applications. Think of it as your go-to tool for streamlining the management of complex Kubernetes resources.</p>"},{"location":"devops/helm/#helm-structure","title":"\ud83c\udfd7\ufe0f Helm Structure:","text":"<p>The Helm structure consists of:</p> <ol> <li>Charts: Bundles of pre-configured Kubernetes resources that define the structure of an application.</li> <li>Templates: Dynamic YAML files within charts, allowing parameterization and customization.</li> <li>Values: Configuration options that can be customized during deployment.</li> <li>Charts Repository: A centralized location for sharing and discovering Helm charts.</li> </ol>"},{"location":"devops/helm/#components-in-a-helm-chart","title":"\ud83e\udde9 Components in a Helm Chart:","text":"<p>When you crack open a Helm chart, you'll find several key components:</p> <pre><code>my-chart/\n\u251c\u2500\u2500 charts/\n\u251c\u2500\u2500 templates/\n\u2502 \u251c\u2500\u2500 NOTES.txt\n\u2502 \u251c\u2500\u2500 deployment.yaml\n\u2502 \u251c\u2500\u2500 service.yaml\n\u2502 \u2514\u2500\u2500 ... (other k8s manifest files)\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 values.yaml\n\u2514\u2500\u2500 README.md\n</code></pre> <ol> <li>Chart.yaml: Metadata about the chart, including name, version, and dependencies.</li> <li>Templates: Kubernetes manifest files with placeholders for dynamic values.</li> <li>Values.yaml: Default configuration values for the chart.</li> <li>Charts: Subcharts or dependencies required by the main chart.</li> <li>README.md: Documentation providing instructions and guidance on using the chart.</li> <li>NOTES.txt: Helpful information and post-installation notes for users.</li> </ol>"},{"location":"devops/helm/#helm-basic-commands","title":"\u2699\ufe0f Helm Basic Commands:","text":"<p>Getting started with Helm is a breeze thanks to its intuitive command-line interface. Here are some essential commands:</p> <ul> <li><code>helm create &lt;chart_name&gt;</code>: Create a new chart.</li> <li><code>helm install &lt;release_name&gt; &lt;chart_name&gt;</code>: Install a chart.</li> <li><code>helm upgrade &lt;release_name&gt; &lt;chart_name&gt;</code>: Upgrade a deployed release.</li> <li><code>helm list</code>: List deployed releases.</li> <li><code>helm uninstall &lt;release_name&gt;</code>: Uninstall a release.</li> </ul>"},{"location":"devops/helm/#sample-helm-project-files","title":"\ud83d\ude80 Sample Helm Project Files:","text":"<p>Now, let's dive into a sample Helm project to see how these concepts come together:</p> K8s Deployment FileK8s Service FileCharts FileValues File templates/deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.deploymentname }}  # \ud83d\ude80 Name of the Deployment\nspec:\n  replicas: 1  # \ud83d\udeb6 Number of desired replicas\n  selector:\n    matchLabels:\n      app: {{ .Values.appname }}  # \ud83c\udff7\ufe0f Selector to match pods with the label \"app: wavecafe\"\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.appname }}  # \ud83c\udff7\ufe0f Labels applied to pods created by this template\n    spec:\n      containers:\n        - name: my-app-container  # \ud83d\udce6 Name of the container\n          image: \"{{ .Values.image.name }}:{{ .Values.image.tag }}\"  # \ud83d\udc33 Docker image to use\n          ports:\n            - name: cafe-port  # \ud83c\udf10 Name of the port\n              containerPort: 80  # \ud83d\udeaa Port that the container listens on\n</code></pre> templates/service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: wave-cafe  # \u2615 Name of the Service\nspec:\n  selector:\n    app: {{ .Values.appname }}  # \ud83c\udff7\ufe0f Select pods with the label \"app: wavecafe\"\n  ports:\n    - protocol: TCP  # \ud83c\udf10 Protocol for the port\n      port: 80  # \ud83d\udeaa Port on the Service\n      targetPort: cafe-port  # \ud83d\ude80 Port on the pods to forward traffic to\n  type: LoadBalancer\n</code></pre> Chart.yaml<pre><code>apiVersion: v2\nname: wavecafe\ndescription: A Helm chart for Kubernetes\n\ntype: application\n\nversion: 0.1.0\n\nappVersion: \"1.16.0\"\n</code></pre> values.yaml<pre><code>appname: wavecafe\ndeploymentname: wavecafe\nservicename: wave-cafe\n\nimage:\n  name: saitejairrinki/wavecafe\n  tag: v1\n</code></pre> Additional Features <p>In this Helm repository, I have added Horizontal Pod Autoscaling (HPA) and Kubernetes probes to ensure high availability and improved reliability.</p>"},{"location":"devops/helm/#step-by-step-guide","title":"Step-by-Step Guide:","text":"<ol> <li> <p>Create a Chart: Start by creating a Helm chart named <code>wavecafe</code> using the command:    <pre><code>helm create wavecafe\n</code></pre></p> </li> <li> <p>Delete Templates: Next, delete all the files in the <code>templates</code> folder created by Helm:    <pre><code>rm wavecafe/templates/*\n</code></pre></p> </li> <li> <p>Update YAML Files: Update the <code>deployment.yaml</code> and <code>service.yaml</code> files with the above provided content in templates folder. wrt <code>values.yaml</code> file.</p> </li> <li> <p>Install the Chart: Deploy your application using Helm with the following command:    <pre><code>helm install my-release wavecafe/ --values wavecafe/values.yaml\n</code></pre></p> </li> <li> <p>Upgrade (Optional): Make changes to your application and upgrade the release:    <pre><code>helm upgrade my-release wavecafe/ --values wavecafe/values.yaml\n</code></pre></p> </li> <li> <p>Uninstall: When you're done, uninstall the release:    <pre><code>helm uninstall my-release\n</code></pre></p> </li> </ol>"},{"location":"devops/helm/#wrapping-up","title":"Wrapping Up:","text":"<p>With Helm as your guiding star, Kubernetes deployments become smoother and more manageable than ever before. Bon voyage on your Kubernetes journey!</p> <p>Happy Helming! \u2693\ud83c\udf1f</p>"},{"location":"devops/jenkins/","title":"Jenkins","text":"<p>Jenkins is a continuous integration tool. It can fetch the code from the version control system, build the code, test it and notify the developer. Jenkins can do continuous delivery also. Jenkins has so many plugins, by using these plugins we can do any task in Jenkins. Jenkins is an open sources project. It is a java based web application server so we need to set up first java on the machine to run the jenkins server.</p>"},{"location":"devops/jenkins/#jenkins-ci-cd-automation","title":"Jenkins CI CD Automation","text":""},{"location":"devops/jenkins/#jenkins-setup","title":"Jenkins Setup","text":"UbuntuCentOS <pre><code> #!/bin/bash\necho \"Installing Java 17\"\nsudo apt update\nsudo apt install fontconfig openjdk-17-jre -y\njava -version\n\necho \"Installing Jenkins\"\nsudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\\n  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install jenkins -y\n</code></pre> <pre><code>#!/bin/bash\nsudo wget -O /etc/yum.repos.d/jenkins.repo \\\n    https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key\nsudo yum upgrade -y\n# Add required dependencies for the jenkins package\nsudo yum install fontconfig java-11-openjdk -y\nsudo yum install jenkins -y\n</code></pre>"},{"location":"devops/jenkins/#cicd-setup","title":"CICD Setup","text":"<p>You can find my Jenkins CICD pipeline jobs which are saved in an HTML format, please download and extract to see the jobs </p> <p>To View My Jenkins CICD Jobs Click here</p>"},{"location":"devops/networking/","title":"Networking Basics You Should Know as a DevOps Engineer \ud83c\udf10\ud83d\ude80","text":""},{"location":"devops/networking/#what-is-an-ip","title":"What is an IP? \ud83c\udf0d","text":"<p>An IP (Internet Protocol) address is like your device's home address on the internet. It helps in identifying and locating devices connected to a network.</p>"},{"location":"devops/networking/#clearing-the-confusion-between-public-and-private-ip-or-network","title":"Clearing the confusion between Public and Private IP or Network \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udd0d","text":""},{"location":"devops/networking/#private-ip-definition","title":"Private IP: Definition \ud83c\udfe1","text":"<p>A Private IP address is used within a private network. These addresses are not routable on the internet, meaning they only work within the local network (LAN).</p>"},{"location":"devops/networking/#private-ip-ranges","title":"Private IP Ranges \ud83d\udccf","text":"<ul> <li><code>10.0.0.0</code>     to  <code>10.255.255.255</code>  - CLASS A</li> <li><code>172.16.0.0</code>   to  <code>172.31.255.255</code>  - CLASS B</li> <li><code>192.168.0.0</code>  to  <code>192.168.255.255</code> - CLASS C</li> </ul>"},{"location":"devops/networking/#public-ip-definition","title":"Public IP: Definition \ud83c\udf10","text":"<p>A Public IP address is used to identify devices on the internet. These addresses are routable on the global internet. </p>"},{"location":"devops/networking/#public-ip-example","title":"Public IP Example \ud83c\udf0d","text":"<p>Any IP address that is not in the private IP ranges mentioned above is a public IP address. For example, <code>8.8.8.8</code> (Google's DNS).</p>"},{"location":"devops/networking/#what-is-an-invalid-ip","title":"What is an Invalid IP? \u274c","text":"<p>An Invalid IP address is one that does not conform to the rules of IP address allocation. For instance, an IP address where any octet exceeds <code>255</code> is invalid.</p>"},{"location":"devops/networking/#explanation-crossing-8-bits","title":"Explanation: Crossing 8 Bits \ud83d\udca1","text":"<p>IP addresses are divided into four octets, each ranging from 0 to 255 (8 bits). If any octet crosses 255, it's invalid.</p>"},{"location":"devops/networking/#invalid-ip-examples","title":"Invalid IP Examples \u274c","text":"<ul> <li><code>256.100.50.25</code></li> <li><code>192.300.1.1</code></li> </ul>"},{"location":"devops/networking/#what-is-a-subnet","title":"What is a Subnet? \ud83c\udf10\u2797","text":"<p>A Subnet (Subnetwork) divides a larger network into smaller, more manageable pieces. It improves network performance and security.</p>"},{"location":"devops/networking/#what-is-a-public-network","title":"What is a Public Network? \ud83c\udf10","text":"<p>A Public Network is a network where devices are connected to the internet and can communicate with other devices globally.</p>"},{"location":"devops/networking/#what-is-a-private-network","title":"What is a Private Network? \ud83c\udfe1","text":"<p>A Private Network is a network where devices are connected within a limited scope, like a home or office, and cannot be accessed from the global internet.</p>"},{"location":"devops/networking/#difference-between-public-and-private-networks-vs","title":"Difference between Public and Private Networks \ud83c\udf10 vs. \ud83c\udfe1","text":"<ul> <li>Public Network: Global, accessible via the internet, less secure.</li> <li>Private Network: Local, not accessible via the internet, more secure.</li> </ul>"},{"location":"devops/networking/#what-is-vpn","title":"What is VPN? \ud83d\udee1\ufe0f","text":"<p>A VPN (Virtual Private Network) creates a secure, encrypted connection over a less secure network, like the internet. It ensures privacy and security. </p>"},{"location":"devops/networking/#what-is-a-bastion-host","title":"What is a Bastion Host? \ud83d\udee1\ufe0f\ud83d\udda5\ufe0f","text":"<p>A Bastion Host is a special-purpose computer on a network specifically designed to withstand attacks. It typically acts as a gateway between a public and private network.</p>"},{"location":"devops/networking/#what-is-vpn-peering","title":"What is VPN Peering? \ud83d\udd17\ud83d\udee1\ufe0f","text":"<p>VPN Peering connects two VPN networks, allowing secure communication between them. It extends the private network across the internet securely.</p>"},{"location":"devops/networking/#calculating-cidr-for-an-ip-address","title":"Calculating CIDR for an IP Address \ud83d\udccf\ud83d\udd0d","text":"<p>CIDR (Classless Inter-Domain Routing) is a method for allocating IP addresses and IP routing. A CIDR notation consists of an IP address followed by a slash (/) and a number (prefix length). This number represents the number of significant bits in the subnet mask.</p>"},{"location":"devops/networking/#steps-to-calculate-cidr-for-an-ip-address","title":"Steps to Calculate CIDR for an IP Address","text":"<ol> <li>Identify the IP Address and Subnet Mask \ud83d\udcdd</li> <li>Example: IP Address: 192.168.1.10</li> <li> <p>Subnet Mask: 255.255.255.0</p> </li> <li> <p>Convert the Subnet Mask to Binary \ud83d\udd22</p> </li> <li>Subnet Mask: 255.255.255.0</li> <li> <p>Binary: 11111111.11111111.11111111.00000000</p> </li> <li> <p>Count the Number of '1' Bits in the Binary Subnet Mask \ud83d\udd0d</p> </li> <li> <p>11111111.11111111.11111111.00000000 has 24 '1' bits.</p> </li> <li> <p>Write the CIDR Notation \ud83d\udd8a\ufe0f</p> </li> <li>The CIDR notation is the IP address followed by a slash and the number of '1' bits in the subnet mask.</li> <li>Example: 192.168.1.10/24</li> </ol>"},{"location":"devops/networking/#example-calculation-with-detailed-steps","title":"Example Calculation with Detailed Steps","text":""},{"location":"devops/networking/#step-1-identify-the-ip-address-and-subnet-mask","title":"Step 1: Identify the IP Address and Subnet Mask","text":"<ul> <li>IP Address: 192.168.1.10</li> <li>Subnet Mask: 255.255.255.0</li> </ul>"},{"location":"devops/networking/#step-2-convert-the-subnet-mask-to-binary","title":"Step 2: Convert the Subnet Mask to Binary","text":"<ul> <li>Decimal to Binary:</li> <li>255 -&gt; 11111111</li> <li>255 -&gt; 11111111</li> <li>255 -&gt; 11111111</li> <li>0 -&gt; 00000000</li> <li>Combined Binary: 11111111.11111111.11111111.00000000</li> </ul>"},{"location":"devops/networking/#step-3-count-the-number-of-1-bits","title":"Step 3: Count the Number of '1' Bits","text":"<ul> <li>Count of '1' bits: 24</li> </ul>"},{"location":"devops/networking/#step-4-write-the-cidr-notation","title":"Step 4: Write the CIDR Notation","text":"<ul> <li>CIDR Notation: 192.168.1.10/24</li> </ul>"},{"location":"devops/networking/#choosing-a-subnet-mask","title":"Choosing a Subnet Mask","text":"<p>The choice of subnet mask depends on the size and structure of the network. Here are a few considerations:</p> <ul> <li>Small Network: Use a subnet mask like 255.255.255.0 (/24) for small networks where you need up to 254 hosts.</li> <li>Medium Network: Use a subnet mask like 255.255.0.0 (/16) for medium-sized networks where you need up to 65,534 hosts.</li> <li>Large Network: Use a subnet mask like 255.0.0.0 (/8) for large networks where you need up to 16,777,214 hosts.</li> </ul>"},{"location":"devops/networking/#example-changing-subnet-masks-for-different-needs","title":"Example: Changing Subnet Masks for Different Needs","text":""},{"location":"devops/networking/#example-1-small-office-network","title":"Example 1: Small Office Network","text":"<ul> <li>IP Address: 192.168.1.0</li> <li>Subnet Mask: 255.255.255.0</li> <li>CIDR Notation: 192.168.1.0/24</li> <li>Number of Hosts: 254</li> </ul>"},{"location":"devops/networking/#example-2-large-corporate-network","title":"Example 2: Large Corporate Network","text":"<ul> <li>IP Address: 10.0.0.0</li> <li>Subnet Mask: 255.0.0.0</li> <li>CIDR Notation: 10.0.0.0/8</li> <li>Number of Hosts: 16,777,214</li> </ul> <p>By choosing the appropriate subnet mask, you can efficiently allocate IP addresses and design networks that meet the specific needs of your environment.</p> Conclusion <p>Understanding these networking basics will help you navigate the complexities of network configurations and enhance your DevOps skills! \ud83c\udf1f\ud83d\ude80</p>"},{"location":"devops/shellscripting/","title":"Shell Scripting","text":""},{"location":"devops/shellscripting/#if-condition","title":"If Condition","text":"If ConditionIf Else ConditionElif ConditionExample <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nelse\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nelif [ &lt;condition&gt; ]\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nelse\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nfi\n</code></pre> <pre><code>#!/bin/bash\n\napt --help &amp;&gt;&gt; /dev/null\n\nif [ $? -eq 0 ]\n\nthen\n     echo \" This is Ubuntu Operating System\"\nelse\n     echo \" This is CentOS Operating System\"\nfi\n</code></pre> Reference Link <p>How to program with Bash</p> <p>https://opensource.com/article/19/10/programming-bash-logical-operators-shell-expansions</p>"},{"location":"devops/shellscripting/#for-loop","title":"For Loop","text":"For LoopExample <pre><code>#!/bin/bash\n\nfor &lt;variable&gt; in &lt;list&gt;\ndo\n   &lt;command&gt;\ndone\n</code></pre> <pre><code>#For loop example for Creating Files with name alpha beta gamma\n\n#!/bin/bash\n\nfor FILE in alpha beta gamma \ndo\n   echo \"Creating file $FILE in system\"\n\n   sudo touch $FILE\n\ndone\n</code></pre>"},{"location":"devops/shellscripting/#while-loop","title":"While Loop","text":"While LoopWhile Loop Example <pre><code>#!/bin/bash\nwhile [ &lt;condition&gt; ]\ndo\n   &lt;command&gt;\ndone\n</code></pre> <pre><code>#!/bin/bash\n#Here variable \"a\" is speed\n\na=0\necho \"Starting the Engine\"\n\nwhile [ $a -le 100 ]\ndo\n    sleep 1\n    echo \"Current Speed $a\"\n\n    a=$(($a+10))\n\ndone\n</code></pre>"},{"location":"devops/shellscripting/#functions","title":"Functions","text":"Empty String  <p>Certainly! Here's an example of how you can create a Bash script with a main function to call the <code>check_empty_string</code> function:</p> <pre><code>#!/bin/bash\n\ncheck_empty_string() {\n    if [ -z \"$1\" ]; then\n        echo \"Error: Input string is empty!\"\n    else\n        echo \"Input string is not empty.\"\n    fi\n}\n\nmain() {\n    input=\"example\"\n    check_empty_string \"$input\"\n}\n\n# Call the main function\nmain\n</code></pre> <p>In this script, the <code>main</code> function is defined to call the <code>check_empty_string</code> function with the <code>input</code> variable. You can replace <code>\"example\"</code> with any input string you want to test. When you run the script, it will execute the <code>main</code> function, which in turn calls the <code>check_empty_string</code> function.</p> <p>Sure, here are a few basic reusable Bash functions along with their use cases and examples:</p> greetis_evenfile_infoSumreverse <p>Use Case: A function to greet a person.</p> <pre><code>greet() {\n    echo \"Hello, $1!\"\n}\n\n# Example usage\ngreet \"Alice\"\ngreet \"Bob\"\n</code></pre> <p>Use Case: A function to check if a number is even.</p> <pre><code>is_even() {\n    if (( $1 % 2 == 0 )); then\n        echo \"Even\"\n    else\n        echo \"Odd\"\n    fi\n}\n\n# Example usage\nis_even 4  # Output: Even\nis_even 7  # Output: Odd\n</code></pre> <p>Use Case: A function to display information about a file.</p> <pre><code>file_info() {\n    echo \"File: $1\"\n    echo \"Size: $(du -h \"$1\" | cut -f1)\"\n    echo \"Owner: $(stat -c %U \"$1\")\"\n}\n\n# Example usage\nfile_info \"/path/to/somefile.txt\"\n</code></pre> <p>Use Case: A function to calculate the sum of two numbers.</p> <pre><code>sum() {\n    echo \"$(($1 + $2))\"\n}\n\n# Example usage\nresult=$(sum 5 3)\necho \"Sum: $result\"  # Output: Sum: 8\n</code></pre> <p>Use Case: A function to reverse a string.</p> <pre><code>reverse() {\n    echo \"$1\" | rev\n}\n\n# Example usage\nreversed=$(reverse \"hello\")\necho \"Reversed: $reversed\"  # Output: Reversed: olleh\n</code></pre>"},{"location":"devops/shellscripting/#shell-script-for-setting-up-website","title":"Shell Script For Setting Up Website","text":"UbuntuCentosUsing VariablesUsing If Else Condition <pre><code>#!/bin/bash\nsudo apt update\nsudo apt install wget net-tools unzip figlet apache2 -y\nsudo systemctl start apache2\nsudo systemctl enable apache2\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip  \nsudo unzip -o 2118_chilling_cafe.zip\nsudo rm -rf /var/www/html/*\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\ncd ..\nsudo rm -rf webfiles\nsudo systemctl restart apache2\nfiglet done\n</code></pre> <pre><code>#!/bin/bash\ncd /etc/yum.repos.d/\nsudo sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*\nsudo sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*\nsudo yum update -y\nsudo yum install wget net-tools unzip httpd -y\nsudo systemctl start httpd\nsudo systemctl enable httpd\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nsudo unzip -o 2118_chilling_cafe.zip\nsudo rm -rf /var/www/html/*\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\ncd ..\nsudo rm -rf webfiles\nsudo systemctl restart httpd\n</code></pre> <pre><code>#!/bin/bash\n#Website setup\n#Adding variables :-)\nURL=https://templatemo.com/tm-zip-files-2020/templatemo_520_highway.zip\nSRV=httpd\nPKG=yum\nFILE=templatemo_520_highway\necho \"  Installing the Services &amp; Extractors\"\necho\nsudo $PKG install $SRV wget unzip -y &amp;&gt;&gt; /dev/null\necho \"Start &amp; Enabling the Services\"\necho\nsudo systemctl start $SRV\nsudo systemctl enable $SRV\necho \"Downloading the zip file from tooplate.com\"\necho\nmkdir -p webfiles\ncd webfiles\necho\nsudo wget $URL &amp;&gt;&gt; /dev/null\necho \"extracting the files \"\necho\nsudo unzip -o $FILE.zip &amp;&gt;&gt; /dev/null\necho \"copying the extracted file into html\"\necho\necho \"Cleaning files in html Directory\"\nsudo rm -rf /var/www/html/*\necho\nsudo cp -r $FILE/* /var/www/html &amp;&gt;&gt; /dev/null\necho \"Restarting the Services\"\nsudo systemctl restart $SRV\ncd ..\nsudo rm -rf webfiles\nsudo systemctl status $SRV | grep Active\ndate\n</code></pre> <pre><code>#!/bin/bash\napt --help &amp;&gt;&gt; /dev/null\nif [ $? -eq 0 ]\nthen \n   sudo apt update\n   sudo apt install wget figlet net-tools unzip apache2 -y\n   sudo systemctl start apache2\n   sudo systemctl enable apache2\n   mkdir -p webfiles\n   cd webfiles\n   sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\n   sudo unzip -o 2118_chilling_cafe.zip\n   sudo rm -rf /var/www/html/*\n   sudo cp -r 2118_chilling_cafe/* /var/www/html/\n   cd ..\n   sudo rm -rf webfiles\n   sudo systemctl restart apache2\n   sudo systemctl status apache2 | grep Active\n   figlet done\nelse\n    sudo yum install wget net-tools unzip httpd -y\n    sudo systemctl start httpd\n    sudo systemctl enable httpd\n    sudo mkdir -p webfiles\n    cd webfiles\n    sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip \n    sudo unzip -o 2118_chilling_cafe.zip\n    sudo rm -rf /var/www/html/*\n    sudo cp -r 2118_chilling_cafe/* /var/www/html/\n    cd ..    \n    sudo rm -rf webfiles\n    sudo systemctl restart httpd\n    sudo systemctl status httpd | grep Active\nfi\n</code></pre> <p>Info</p> <p>Feel free to explore further templates on tooplate.com.</p>"},{"location":"devops/terraform/","title":"Terraform","text":"<p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can help with multi-cloud by having one workflow for all clouds. The infrastructure Terraform manages can be hosted on public clouds like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, or on-prem in private clouds such as VMWare vSphere.</p>"},{"location":"devops/terraform/#terraform-installation","title":"Terraform Installation \ud83d\udce5","text":"Ubuntu \ud83d\udc27CentOS \ud83d\udc27 <pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\n</code></pre> <pre><code>sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\n</code></pre> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install terraform\n</code></pre> <pre><code>sudo yum install -y yum-utils\n</code></pre> <pre><code>sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\n</code></pre> <pre><code>sudo yum -y install terraform\n</code></pre>"},{"location":"devops/terraform/#reference","title":"Reference \ud83d\udccc","text":"Terraform Registry <p>https://registry.terraform.io/</p> Terraform Commands Cheat sheet Reference Link <p>https://acloudguru.com/blog/engineering/the-ultimate-terraform-cheatsheet</p>"},{"location":"devops/terraform/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like Ec2 Instances, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., Ec2 Full Access, S3 Full Access). Note down the user's access key ID and secret access key. \ud83d\udccc Reference Document Link</p> </li> </ol> <p>Warning</p> <p>Utilize Role-Based Authentication when working with Terraform on AWS Instances or Services, as it offers a higher level of security compared to using access keys.</p>"},{"location":"devops/terraform/#terraform-sample-scripts","title":"Terraform Sample Scripts","text":"<p>Provisioning an AWS EC2 Instance </p> Utilizing an existing key and security groupProvisioning with New Key and Security Group main.tf<pre><code>provider \"aws\" {\n  region = \"us-east-2\"\n}\n\nresource \"aws_instance\" \"Instance\" {\n  ami                    = \"ami-0fb653ca2d3203ac1\"\n  instance_type          = \"t2.micro\"\n  key_name               = \"terraform\"\n  vpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n  tags = {\n    Name = \"IAAC\"\n    Team = \"DevOps\"\n  }\n}\n</code></pre> main.tf<pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_key_pair\" \"example_keypair\" {\n  key_name   = \"satrun\"\n  public_key = tls_private_key.example_keypair.public_key_openssh\n}\n\nresource \"tls_private_key\" \"example_keypair\" {\n  algorithm = \"RSA\"\n}\n\nresource \"local_file\" \"private_key_file\" {\n  content  = tls_private_key.example_keypair.private_key_pem\n  filename = \"satrun.pem\"\n}\n\nresource \"aws_security_group\" \"example_security_group\" {\n  name        = \"satrun-security-group\"\n  description = \"Example Security Group\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"YOUR PUBLIC IP/32\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"YOUR PUBLIC IP/32\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"example_instance\" {\n  ami                = \"ami-09e67e426f25ce0d7\" # Ubuntu 20.04 LTS in us-east-1 (N. Virginia) region\n  instance_type      = \"t2.micro\"\n  key_name           = aws_key_pair.example_keypair.key_name\n  vpc_security_group_ids = [aws_security_group.example_security_group.id]\n\n  tags = {\n    Name = \"satrun-instance\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#terraform-vars","title":"Terraform Vars","text":"<p>Using Variables</p> <p>Create a File with name vars.tf </p> <p><pre><code>vim vars.tf\n</code></pre> vars.tf<pre><code>variable \"REGION\" {\n  default = \"us-east-2\"\n}\n\nvariable \"ZONE1\" {\n  default = \"us-east-2a\"\n}\n\nvariable \"AMIS\" {\n  type = map(any)\n  default = {\n    us-east-2 = \"ami-0fb653ca2d3203ac1\"\n    us-east-1 = \"ami-0e1d30f2c40c4c701\"\n  }\n}\n</code></pre></p>"},{"location":"devops/terraform/#terraform-provider","title":"Terraform Provider","text":"<p>Example for AWS <pre><code>vim provider.tf\n</code></pre></p> provider.tf<pre><code>provider \"aws\" {\n  region = var.REGION\n}\n</code></pre> <p>Launching Instance with Vars File</p> <pre><code>vim main.tf\n</code></pre> main.tf<pre><code>resource \"aws_instance\" \"Instance\" {\n  ami                    = var.AMIS[var.REGION]\n  instance_type          = \"t2.micro\"\n  key_name               = \"terraform\"\n  vpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n  tags = {\n    Name = \"IAAC\"\n    Team = \"DevOps\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#terraform-provisioning","title":"Terraform Provisioning","text":"<p>Launching AWS Resources and Setting Up a Website with Terraform with existing key-pair</p> <p>Note</p> <p>Generate a key pair using the 'ssh-keygen' command and then save the contents of the public and private keys into two new files named 'testing007.pub' and 'testing007'.</p> main.tf<pre><code>resource \"aws_key_pair\" \"testing007\" {\n  key_name   = \"testing007\"\n  public_key = file(\"testing007.pub\")\n }\n\nresource \"aws_instance\" \"Instance\" {\n  ami                    = var.AMIS[var.REGION]\n  instance_type          = \"t2.micro\"\n  key_name               = aws_key_pair.testing007.key_name\n  vpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n  tags = {\n    Name = \"IAAC\"\n    Team = \"DevOps\"\n }\n\n  provisioner \"file\" {\n    source      = \"./web.sh\"\n    destination = \"/tmp/web.sh\"\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"chmod u+x /tmp/web.sh\",\n      \"sudo /tmp/web.sh\"\n    ]\n  }\n\n  connection {\n    user        = var.USER\n    private_key = file(\"testing007\")\n    host        = self.public_ip\n  }\n }\n\noutput \"PublicIP\" {\n  value = aws_instance.Instance.public_ip\n}\n</code></pre> <p>Variables file - vars.tf </p> vars.tf<pre><code>variable \"REGION\" {\n  default = \"us-east-2\"\n}\n\nvariable \"ZONE1\" {\n  default = \"us-east-2a\"\n}\n\nvariable \"USER\" {\n  default = \"ubuntu\"\n}\n\nvariable \"AMIS\" {\n  type = map(any)\n  default = {\n    us-east-2 = \"ami-0fb653ca2d3203ac1\"\n    us-east-1 = \"ami-0e1d30f2c40c4c701\"\n  }\n\n}\n</code></pre>"},{"location":"devops/terraform/#to-store-state-remotely-in-s3-bucket","title":"To Store State Remotely in S3 Bucket","text":"<p>Create an S3 Bucket</p> main.tf<pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"terraform-state-009\"\n    key    = \"terraform/remote\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#running-terraform-script-using-pipeline","title":"Running Terraform Script using Pipeline","text":"<p>Launching EC2 Instances and Storing State Files in an S3 Bucket with Terraform</p> Info <p>Enable Authentications such as service connections, OAuth Tokens and etc of the required platform with your pipeline</p> Launching EC2 InstanceProvisioning Website main.tf<pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_key_pair\" \"example_keypair\" {\n  key_name   = \"mars\"\n  public_key = tls_private_key.example_keypair.public_key_openssh\n}\n\nresource \"tls_private_key\" \"example_keypair\" {\n  algorithm = \"RSA\"\n}\n\nresource \"local_file\" \"private_key_file\" {\n  content  = tls_private_key.example_keypair.private_key_pem\n  filename = \"mars.pem\"\n}\n\nresource \"aws_security_group\" \"example_security_group\" {\n  name        = \"mars-security-group\"\n  description = \"Example Security Group\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"183.82.115.199/32\"]\n}\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"183.82.115.199/32\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"example_instance\" {\n  ami                = \"ami-09e67e426f25ce0d7\" # Ubuntu 20.04 LTS in us-east-1 (N. Virginia) region\n  instance_type      = \"t2.micro\"\n  key_name           = aws_key_pair.example_keypair.key_name\n  vpc_security_group_ids = [aws_security_group.example_security_group.id]\n\n  tags = {\n    Name = \"mars-instance\"\n  }\n}\n\nterraform {\n\n  backend \"s3\" {\n    bucket = \"terraform-azuredevops-saitejairrinki-org\"\n    key    = \"terraform/remote\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre> main.tf<pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_key_pair\" \"example_keypair\" {\n  key_name   = \"satrun\"\n  public_key = tls_private_key.example_keypair.public_key_openssh\n}\n\nresource \"tls_private_key\" \"example_keypair\" {\n  algorithm = \"RSA\"\n}\n\nresource \"local_file\" \"private_key_file\" {\n  content  = tls_private_key.example_keypair.private_key_pem\n  filename = \"satrun.pem\"\n}\n\nresource \"aws_security_group\" \"example_security_group\" {\n  name        = \"satrun-security-group\"\n  description = \"Example Security Group\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"example_instance\" {\n  ami                = \"ami-09e67e426f25ce0d7\" # Ubuntu 20.04 LTS in us-east-1 (N. Virginia) region\n  instance_type      = \"t2.micro\"\n  key_name           = aws_key_pair.example_keypair.key_name\n  vpc_security_group_ids = [aws_security_group.example_security_group.id]\n\n  tags = {\n    Name = \"satrun-instance\"\n  }\n\n  provisioner \"file\" {\n    source      = \"./web.sh\"\n    destination = \"/tmp/web.sh\"\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"chmod +x /tmp/web.sh\",\n      \"sudo /tmp/web.sh\"\n    ]\n  }\n\n  connection {\n    type        = \"ssh\"\n    user        = \"ubuntu\"  # Change to the appropriate SSH user for your AMI\n    private_key = tls_private_key.example_keypair.private_key_pem\n    host        = self.public_ip\n  }\n}\n\noutput \"PublicIP\" {\n  value = aws_instance.example_instance.public_ip\n}\n\nterraform {\n  backend \"s3\" {\n    bucket = \"1terraform\"\n    key    = \"terraform/remote\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#additional-concepts","title":"Additional Concepts\ud83d\udd17","text":"<ul> <li> <p>Terraform Import </p> </li> <li> <p>State Management</p> </li> <li> <p>Terraform Workspace</p> </li> <li> <p>Terraform Modules</p> </li> </ul>"},{"location":"devops/vagrant/","title":"Vagrant \ud83d\udee0\ufe0f","text":"<p>Vagrant is an open-source tool that helps us to automate the creation and management of Virtual Machines. In a nutshell, we can specify the configuration of a virtual machine in a simple configuration file, and Vagrant creates the same Virtual machine using just one simple command. It provides command-line interfaces to automate such tasks.</p> Requirements <p>Virtualbox \ud83d\udce6</p> <p>Vagrant \ud83d\udce6</p>"},{"location":"devops/vagrant/#installing-virtualbox-in-the-host-machine","title":"Installing Virtualbox in the Host Machine","text":"Windows \ud83e\ude9fUbuntu Desktop \ud83d\udc27 <p>Download link <pre><code>https://download.virtualbox.org/virtualbox/6.1.30/VirtualBox-6.1.30-148432-Win.exe\n</code></pre></p> <p>Through Command line  <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install virtualbox\n</code></pre></p> Note <p>To Run Virtual Machine You should need to Disable the Secure-Boot in BIOS Options of your Machine \ud83d\udeab</p> <p>To Verify that your Virtual Box is Working fine or not in Linux, Please hit the below command. <pre><code>sudo systemctl status virtualbox.service\n</code></pre></p>"},{"location":"devops/vagrant/#installing-vagrant-in-the-host-machine","title":"Installing Vagrant in the Host Machine","text":"Windows \ud83e\ude9fUbuntu Desktop \ud83d\udc27 <p>Download link <pre><code>https://releases.hashicorp.com/vagrant/2.3.4/vagrant_2.3.4_windows_i686.msi\n</code></pre></p> <p>Through Command line  <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install vagrant -y\n</code></pre></p> Note <p>You Should Need to Restart Your Machine After Installation of Vagrant </p>"},{"location":"devops/vagrant/#creating-a-common-directory-for-all-vms","title":"Creating a Common Directory for all VMs","text":"<p>Create a Directory with name vagrantvms \ud83d\udcc2 <pre><code>mkdir vagrantvms\n</code></pre> Change Directory to vagrantvms <pre><code>cd vagrantvms\n</code></pre></p>"},{"location":"devops/vagrant/#to-bring-up-vm","title":"To Bring up VM \ud83d\ude80","text":"Ubuntu \ud83d\udc27CentOS \ud83d\udc27 <p>Create a Directory name ubuntu <pre><code>mkdir ubuntu \n</code></pre> Change Directory to ubuntu <pre><code>cd ubuntu\n</code></pre> Initialize Ubuntu VM <pre><code>vagrant init ubuntu/bionic64 \n</code></pre> Now Bring up your Ubuntu VM  <pre><code>vagrant up\n</code></pre> Once your VM is up then login to your vm  <pre><code>vagrant ssh\n</code></pre> Now you can see the prompt of the ubuntu machine, If you wanna exit type exit</p> <p>Create a Directory name centos <pre><code>mkdir centos \n</code></pre> Change Directory to centos <pre><code>cd centos\n</code></pre> Initialize centos VM <pre><code>vagrant init geerlingguy/centos7 \n</code></pre> Now Bring up your centos VM  <pre><code>vagrant up\n</code></pre> Once your VM is up then login to your VM  <pre><code>vagrant ssh\n</code></pre> Now you can see the prompt of the centos machine, If you wanna exit type  \ud83d\udc49 exit</p> To Init Specific Vagrant box <p>https://app.vagrantup.com/boxes/search</p>"},{"location":"devops/vagrant/#multi-vm-vagrantfile","title":"Multi VM Vagrantfile \ud83d\udce6","text":"<p><pre><code>mkdir -p vagrantvms &amp;&amp; cd vagrantvms\n</code></pre> <pre><code>vim Vagrantfile\n</code></pre></p> <p><pre><code>Vagrant.configure(\"2\") do |config|\n  config.hostmanager.enabled = true \n  config.hostmanager.manage_host = true\n\n### Nginx VM ###\n  config.vm.define \"web01\" do |web01|\n    web01.vm.box = \"ubuntu/bionic64\"\n    web01.vm.hostname = \"web01\"\n  web01.vm.network \"private_network\", ip: \"192.168.33.11\"\n  web01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n  end\n\n### tomcat vm ###\n  config.vm.define \"app01\" do |app01|\n    app01.vm.box = \"geerlingguy/centos7\"\n    app01.vm.hostname = \"app01\"\n    app01.vm.network \"private_network\", ip: \"192.168.33.12\"\n    app01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n  app01.vm.provider \"virtualbox\" do |vb|\n    vb.memory = \"1024\"\n  end\n  app01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n#     yum update -y\n  yum install epel-release -y\n  SHELL\n  end\n\n### RabbitMQ vm  ####\n  config.vm.define \"rmq01\" do |rmq01|\n    rmq01.vm.box = \"geerlingguy/centos7\"\n  rmq01.vm.hostname = \"rmq01\"\n    rmq01.vm.network \"private_network\", ip: \"192.168.33.16\"\n  rmq01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n  rmq01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n  # yum update -y\n  #yum install epel-release -y\n  # yum install wget -y\n    SHELL\n  end ; f\n  ., v/\n### Memcache vm  #### \n  config.vm.define \"mc01\" do |mc01|\n    mc01.vm.box = \"geerlingguy/centos7\"\n  mc01.vm.hostname = \"mc01\"\n    mc01.vm.network \"private_network\", ip: \"192.168.33.14\"\n  mc01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n  mc01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n    yum install epel-release -y\n    SHELL\n  end\n\n### DB vm  ####\n  config.vm.define \"db01\" do |db01|\n    db01.vm.box = \"geerlingguy/centos7\"\n  db01.vm.hostname = \"db01\"\n    db01.vm.network \"private_network\", ip: \"192.168.33.15\"\n  db01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n  db01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n#   yum update -y\n    yum install epel-release -y\n    SHELL\n  end\n\n\nend\n</code></pre> <pre><code>vagrant up\n</code></pre> <pre><code>vagrant ssh\n</code></pre></p>"},{"location":"devops/k8s/architecture/","title":"Kubernetes (K8s) Architecture and Its Components \ud83d\ude80","text":""},{"location":"devops/k8s/architecture/#what-is-kubernetes-and-why-do-we-need-it","title":"What is Kubernetes, and Why Do We Need It? \ud83e\udd14","text":"<p>Kubernetes, often abbreviated as K8s (K-eight-s), is a powerful open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). But why do we need Kubernetes in the first place?</p> <p>Containerization, popularized by technologies like Docker, allows developers to package applications and their dependencies into portable units called containers. Containers make it easy to ensure consistency across different environments and simplify the deployment process. However, managing containers at scale can quickly become complex and challenging. This is where Kubernetes comes to the rescue.</p> <p>Kubernetes provides a robust and flexible framework for orchestrating containerized applications, allowing you to:</p> <ul> <li>Scale: Automatically scale your applications up or down based on demand.</li> <li>Deploy: Roll out new versions of your application without downtime.</li> <li>Manage: Efficiently manage and distribute workloads across clusters of machines.</li> <li>Maintain: Handle failovers and ensure high availability.</li> <li>Control: Define and enforce resource usage policies.</li> <li>Automate: Automate many aspects of application management.</li> </ul>"},{"location":"devops/k8s/architecture/#kubernetes-architecture","title":"Kubernetes Architecture \ud83c\udfdb\ufe0f","text":"<p>To understand Kubernetes fully, it's essential to grasp its architecture. Kubernetes consists of several major components and concepts working together seamlessly to create a powerful container orchestration system.</p> <p></p> <p>Let's dive into these major components and explore some example commands for managing them:</p>"},{"location":"devops/k8s/architecture/#k8s-cluster","title":"K8s Cluster \ud83c\udf10","text":"<p>A Kubernetes cluster is the foundation of your container orchestration environment. It's a set of machines, physical or virtual, that run your containerized applications. The cluster is divided into two main components: the control plane and the nodes.</p>"},{"location":"devops/k8s/architecture/#control-plane","title":"Control Plane \ud83d\udd79\ufe0f","text":"<p>The control plane is the brain of the Kubernetes cluster. It manages and controls all the nodes and containers within the cluster. It consists of several key components:</p>"},{"location":"devops/k8s/architecture/#kube-api-server","title":"Kube API Server \ud83c\udf10","text":"<p>The Kubernetes API server is the entry point for all administrative tasks. It exposes the Kubernetes API, which administrators, developers, and other services interact with to manage and control the cluster.</p> <pre><code>kubectl get pods --namespace=kube-system\n</code></pre>"},{"location":"devops/k8s/architecture/#etcd-db","title":"Etcd (DB) \ud83d\uddc4\ufe0f","text":"<p>Etcd is a distributed key-value store used to store all cluster data. It serves as Kubernetes' backing store for all cluster configuration data, ensuring consistency and high availability.</p> <pre><code>kubectl get etcd --namespace=kube-system\n</code></pre>"},{"location":"devops/k8s/architecture/#kube-scheduler","title":"Kube Scheduler \ud83d\udcc5","text":"<p>The scheduler is responsible for distributing work (containers) across multiple nodes in the cluster. It considers factors like resource requirements and constraints to make intelligent decisions.</p> <pre><code>kubectl get pods --namespace=kube-system -o wide\n</code></pre>"},{"location":"devops/k8s/architecture/#kube-controller-manager","title":"Kube Controller Manager \ud83d\udd74\ufe0f","text":"<p>The controller manager runs controller processes that regulate the state of the system. These controllers ensure that the cluster moves towards the desired state and handles events like node failures or application scaling.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/k8s/architecture/#node-components","title":"Node Components \ud83d\udda5\ufe0f","text":"<p>Nodes are the worker machines that run containerized applications. Each node in the cluster has the following components:</p>"},{"location":"devops/k8s/architecture/#kubelet","title":"Kubelet \ud83e\uddd1\u200d\u2708\ufe0f","text":"<p>The kubelet is an agent that runs on each node. It ensures that containers are running in a Pod, a basic deployable unit in Kubernetes. Kubelet communicates with the control plane to manage containers on the node.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/k8s/architecture/#kube-proxy","title":"Kube Proxy \ud83d\udd00","text":"<p>Kube Proxy is responsible for network connectivity. It maintains network rules on nodes and performs network proxying for services in the cluster.</p> <pre><code>kubectl get services\n</code></pre>"},{"location":"devops/k8s/architecture/#container-runtime","title":"Container Runtime \ud83d\udc33","text":"<p>The container runtime is the software responsible for running containers. Kubernetes supports various container runtimes, including Docker, containerd, and CRI-O.</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"devops/k8s/architecture/#coredns","title":"CoreDNS \u2601\ufe0f","text":"<p>CoreDNS is a lightweight, flexible, and extensible DNS server that Kubernetes uses for service discovery within the cluster.</p> <pre><code>kubectl get svc --namespace=kube-system\n</code></pre>"},{"location":"devops/k8s/architecture/#kubectl","title":"Kubectl \ud83d\udee0\ufe0f","text":"<p>Kubectl is the command-line tool used to interact with Kubernetes clusters. It allows you to create, inspect, update, and delete resources in your cluster.</p> <pre><code>kubectl version\n</code></pre>"},{"location":"devops/k8s/architecture/#kubernetes-objects-resources","title":"Kubernetes Objects (Resources) \ud83d\udce6","text":"<p>Kubernetes uses a declarative configuration model. You describe the desired state of your applications and infrastructure using Kubernetes objects or resources. Here are some of the essential types of Kubernetes objects along with example commands for managing them:</p>"},{"location":"devops/k8s/architecture/#workloads","title":"Workloads \ud83d\udcbc","text":"<p>Workloads represent your applications and their components. Common workload objects include:</p> <ul> <li>Deployment: Manages the deployment and scaling of Pods.</li> </ul> <pre><code>kubectl create deployment my-deployment --image=my-image:v1\n</code></pre> <ul> <li>ReplicaSet: Ensures a specified number of replicas of a Pod are running.</li> </ul> <pre><code>kubectl scale deployment my-deployment --replicas=3\n</code></pre> <ul> <li>StatefulSet: Manages stateful applications with unique network identities.</li> </ul> <pre><code>kubectl get statefulsets\n</code></pre> <ul> <li>Pod: The smallest deployable unit in Kubernetes.</li> </ul> <pre><code>kubectl get pods\n</code></pre> <ul> <li>DaemonSet: Ensures that all or some nodes run a copy of a Pod.</li> </ul> <pre><code>kubectl get daemonset\n</code></pre> <ul> <li>ScheduledJob: Runs Jobs periodically.</li> </ul> <pre><code>kubectl get scheduledjobs\n</code></pre> <ul> <li>Job: Runs a specified task to completion.</li> </ul> <pre><code>kubectl get jobs\n</code></pre> <ul> <li>CronJob: Runs Jobs on a schedule.</li> </ul> <pre><code>kubectl get cronjobs\n</code></pre>"},{"location":"devops/k8s/architecture/#services","title":"Services \ud83c\udf10","text":"<p>Services define networking rules to access your application. They include:</p> <ul> <li>Service: Exposes a set of Pods as a network service.</li> </ul> <pre><code>kubectl expose deployment my-deployment --port=80 --target-port=8080 --type=LoadBalancer\n</code></pre> <ul> <li>Service Types: ClusterIP, NodePort, LoadBalancer.</li> </ul> <pre><code>kubectl get services\n</code></pre> <ul> <li>IngressController and Ingress: Manages external access to the services.</li> </ul> <pre><code>kubectl get ingresses\n</code></pre> <ul> <li>Labels and Selectors: Labels are used to select and organize objects.</li> </ul> <pre><code>kubectl get pods -l app=my-app\n</code></pre>"},{"location":"devops/k8s/architecture/#configuration","title":"Configuration \u2699\ufe0f","text":"<p>Configuration objects help manage application configuration data:</p> <ul> <li>Secrets: Store sensitive information, like passwords or API keys.</li> </ul> <pre><code>kubectl create secret generic my-secret --from-literal=username=myuser --from-literal=password=mypassword\n</code></pre> <ul> <li>ConfigMaps: Store configuration data as key-value pairs.</li> </ul> <pre><code>kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2\n</code></pre> <ul> <li>KubeConfig: Configure access to Kubernetes clusters.</li> </ul> <pre><code>kubectl config set-context my-context --cluster=my-cluster --user=my-user\n</code></pre>"},{"location":"devops/k8s/architecture/#storage","title":"Storage \ud83d\uddc4\ufe0f","text":"<p>Storage objects manage data storage:</p> <ul> <li>Volumes and PVC: Provide storage for Pods.</li> </ul> <pre><code>kubectl get pv\n</code></pre> <ul> <li>Storage Classes: Define different classes of storage.</li> </ul> <pre><code>kubectl get storageclass\n</code></pre>"},{"location":"devops/k8s/architecture/#security","title":"Security \ud83d\udd10","text":"<p>Security objects help secure your cluster:</p> <ul> <li>RBAC (Role-Based Access Control): Control access to resources based on roles.</li> </ul> <pre><code>kubectl create clusterrole my-role --verb=get,list --resource=pods\n</code></pre> <ul> <li>Service Accounts: Manage permissions for Pods.</li> </ul> <pre><code>kubectl create serviceaccount my-serviceaccount\n</code></pre>"},{"location":"devops/k8s/architecture/#set-resource-limits","title":"Set Resource Limits \u2696\ufe0f","text":"<p>You can set resource limits on Pods to manage resource usage effectively.</p> <pre><code>kubectl set resources deployment/my-deployment --limits=cpu=500m,memory=256Mi\n</code></pre>"},{"location":"devops/k8s/architecture/#health-checks","title":"Health Checks \u2764\ufe0f","text":"<p>Kubernetes supports health checks to ensure that your applications are running correctly.</p> <pre><code>kubectl get pods --field-selector=status.phase=Running\n</code></pre>"},{"location":"devops/k8s/eks/","title":"Spinning Up a Kubernetes Cluster on AWS EKS","text":"<p>The process of creating a Kubernetes cluster on Amazon Web Services (AWS) using Amazon Elastic Kubernetes Service (EKS). Kubernetes is an open-source container orchestration platform that helps manage containerized applications and services. Amazon EKS simplifies the setup and management of Kubernetes clusters, allowing you to focus on deploying and managing your applications.</p>"},{"location":"devops/k8s/eks/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like EKS clusters, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>kubectl: Install <code>kubectl</code>, the Kubernetes command-line tool, to interact with your cluster. You can follow the installation instructions in the Kubernetes Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., AmazonEKSClusterPolicy, AmazonEKSServicePolicy). Note down the user's access key ID and secret access key. Document Link</p> </li> <li> <p>Key Pair (Optional): If you plan to SSH into your worker nodes, create an EC2 key pair or use an existing one. This can be done through the AWS Management Console.</p> </li> </ol>"},{"location":"devops/k8s/eks/#steps-to-create-an-eks-cluster","title":"Steps to Create an EKS Cluster","text":""},{"location":"devops/k8s/eks/#step1-install-aws-cli","title":"Step1: Install AWS CLI","text":"Linux \ud83d\udc27Windows \ud83e\ude9f <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n</code></pre> <pre><code>unzip awscliv2.zip\n</code></pre> <pre><code>sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update\n</code></pre> <pre><code>choco install awscli\n</code></pre> <pre><code>aws --version\n</code></pre>"},{"location":"devops/k8s/eks/#step-2-configure-the-aws-cli-with-the-iam-users-access-key-id-and-secret-access-key","title":"Step 2: Configure the AWS CLI with the IAM user's access key ID and secret access key:","text":"<p><pre><code>aws configure\n</code></pre>  Once it is done try some aws cli commands like aws s3 ls If u have any buckets in your s3 it will list (Checking Access)</p>"},{"location":"devops/k8s/eks/#step-3-install-kubectl","title":"Step 3: Install Kubectl","text":"Linux \ud83d\udc27Windows \ud83e\ude9f <pre><code>curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.17/2023-10-17/bin/linux/amd64/kubectl\n</code></pre> <pre><code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> <pre><code>kubectl version --client\n</code></pre> <pre><code>choco install kubernetes-cli --version=1.23.4\n</code></pre> <pre><code>kubectl version --client\n</code></pre>"},{"location":"devops/k8s/eks/#step-4-install-eksctl","title":"Step 4: Install eksctl","text":"Linux \ud83d\udc27Windows \ud83e\ude9f <pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n</code></pre> <pre><code>sudo mv /tmp/eksctl /usr/local/bin\n</code></pre> <pre><code>eksctl version\n</code></pre> <pre><code>choco install -y eksctl\n</code></pre> <pre><code>eksctl version\n</code></pre>"},{"location":"devops/k8s/eks/#step-5-create-an-eks-cluster","title":"Step 5: Create an EKS Cluster","text":"<p>Here are all the commands:</p>"},{"location":"devops/k8s/eks/#command-1-create-an-eks-cluster","title":"Command 1: Create an EKS Cluster","text":"<pre><code>eksctl create cluster --name &lt;cluster-name&gt; --version &lt;k8s-version&gt; --region &lt;region&gt; --nodegroup-name &lt;nodegroup-name&gt; --node-type &lt;node-type&gt; --nodes &lt;node-count&gt; --nodes-min &lt;min-nodes&gt; --nodes-max &lt;max-nodes&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl create cluster --name kube-app --version 1.24 --region eu-central-1 --nodegroup-name kube-node-demo --node-type t2.micro --nodes 2 --nodes-min 2 --nodes-max 3\n</code></pre>"},{"location":"devops/k8s/eks/#command-2-get-information-about-nodes","title":"Command 2: Get Information About Nodes","text":"<pre><code>kubectl get nodes\n</code></pre> After setting up your EKS Cluster, go ahead and deploy my coffeeshop website for a warm-up. Enjoy the deployment! \u2615\ud83d\ude80 <p>https://softwarelife.github.io/devops/k8s/exercise01/#website-deployment</p>"},{"location":"devops/k8s/eks/#command-3-update-node-group-maximum-nodes-and-minimum-nodes","title":"Command 3: Update Node Group Maximum Nodes and Minimum Nodes","text":"<pre><code>eksctl update nodegroup --cluster=&lt;cluster-name&gt; --region=&lt;region&gt; --name=&lt;nodegroup-name&gt; --nodes-min=&lt;min-nodes&gt; --nodes-max=&lt;max-nodes&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl update nodegroup --cluster kube-app --region eu-central-1 --name kube-app --nodes-min=1 --nodes-max=2\n</code></pre>"},{"location":"devops/k8s/eks/#command-4-scale-node-group","title":"Command 4: Scale Node Group","text":"<pre><code>eksctl scale nodegroup --cluster &lt;cluster-name&gt; --region &lt;region&gt; --name &lt;nodegroup-name&gt; --nodes &lt;desired-nodes&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl scale nodegroup --cluster kube-app --region eu-central-1 --name kube-app --nodes 2\n</code></pre>"},{"location":"devops/k8s/eks/#command-5-get-node-group-details","title":"Command 5: Get Node Group Details","text":"<pre><code>eksctl get nodegroup --region &lt;region&gt; --name &lt;nodegroup-name&gt; --cluster &lt;cluster-name&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl get nodegroup --region eu-central-1 --name kube-app --cluster kube-app\n</code></pre>"},{"location":"devops/k8s/eks/#command-6-delete-eks-cluster","title":"Command 6: Delete EKS Cluster","text":"<pre><code>eksctl delete cluster --name &lt;cluster-name&gt; --region &lt;region&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl delete cluster --name kube-app --region eu-central-1\n</code></pre>"},{"location":"devops/k8s/exercise01/","title":"Hands-on Exercises with Kubernetes \ud83d\udcbb\ud83d\ude80","text":"<p>Kubernetes is the go-to platform for container orchestration, and mastering it is essential for modern DevOps and cloud-native development. One fundamental aspect of Kubernetes is managing resources using either imperative or declarative methods. In this hands-on blog post, we'll dive into both approaches with practical examples and real-time use cases.</p>"},{"location":"devops/k8s/exercise01/#kubernetes-syntax","title":"Kubernetes syntax \ud83e\udde9","text":"<p>Let's dive into the basic Kubernetes syntax for four key fields: <code>apiVersion</code>, <code>kind</code>, <code>metadata</code>, and <code>spec</code>:</p> <ol> <li> <p>apiVersion:</p> <p>Purpose: The <code>apiVersion</code> field specifies the version of the Kubernetes API that the resource definition uses. It ensures compatibility between the resource and the Kubernetes cluster.</p> <p>Format: Typically, the <code>apiVersion</code> field consists of two parts separated by a forward slash (\"/\"). The first part is the API group, and the second part is the API version within that group.</p> <p>Examples: Common API versions include <code>v1</code> (core API for basic resources), <code>apps/v1</code> (for application-related resources), and <code>networking.k8s.io/v1</code> (for network policies).</p> <p>Usage: It tells the Kubernetes cluster how to interpret the resource definition. It's essential to specify the correct <code>apiVersion</code> to create resources successfully.</p> </li> <li> <p>kind:</p> <p>Purpose: The <code>kind</code> field defines the type of Kubernetes resource you are creating or managing. It determines the behavior and attributes of the resource.</p> <p>Examples: Common resource kinds include <code>Pod</code>, <code>Service</code>, <code>Deployment</code>, <code>ConfigMap</code>, <code>Secret</code>, <code>Namespace</code>, and more.</p> <p>Usage: The <code>kind</code> field ensures that Kubernetes understands the type of resource you intend to create or modify.</p> </li> <li> <p>metadata:</p> <p>Purpose: The <code>metadata</code> field provides information about the resource, such as its name, labels, and annotations.</p> <p>Format: It is a YAML dictionary that includes various metadata fields like <code>name</code>, <code>labels</code>, <code>annotations</code>, and more.</p> <p>Examples: Setting <code>name</code> specifies the name of the resource, while <code>labels</code> allow you to add key-value pairs for categorization and selection.</p> <p>Usage: Metadata helps in identifying, organizing, and managing resources within the Kubernetes cluster.</p> </li> <li> <p>spec:</p> <p>Purpose: The <code>spec</code> field describes the desired state of the resource. It specifies the configuration settings, properties, and behaviors that the resource should have.</p> <p>Format: It depends on the resource type and its attributes. For example, in a Pod, the <code>spec</code> field may include details about containers, volumes, and networking.</p> <p>Usage: The <code>spec</code> field is where you define how you want the resource to function within the Kubernetes cluster. It represents the resource's desired state.</p> </li> </ol> <p>Here's an example of how these fields come together in a Kubernetes YAML file for creating a Pod:</p> <pre><code># Kubernetes YAML file for a basic Pod\n\n#The 'apiVersion' field specifies the Kubernetes API version. \napiVersion: v1 # (1)!\n\n# The 'kind' field defines the type of Kubernetes resource.\nkind: Pod # (2)!\n\n# Metadata section provides information about the resource.\nmetadata:\n  # 'name' specifies the name of the Pod.\n  name: my-pod\n\n  # 'labels' are key-value pairs used to organize and select resources.\n  # Here, we label the Pod as 'app: my-app.'\n  labels:\n    app: my-app \n\n# The 'spec' section describes the desired state of the resource.\nspec:\n  # 'containers' is a list of containers to run within the Pod.\n  containers:\n    - name: my-container\n      # 'image' specifies the Docker image to use for this container.\n      # In this case, we use the 'nginx:latest' image.\n      image: nginx:latest\n\n      # 'ports' section defines the ports to be exposed by the container.\n      ports:\n        - \n          # 'containerPort' specifies the port number (80) on which the container listens.\n          containerPort: 80\n</code></pre> <ol> <li> <p>Common API versions include:</p> <ul> <li> <p>v1: Core resources like Pods, Services, ConfigMaps, and more.</p> </li> <li> <p>apps/v1: Resources like Deployments, StatefulSets, and DaemonSets.</p> </li> <li> <p>extensions/v1beta1: Older version for resources like Ingress.</p> </li> <li> <p>networking.k8s.io/v1: Resources like NetworkPolicies.</p> </li> </ul> </li> <li> <p>In this case, we are creating a Pod, but other common resource types include:</p> <ul> <li> <p>Deployment</p> </li> <li> <p>Service</p> </li> <li> <p>ConfigMap</p> </li> <li> <p>Secret</p> </li> <li> <p>PersistentVolume</p> </li> <li> <p>PersistentVolumeClaim</p> </li> <li> <p>StatefulSet</p> </li> <li> <p>Namespace</p> </li> </ul> </li> </ol> <p>In this example, we have:</p> <ul> <li><code>apiVersion: v1</code> indicating the core API version.</li> <li><code>kind: Pod</code> specifying that we are defining a Pod resource.</li> <li><code>metadata</code> providing metadata like the Pod's name and labels.</li> <li><code>spec</code> describing the desired state of the Pod, including container details and port configuration.</li> </ul> <p>Understanding and correctly using these basic Kubernetes syntax elements is crucial for defining and managing Kubernetes resources effectively.</p>"},{"location":"devops/k8s/exercise01/#imperative-declarative-methods","title":"Imperative \ud83c\udd9a Declarative Methods","text":""},{"location":"devops/k8s/exercise01/#imperative-method","title":"Imperative Method:","text":"<p>Example</p> <p>Imperative commands directly manipulate Kubernetes resources in real-time. They are ideal for quick, one-off tasks. Let's explore some imperative examples:</p> <p>1. Create a Namespace: Command Line<pre><code>kubectl create namespace my-namespace \n</code></pre></p> <p>2. Run a Pod: Command Line<pre><code>kubectl run my-pod --image=nginx --port=80\n</code></pre></p> <p>3. Scale a Deployment: Command Line<pre><code>kubectl scale deployment my-deployment --replicas=3 # (1)\n</code></pre></p> <ol> <li>\ud83d\udd90\ufe0f Modify the quantity of replicas here</li> </ol>"},{"location":"devops/k8s/exercise01/#real-time-use-case","title":"Real-time Use Case:","text":"<p>Suppose you have a sudden spike in traffic to your website, and you need to quickly scale up your application. Imperative commands allow you to react swiftly without worrying about YAML configuration files.</p>"},{"location":"devops/k8s/exercise01/#declarative-method","title":"Declarative Method:","text":"<p>Example</p> <p>Declarative Kubernetes management relies on YAML files to define the desired state of resources. Here are some declarative examples:</p> <p>1. Create a Namespace namespace.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n</code></pre></p> <p>2. Run a Pod pod.yaml<pre><code>apiVersion: v1  # Kubernetes API version \ud83c\udf10\nkind: Pod  # Type of resource: a Pod \ud83d\ude80\nmetadata:\n  name: my-pod  # Name of the Pod \ud83c\udff7\ufe0f\nspec:\n  containers:\n    - name: nginx-container  # Container name \ud83d\udc33\n      image: nginx  # Docker image used \ud83d\udce6\n      ports:\n        - containerPort: 80  # Container's port configuration \ud83d\udd0c\n</code></pre></p> <p>3. Scale a Deployment deployment.yaml<pre><code>apiVersion: apps/v1  # Kubernetes API version for Deployments \ud83c\udf10\nkind: Deployment  # Type of resource: a Deployment \ud83d\ude80\nmetadata:\n  name: my-deployment  # Name of the Deployment \ud83c\udff7\ufe0f\nspec:\n  replicas: 3  # Number of desired replicas of the app \ud83d\udd04\n  selector:\n    matchLabels:\n      app: my-app  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  template:\n    metadata:\n      labels:\n        app: my-app  # Labels for Pods created by this Deployment \ud83c\udff7\ufe0f\n    spec:\n      containers:\n        - name: my-container  # Container name \ud83d\udc33\n          image: my-image  # Docker image used for the container \ud83d\udce6\n</code></pre></p>"},{"location":"devops/k8s/exercise01/#real-time-use-case_1","title":"Real-time Use Case:","text":"<p>Imagine you are setting up a Continuous Integration/Continuous Deployment (CI/CD) pipeline. Declarative manifests are the preferred choice as they clearly define the desired state of your resources, making it easier to track changes and collaborate with your team.</p>"},{"location":"devops/k8s/exercise01/#converting-imperative-kubernetes-commands-to-declarative-yaml-files","title":"Converting Imperative Kubernetes Commands to Declarative YAML Files","text":"<p>Command Line<pre><code>kubectl create deployment my-nginx-pod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> This command converts an imperative action (creating a pod) into a declarative file using <code>kubectl</code>.</p>"},{"location":"devops/k8s/exercise01/#sample-pod-file-imperative-vs-declarative","title":"Sample Pod File - Imperative vs. Declarative:","text":"<p>Let's create a sample Pod file to run an Nginx container using both imperative and declarative methods:</p> <p>**Imperative ** Command Line<pre><code>kubectl run my-nginx-pod --image=nginx --port=80 \n</code></pre></p> <p>Declarative pod.yaml<pre><code>apiVersion: v1  # Kubernetes API version \ud83c\udf10\nkind: Pod  # Type of resource: a Pod \ud83d\ude80\nmetadata:\n  name: my-nginx-pod  # Name of the Pod \ud83c\udff7\ufe0f\nspec:\n  containers:\n    - name: nginx-container  # Container name \ud83d\udc33\n      image: nginx  # Docker image used \ud83d\udce6\n      ports:\n        - containerPort: 80  # Port configuration for the container \ud83d\udd0c\n</code></pre></p>"},{"location":"devops/k8s/exercise01/#sample-deployment-and-service-files-imperative-vs-declarative","title":"Sample Deployment and Service Files - Imperative vs. Declarative:","text":"<p>Now, let's create deployment and service files for deploying Nginx with a NodePort service using both methods:</p> <p>Imperative (command-line): <pre><code>kubectl create deployment nginx-deployment --image=nginx\nkubectl expose deployment nginx-deployment --port=80 --type=NodePort\n</code></pre></p> <p>Declarative deployment.yaml<pre><code>apiVersion: apps/v1  # Kubernetes API version for Deployments \ud83c\udf10\nkind: Deployment  # Type of resource: a Deployment \ud83d\ude80\nmetadata:\n  name: nginx-deployment  # Name of the Deployment \ud83c\udff7\ufe0f\nspec:\n  replicas: 3  # Number of desired replicas of the app \ud83d\udd04\n  selector:\n    matchLabels:\n      app: nginx  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  template:\n    metadata:\n      labels:\n        app: nginx  # Labels for Pods created by this Deployment \ud83c\udff7\ufe0f\n    spec:\n      containers:\n        - name: nginx-container  # Container name \ud83d\udc33\n          image: nginx  # Docker image used for the container \ud83d\udce6\n</code></pre></p> <p>Declarative service.yaml<pre><code>apiVersion: v1  # Kubernetes API version for Services \ud83c\udf10\nkind: Service  # Type of resource: a Service \ud83d\ude80\nmetadata:\n  name: nginx-service  # Name of the Service \ud83c\udff7\ufe0f\nspec:\n  selector:\n    app: nginx  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  ports:\n    - protocol: TCP  # Protocol used for the port \ud83d\udd0c\n      port: 80  # Port exposed by the Service \ud83c\udf10\n      targetPort: 80  # Target port on Pods \ud83c\udfaf\n  type: NodePort  # Type of Service: NodePort for external access \ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfe2\n</code></pre></p>"},{"location":"devops/k8s/exercise01/#deploying-grafana-with-loadbalancer","title":"Deploying Grafana with LoadBalancer:","text":"<p>To further demonstrate declarative YAML, let's create a deployment and service file to deploy Grafana with a NodePort service. You can customize the Grafana deployment based on your requirements.</p> deployment.yaml<pre><code>apiVersion: apps/v1  # Kubernetes API version for Deployments \ud83c\udf10\nkind: Deployment  # Type of resource: a Deployment \ud83d\ude80\nmetadata:\n  name: grafana-deployment  # Name of the Deployment \ud83c\udff7\ufe0f\nspec:\n  replicas: 1  # Number of desired replicas of the app \ud83d\udd04\n  selector:\n    matchLabels:\n      app: grafana  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  template:\n    metadata:\n      labels:\n        app: grafana  # Labels for Pods created by this Deployment \ud83c\udff7\ufe0f\n    spec:\n      containers:\n        - name: grafana-container  # Container name \ud83d\udc33\n          image: grafana/grafana:latest  # Docker image used for the container \ud83d\udce6\n          ports:\n            - containerPort: 3000  # Port configuration for the container \ud83d\udd0c\n</code></pre> service.yaml<pre><code>apiVersion: v1  # Kubernetes API version for Services \ud83c\udf10\nkind: Service  # Type of resource: a Service \ud83d\ude80\nmetadata:\n  name: grafana-service  # Name of the Service \ud83c\udff7\ufe0f\nspec:\n  selector:\n    app: grafana  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  ports:\n    - protocol: TCP  # Protocol used for the port \ud83d\udd0c\n      port: 80  # Port exposed by the Service \ud83c\udf10\n      targetPort: 3000  # Target port on Pods \ud83c\udfaf\n  type: LoadBalancer  # Type of Service: LoadBalancer link for external access \ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfe2\n</code></pre> <p>In this example, we're deploying Grafana using declarative YAML files, allowing for a well-defined and repeatable process.</p>"},{"location":"devops/k8s/exercise01/#website-deployment","title":"Website Deployment","text":"deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wavecafe  # \ud83d\ude80 Name of the Deployment\nspec:\n  replicas: 1  # \ud83d\udeb6 Number of desired replicas\n  selector:\n    matchLabels:\n      app: cafe  # \ud83c\udff7\ufe0f Selector to match pods with the label \"app: cafe\"\n  template:\n    metadata:\n      labels:\n        app: cafe  # \ud83c\udff7\ufe0f Labels applied to pods created by this template\n    spec:\n      containers:\n        - name: my-app-container  # \ud83d\udce6 Name of the container\n          image: saitejairrinki/wavecafe:v1  # \ud83d\udc33 Docker image to use\n          ports:\n            - name: cafe-port  # \ud83c\udf10 Name of the port\n              containerPort: 80  # \ud83d\udeaa Port that the container listens on\n</code></pre> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: wave-cafe  # \u2615 Name of the Service\nspec:\n  selector:\n    app: cafe  # \ud83c\udff7\ufe0f Select pods with the label \"app: cafe\"\n  ports:\n    - protocol: TCP  # \ud83c\udf10 Protocol for the port\n      port: 80  # \ud83d\udeaa Port on the Service\n      targetPort: cafe-port  # \ud83d\ude80 Port on the pods to forward traffic to\n  type: NodePort  # \ud83c\udfe2 Expose the Service as a NodePort\n</code></pre>"},{"location":"devops/k8s/intro/","title":"Kubernetes","text":"<p>Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.</p> <p>In the digital realm, Kubernetes serves as a versatile \ud83c\udf10 platform. It's your trusted partner for managing containerized workloads and services, ensuring they run smoothly across diverse environments. It offers the power of declarative configuration and automation, simplifying the complex.</p>"},{"location":"devops/k8s/intro/#lets-explore-the-top-10-reasons-why-kubernetes-has-captured-the-hearts-of-developers-and-organizations-worldwide","title":"Let's explore the top 10 reasons why Kubernetes has captured the hearts of developers and organizations worldwide \ud83d\udea2","text":"<p>\ud83c\udf0d Largest Open Source Project: Kubernetes holds the title of being the largest open-source project globally. Its expansive reach means you can trust its capabilities.</p> <p>\ud83e\udd17 Great Community Support: The Kubernetes community is a treasure trove of knowledge and support. It's like having an army of experts at your disposal.</p> <p>\ud83d\udce6 Robust Container Deployment: Kubernetes excels at container management, ensuring your applications run smoothly, just like a well-oiled machine.</p> <p>\ud83d\udcc2 Effective Persistent Storage: Storage management is a breeze with Kubernetes, allowing your data to stay safe and accessible.</p> <p>\u2601\ufe0f Multi-Cloud Support (Hybrid Cloud): Whether your applications reside in the cloud, on-premises, or both, Kubernetes seamlessly bridges the gap.</p> <p>\ud83d\udc93 Container Health Monitoring: It keeps a vigilant eye on your containers, ensuring they're always in top-notch health.</p> <p>\ud83d\udda5\ufe0f Compute Resource Management: Kubernetes optimizes resource allocation, ensuring efficient use of computing power.</p> <p>\u2696\ufe0f Auto-Scaling Feature Support: When traffic surges, Kubernetes automatically scales your applications, maintaining responsiveness.</p> <p>\ud83c\udfed Real-world Use Cases: Kubernetes shines in real-world scenarios, from e-commerce to finance, demonstrating its versatility.</p> <p>\ud83c\udf10 High Availability by Cluster Federation: Ensuring your applications are always available, Kubernetes employs cluster federation.</p>"},{"location":"devops/k8s/intro/#types-few-types-of-kubernetes-clusters-for-various-situations","title":"Types Few types of Kubernetes clusters for various situations:","text":"<p>\ud83d\udc68\u200d\ud83d\udcbb Single-node Cluster (Minikube): Great for trying things out on your own computer. It's like a mini playground for Kubernetes.</p> <p>\ud83c\udf10 Multi-node Cluster (Kubeadm, KOPS): Perfect for running your apps in production. It can handle multiple containers and keeps everything running smoothly.</p> <p>\u2601\ufe0f Managed Kubernetes Services (EKS, GKE, AKS): If you want Kubernetes without the hassle of managing the servers, cloud providers like Amazon, Google, and Azure offer managed services.</p> <p>\ud83d\udee0\ufe0f Development and Staging Clusters: Think of these as testing areas. Developers use them to make sure everything works before it goes live.</p> <p>\ud83c\udfe2 On-Premises Clusters: For businesses that need to keep everything in their own data centers for security or compliance reasons.</p> <p>\u2601\ufe0f\ud83c\udfe2 Hybrid Clusters: If you want to use both your own servers and the cloud, hybrid clusters connect them seamlessly.</p> <p>So, Kubernetes is like your trusty orchestra conductor, making sure all your containerized applications play in harmony, no matter where they are. \ud83d\ude80\ud83c\udfb5</p>"},{"location":"devops/k8s/k3s/","title":"Creating Kubernetes Cluster with k3s","text":"<p>K3s is a lightweight Kubernetes distribution that Rancher Labs, which is a fully certified Kubernetes offering by CNCF. In K3s, we see that the memory footprint or binary which contains the components to run a cluster is small. It means that K3s are small in size.</p>"},{"location":"devops/k8s/k3s/#setup","title":"Setup","text":"<pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre> <p><pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <pre><code>sudo chmod 644 $KUBECONFIG\n</code></pre></p>"},{"location":"devops/k8s/k8sdashboard/","title":"Deploy the latest Kubernetes dashboard","text":"<p>Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started.</p> <p>The first thing to know about the web UI is that it can only be accessed using the localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the  and  with the relevant details to your Kubernetes cluster. <pre><code>ssh -L localhost:8001:127.0.0.1:8001 &lt;user&gt;@&lt;master_public_IP&gt;\n</code></pre> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre> If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output <pre><code>namespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre> <p>Afterward, you should have two new pods running on your cluster.</p> <pre><code>kubectl get pods -A\n</code></pre> Output <pre><code>...\nkubernetes-dashboard   dashboard-metrics-scraper-78ujd94gf7-ff6h8   1/1     Running   0          30m\nkubernetes-dashboard   kubernetes-dashboard-g6hujkirf7-df65g        1/1     Running   0          30m\n</code></pre> <p>You can then continue ahead with creating the required user accounts.</p>"},{"location":"devops/k8s/k8sdashboard/#creating-admin-user","title":"Creating Admin user","text":"<p>The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and use tokens.</p> <p>Start by making a new directory for the dashboard configuration files. <pre><code>mkdir ~/dashboard &amp;&amp; cd ~/dashboard\n</code></pre> Create the following configuration and save it as a dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor.</p> <pre><code>vim dashboard-admin.yaml\n</code></pre> <p><pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n</code></pre> Once set, save the file and exit the editor.</p> <p>Then deploy the admin user role with the next command.</p> <p><pre><code>kubectl apply -f dashboard-admin.yaml\n</code></pre> You should see a service account and a cluster role binding created.</p> Output <pre><code>serviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\n</code></pre> <p>Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token.</p> <p>Get the admin token using the command below. <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre> You\u2019ll then see an output of a long string of seemingly random characters like in the example below.</p> Output <p>eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew</p> <p>The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed.</p>"},{"location":"devops/k8s/k8sdashboard/#creating-read-only-user","title":"Creating Read-Only user","text":"<p>If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster.</p> <p>Similarly to the admin account, save the following configuration in dashboard-read-only.yaml <pre><code>vim dashboard-read-only.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: read-only-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n  name: read-only-clusterrole\n  namespace: default\nrules:\n- apiGroups:\n  - \"\"\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-only-binding\nroleRef:\n  kind: ClusterRole\n  name: read-only-clusterrole\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: read-only-user\n  namespace: kubernetes-dashboard\n</code></pre></p> <p>Once set, save the file and exit the editor.</p> <p>Then deploy the read-only user account with the command below.</p> <p><pre><code>kubectl apply -f dashboard-read-only.yaml\n</code></pre> To allow users to log in via the read-only account, you\u2019ll need to provide a token that can be fetched using the next command. <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre> The toke will be a long series of characters and unique to the dashboard currently running.</p>"},{"location":"devops/k8s/k8sdashboard/#accessing-the-dashboard","title":"Accessing the dashboard","text":"<p>We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself.</p> <p>However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. <pre><code>kubectl proxy\n</code></pre> This will start the server at 127.0.0.1:8001 as shown by the output.</p> Output <p>Starting to serve on 127.0.0.1:8001</p> <p>Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below.</p> <p>Link</p> <p>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</p> <p>If everything is running correctly, you should see the dashboard login window.</p> <p></p> <p>Select the token authentication method and copy your admin token into the field below. Then click the Sign in button.</p> <p>You will then be greeted by the overview of your Kubernetes cluster.</p> <p></p> <p>While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard.</p> <p></p> <p>Then either copy in any configuration file you wish, select the file directly from your machine, or create a new configuration from a form.</p>"},{"location":"devops/k8s/kops/","title":"Creating Kubernetes Cluster with KOPS","text":"<p>Install AWS CLI  <pre><code>apt update &amp;&amp; apt install awscli -y\n</code></pre> Configure AWS CLI with IAM user Credentials with a specific Region  <pre><code>aws configure\n</code></pre></p> <p>Note</p> <p>If you are using AWS Instance better to use IAM Role than Creating User with Access-key</p> <p>Check Whether AWS CLI Commands Working or not  <pre><code>aws s3 ls\n</code></pre> Generate SSH Keys <pre><code>ssh-keygen\n</code></pre></p>"},{"location":"devops/k8s/kops/#install-kubectl-binary-with-curl-on-linux","title":"Install kubectl binary with curl on Linux","text":"<p>Download the latest release with the command: <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n</code></pre> Install kubectl <pre><code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> <pre><code>kubectl version --client\n</code></pre></p>"},{"location":"devops/k8s/kops/#installing-kubernetes-with-kops","title":"Installing Kubernetes with kops","text":"<p>Download the latest release with the command:</p> <p><pre><code>curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64\n</code></pre> <pre><code>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64\n</code></pre></p> <p>Make the kops binary executable <pre><code>chmod +x kops-linux-amd64\n</code></pre> Move the kops binary into your PATH. <pre><code>sudo mv kops-linux-amd64 /usr/local/bin/kops\n</code></pre> <pre><code>kops\n</code></pre></p>"},{"location":"devops/k8s/kops/#creating-k8s-cluster-with-kops","title":"Creating K8s Cluster with KOPS","text":""},{"location":"devops/k8s/kops/#kops-commands-to-setup-k8s-cluster-","title":"Kops commands to setup k8s cluster:-","text":"<p>Creating S3 Bucket for Kubernetes Cluster <pre><code>aws s3 mb s3:// &lt;bucket name&gt;\n</code></pre> <pre><code>kops create cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --zones=ap-south-1a,ap-south-1b --node-count=2 --node-size=t3.medium --master-size=t3.medium --dns-zone=saiteja.irrinki.xyz --node-volume-size=8 --master-volume-size=8\n</code></pre> It will create a configuration of kops <pre><code>kops update cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --yes --admin\n</code></pre> It will create kops data in the S3 bucket. It starts creating a cluster &amp; it takes 10 mins <pre><code>kops validate cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt;\n</code></pre> It shows ur cluster is ready</p> <p>To Delete Cluster</p> <pre><code>kops delete cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --yes \n</code></pre>"},{"location":"devops/k8s/kubeadm/","title":"Creating Kubernetes Cluster with kubeadm","text":"For this activity, deploy minimum 2 AWS instances with the Security groups <p>All traffic is enabled between the instances </p> Node Name Instance Details Resources Master Node -t2.medium 4GB Ram , 2 CPU Worker Node -t2.micro 1GB Ram , 1 CPU"},{"location":"devops/k8s/kubeadm/#preparing-the-master-and-worker-nodes-for-kubeadm","title":"Preparing the Master and Worker nodes for kubeadm","text":"<p>Execute the below Commands on both Master &amp; Worker Nodes</p> <p>Change the hostname for nodes as master &amp; worker</p>"},{"location":"devops/k8s/kubeadm/#installing-docker","title":"Installing Docker:","text":"<p>Add the Docker repository key and Docker repository. <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\nadd-apt-repository \\\n\"deb https://download.docker.com/linux/$(. /etc/os-release; echo \"$ID\") \\\n$(lsb_release -cs) \\\nstable\"\n</code></pre> Update the list of packages and install the docker</p> <p><pre><code>apt-get update\n</code></pre> <pre><code>apt-get install -y docker-ce\n</code></pre> <pre><code>echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json\n</code></pre> <pre><code>systemctl daemon-reload\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <pre><code>systemctl enable docker\n</code></pre></p>"},{"location":"devops/k8s/kubeadm/#installing-kubeadm-kublet-and-kubectl","title":"Installing kubeadm, kublet, and kubectl:","text":"<p>Add the Google repository key and Google repository</p> <p><pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n</code></pre> <pre><code>vim /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <pre><code>deb http://apt.kubernetes.io kubernetes-xenial main\n</code></pre> Update the list of packages. And install kubelet, kubeadm, and kubectl.</p> <p><pre><code>apt-get update \n</code></pre> <pre><code>apt-get install kubelet kubeadm kubectl -y\n</code></pre> Disable the swap.</p> <pre><code>swapoff -a\n</code></pre> <p>Setup Master Node to Connect with Worker Nodes</p> <p><pre><code>sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> <pre><code> echo NODENAME=$(hostname -s)\n</code></pre> <pre><code>kubeadm init --apiserver-advertise-address=172.31.85.246  --apiserver-cert-extra-sans=172.31.85.246  --pod-network-cidr=10.0.0.0/16 \n</code></pre> Now Create a Directory Name .kube in the master node home directory <pre><code>mkdir .kube\n</code></pre> Copy the Default conf file to the .kube directory <pre><code>cp /etc/kubernetes/admin.conf .kube/config\n</code></pre> Replace the Ip address with your Instance Private Ip Address and set pod Network </p> <p>After Executing kubeadm init command you will get one command as output that need to execute on worker nodes</p> <p><pre><code>kubeadm join 172.31.85.246:6443 --token n6v08z.53b057pfwnj8mq10         --discovery-token-ca-cert-hash sha256:9e8a38ddfc46c41ecb3317db9fc2145f70bddbdd2c6b8091fbdbab18e1dbcb19 \n</code></pre> Configuring the Calico in Master Node <pre><code>kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre> Now check Kubectl commands <pre><code>kubectl get node\n</code></pre> Now test the cluster </p> <p><pre><code>kubectl run pod --image=nginx\n</code></pre> <pre><code>kubectl get pod\n</code></pre> <pre><code>kubectl get pod -o wide\n</code></pre></p>"},{"location":"miscellaneous/activedir/","title":"MISCELLANEOUS","text":"<p>To Setup Active Directory Domain Service in Windows Server Click here</p>"},{"location":"miscellaneous/bluegreen_deployment/","title":"Blue-Green Deployment","text":"<p>Blue-Green Deployment is a deployment strategy that involves maintaining two identical environments: a \"blue\" environment representing the current production version and a \"green\" environment representing the new version or update. The deployment process follows these steps:</p> <p></p> <ol> <li> <p>Initially, the blue environment is live and serving user traffic.</p> </li> <li> <p>The new version or update is deployed and tested in the green environment, which is isolated from the live production environment.</p> </li> <li> <p>The green environment undergoes thorough testing and validation to ensure that the new version functions as expected and meets quality standards.</p> </li> <li> <p>Once the green environment is deemed stable and ready, a traffic switch is performed, redirecting user traffic from the blue environment to the green environment.</p> </li> <li> <p>Now, the green environment becomes the live production environment, serving user traffic, while the blue environment is kept as a backup.</p> </li> <li> <p>If any issues arise in the green environment, a rollback can be quickly performed by switching the traffic back to the blue environment.</p> </li> </ol>"},{"location":"miscellaneous/bluegreen_deployment/#the-key-benefits-of-blue-green-deployment-are","title":"The key benefits of blue-green deployment are:","text":"<ol> <li> <p>Reduced downtime and risk: By maintaining separate environments, the switch between blue and green can be done seamlessly, minimizing downtime and reducing the risk of potential issues affecting users.</p> </li> <li> <p>Quick rollback: If any problems are encountered in the green environment, the switch can be reverted instantly by directing traffic back to the blue environment.</p> </li> <li> <p>Controlled release: Blue-green deployment allows for thorough testing and validation of the new version in a separate environment, ensuring that any issues are identified before impacting users.</p> </li> <li> <p>Easy rollbacks: In case of issues, the blue environment is readily available as a fallback option, providing a safety net for the production environment.</p> </li> </ol> <p>Conclusion</p> <p>Overall, blue-green deployment provides a controlled and low-risk approach to deploying updates, promoting stability, availability, and fast recovery in case of failures.</p>"},{"location":"miscellaneous/canary/","title":"Canary Deployment","text":"<p>Canary deployment is a deployment strategy that involves gradually rolling out a new version or update to a subset of users or servers, allowing for testing and monitoring before fully deploying to the entire infrastructure. In canary mode, the deployment process follows these steps:</p> <p></p> <ol> <li> <p>A small portion of the user traffic or a subset of servers, typically referred to as the \"canary group,\" is selected to receive the new version or update.</p> </li> <li> <p>The new version is deployed to the canary group while the rest of the infrastructure, including the major infrastructure and all other components, continues to run the current stable version.</p> </li> <li> <p>The canary group starts receiving a fraction of the user traffic, and their behavior and performance are closely monitored and analyzed.</p> </li> <li> <p>If the new version performs well without any issues or negative impact on the canary group, the deployment is gradually expanded to include a larger portion of the infrastructure, including the major infrastructure and ultimately all components.</p> </li> <li> <p>Monitoring and metrics are continuously analyzed during the rollout to identify any anomalies, errors, or performance degradation across the entire infrastructure.</p> </li> <li> <p>If issues are detected, the rollout can be quickly halted or rolled back, preventing widespread impact and minimizing disruption to the major infrastructure and all other components.</p> </li> <li> <p>Once the new version has been successfully rolled out to the entire infrastructure, encompassing the major infrastructure and all components, and is proven to be stable, the canary deployment is completed.</p> </li> </ol>"},{"location":"miscellaneous/canary/#the-key-benefits-of-canary-deployments-including-major-infrastructure-and-all-infrastructure-are","title":"The key benefits of canary deployments, including major infrastructure and all infrastructure, are:","text":"<ol> <li> <p>Risk mitigation: By initially deploying the new version to a small subset of users or servers, any issues or bugs can be detected early, reducing the risk of a widespread negative impact on the major infrastructure and all components.</p> </li> <li> <p>Controlled rollout: Canary deployments allow for controlled and gradual rollouts, providing an opportunity to monitor the performance, stability, and user experience of the new version across the major infrastructure and all other components before a full deployment.</p> </li> <li> <p>Continuous monitoring: During the canary phase, metrics and monitoring systems help identify any abnormalities, errors, or performance issues across the major infrastructure and all components, enabling timely actions such as rollback or further optimization.</p> </li> <li> <p>Faster feedback loop: Canary deployments facilitate faster feedback and validation of the new version, allowing teams to make necessary adjustments or improvements based on real-world usage and feedback across the major infrastructure and all components.</p> </li> </ol> <p>Conclusion</p> <p>Overall, canary deployments provide a controlled and incremental approach to deploying updates across the major infrastructure and all components, enabling teams to validate the new version in a controlled environment and ensure a smooth transition to the updated system.</p>"},{"location":"miscellaneous/ldap/","title":"Install and Configure OpenLDAP Server on Ubuntu","text":""},{"location":"miscellaneous/ldap/#set-hostname-for-the-ubuntu-server","title":"Set hostname for the Ubuntu server","text":"<p><pre><code>sudo hostnamectl set-hostname ldap.k8sengineers.com\n</code></pre> Add the IP and FQDN to file /etc/hosts <pre><code>vim /etc/hosts\n</code></pre></p> <pre><code>192.168.0.151 ldap.k8sengineers.com \n</code></pre> <p>Note</p> <p>Replace ldap.k8sengineers.com with your correct hostname/valid domain name.</p>"},{"location":"miscellaneous/ldap/#install-openldap-server-on-ubuntu","title":"Install OpenLDAP Server on Ubuntu","text":"<p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt -y install slapd ldap-utils\n</code></pre> During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press  <p></p> <p>Confirm the password and continue the installation by selecting  <p>You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents.</p> <p><pre><code>sudo slapcat\n</code></pre> <pre><code>dn: dc=k8sengineers,dc=com\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: k8sengineers.com\ndc: k8sengineers\nstructuralObjectClass: organization\nentryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd\ncreatorsName: cn=admin,dc=k8sengineers,dc=com\ncreateTimestamp: 20211213045057Z\nentryCSN: 20211213045057.125264Z#000000#000#000000\nmodifiersName: cn=admin,dc=k8sengineers,dc=com\nmodifyTimestamp: 20211213045057Z\n\ndn: cn=admin,dc=k8sengineers,dc=com\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4=\nstructuralObjectClass: organizationalRole\nentryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd\ncreatorsName: cn=admin,dc=k8sengineers,dc=com\ncreateTimestamp: 20211213045057Z\nentryCSN: 20211213045057.128175Z#000000#000#000000\nmodifiersName: cn=admin,dc=k8sengineers,dc=com\nmodifyTimestamp: 20211213045057Z  \n</code></pre></p>"},{"location":"miscellaneous/ldap/#add-base-dn-for-users-and-groups","title":"Add base dn for Users and Groups","text":"<p>The next step is adding a base DN for users and groups. Create a file named basedn.ldif with the below contents:</p> <p><pre><code>vim basedn.ldif\n</code></pre> <pre><code>dn: ou=people,dc=k8sengineers,dc=com\nobjectClass: organizationalUnit\nou: people\n\ndn: ou=groups,dc=k8sengineers,dc=com\nobjectClass: organizationalUnit\nou: groups\n</code></pre> Replace k8sengineers and com with your correct domain components.</p> <p>Now add the file by running the command:</p> <p><pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f basedn.ldif\n</code></pre> Enter LDAP Password:</p> Output <pre><code>adding new entry \"ou=people,dc=k8sengineers,dc=com\"\nadding new entry \"ou=groups,dc=k8sengineers,dc=com\"\n</code></pre>"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups","title":"Add User Accounts and Groups","text":"<p>Generate a password for the user account to add.</p> <p><pre><code>sudo slappasswd\n</code></pre> Set the Password</p> Output <pre><code>{SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx\n</code></pre> <p>Create a ldif file for adding users.</p> <p><pre><code>vim ldapusers.ldif\n</code></pre> <pre><code>dn: uid=admin,ou=people,dc=k8sengineers,dc=com\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: shadowAccount\ncn: admin\nsn: Wiz\nuserPassword: {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx\nloginShell: /bin/bash\nuidNumber: 2000\ngidNumber: 2000\nhomeDirectory: /home/admin\n</code></pre></p> <ul> <li>Replace admin with the username to add</li> <li>dc=k8sengineers,dc=com with your correct domain values.</li> <li>cn &amp; sn with your Username Values</li> <li>{SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password</li> </ul> <p>When done with the edit, add an account by running.</p> <pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f ldapusers.ldif \n</code></pre> Output <pre><code>adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\"\n</code></pre> <p>Do the same for the group. Create a ldif file:</p> <p><pre><code>vim ldapgroups.ldif\n</code></pre> <pre><code>dn: cn=admin,ou=groups,dc=k8sengineers,dc=com\nobjectClass: posixGroup\ncn: admin\ngidNumber: 2000\nmemberUid: admin\n</code></pre> Add group:</p> <pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f ldapgroups.ldif\n</code></pre> Output <pre><code>adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\"\n</code></pre> <p>You can combine the two into single file.</p>"},{"location":"miscellaneous/ldap/#install-ldap-account-manager-on-ubuntu","title":"Install LDAP Account Manager on Ubuntu","text":"<p>Install Apache Web server &amp; PHP <pre><code>sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear\n</code></pre></p> <ul> <li> <p>For Ubuntu 22.04:sudo a2enconf php8.0-cgi</p> </li> <li> <p>For Ubuntu 20.04:sudo a2enconf php7.4-cgi</p> </li> <li> <p>For Ubuntu 18.04: sudo a2enconf php7.2-cgi</p> </li> </ul> <p>Here I'm using Ubuntu 20.04:</p> <p><pre><code>sudo a2enconf php7.4-cgi\n</code></pre> <pre><code>sudo systemctl reload apache2\n</code></pre> Install LDAP Account Manager <pre><code>sudo apt -y install ldap-account-manager\n</code></pre> Configure LDAP Account Manager <pre><code>http://&lt; IP address &gt;/lam\n</code></pre> </p> <p>The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. </p> <p>Then click on, Edit server profiles</p> <p>This will ask you for the LAM Profile name Password</p> <p>Note</p> <p>The default password is lam</p> <p> </p> <p>The first thing to change is Profile Password, this is at the end of the General Settings page.</p> <p></p> <p>Next is to set the LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in the server hostname.</p> <p></p> <p>Set Dashboard login by specifying the admin user account and domain components under the \u201cSecurity settings\u201d section</p> <p></p> <p>Switch to the \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes.</p> <p></p>"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups-with-ldap-account-manager","title":"Add user accounts and groups with LDAP Account Manager","text":"<p>Log in with the account admin to the LAM dashboard to start managing user accounts and groups.</p> <p></p> <p>Add User Group</p> <p>Give the group a name, optional group ID, and description. </p> <p>Add User Accounts Once you have the groups for user accounts to be added, click on Users &gt; New user to add a new user account to your LDAP server. You have three sections for user management:</p> <ul> <li>Personal \u2013 This contains the user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c</li> </ul> <p></p>"},{"location":"miscellaneous/ldap/#configure-your-ubuntu-220420041804-as-ldap-client","title":"Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client","text":""},{"location":"miscellaneous/ldap/#the-last-step-is-to-configure-the-systems-in-your-network-to-authenticate-against-the-ldap-server-weve-just-configured","title":"The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured:","text":"<p>Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. <pre><code>sudo vim /etc/hosts\n</code></pre> <pre><code>192.168.18.50 ldap.k8sengineers.com\n</code></pre> Install LDAP client utilities on your Ubuntu system:</p> <p><pre><code>sudo apt -y install libnss-ldap libpam-ldap ldap-utils\n</code></pre> Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname </p> <ul> <li> <p>Set a Distinguished name for the search base </p> </li> <li> <p>Select LDAP version 3</p> </li> <li>Select Yes for Make local root Database admin</li> <li>Answer No for Does the LDAP database require login?</li> <li>Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com</li> <li>Provide LDAP root account Password</li> </ul> <p>After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. <pre><code>vim /etc/nsswitch.conf\n</code></pre> <pre><code>passwd: compat systemd ldap\ngroup: compat systemd ldap\nshadow: compat\n</code></pre></p> <p>Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. <pre><code>vim /etc/pam.d/common-password\n</code></pre> <pre><code>password [success=1 user_unknown=ignore default=die] pam_ldap.so try_first_pass\n</code></pre></p> <p>Enable creation of home directory on the first login by adding the following line to the end of file /etc/pam.d/common-session <pre><code>session optional pam_mkhomedir.so skel=/etc/skel umask=077\n</code></pre> See the below screenshot: </p> <p>Test by switching to a user account on LDAP</p> <pre><code>sudo su - &lt;username&gt;\n</code></pre>"},{"location":"miscellaneous/nexus/","title":"Nexus Installation Setup","text":"Instance Details Resources CentOS - t2.medium 4GB Ram , 2 CPU Nexus Script <pre><code>#!/bin/bash\nyum install java-1.8.0-openjdk.x86_64 wget -y   \nmkdir -p /opt/nexus/   \nmkdir -p /tmp/nexus/                           \ncd /tmp/nexus\nNEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\"\nwget $NEXUSURL -O nexus.tar.gz\nEXTOUT=`tar xzvf nexus.tar.gz`\nNEXUSDIR=`echo $EXTOUT | cut -d '/' -f1`\nrm -rf /tmp/nexus/nexus.tar.gz\nrsync -avzh /tmp/nexus/ /opt/nexus/\nuseradd nexus\nchown -R nexus.nexus /opt/nexus \ncat &lt;&lt;EOT&gt;&gt; /etc/systemd/system/nexus.service\n[Unit]                                                                          \nDescription=nexus service                                                       \nAfter=network.target                                                            \n\n[Service]                                                                       \nType=forking                                                                    \nLimitNOFILE=65536                                                               \nExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start                                  \nExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop                                    \nUser=nexus                                                                      \nRestart=on-abort                                                                \n\n[Install]                                                                       \nWantedBy=multi-user.target                                                      \n\nEOT\n\necho 'run_as_user=\"nexus\"' &gt; /opt/nexus/$NEXUSDIR/bin/nexus.rc\nsystemctl daemon-reload\nsystemctl start nexus\nsystemctl enable nexus\n</code></pre>"},{"location":"miscellaneous/sonarqube/","title":"SonarQube Installation Script","text":"Instance Details Resources Ubuntu - t2.medium 4GB Ram , 2 CPU SonarQube Script <pre><code>#!/bin/bash\ncp /etc/sysctl.conf /root/sysctl.conf_backup\ncat &lt;&lt;EOT&gt; /etc/sysctl.conf\nvm.max_map_count=262144\nfs.file-max=65536\nulimit -n 65536\nulimit -u 4096\nEOT\ncp /etc/security/limits.conf /root/sec_limit.conf_backup\ncat &lt;&lt;EOT&gt; /etc/security/limits.conf\nsonarqube   -   nofile   65536\nsonarqube   -   nproc    409\nEOT\n\nsudo apt-get update -y\nsudo apt-get install openjdk-11-jdk -y\nsudo update-alternatives --config java\njava -version\n\nsudo apt update\nwget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -\n\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'\nsudo apt install postgresql postgresql-contrib -y\n#sudo -u postgres psql -c \"SELECT version();\"\nsudo systemctl enable postgresql.service\nsudo systemctl start  postgresql.service\nsudo echo \"postgres:admin123\" | chpasswd\nrunuser -l postgres -c \"createuser sonar\"\nsudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\"\nsudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\"\nsudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\"\nsystemctl restart  postgresql\n#systemctl status -l   postgresql\nnetstat -tulpena | grep postgres\nsudo mkdir -p /sonarqube/\ncd /sonarqube/\nsudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip\nsudo apt-get install zip -y\nsudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/\nsudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube\nsudo groupadd sonar\nsudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar\nsudo chown sonar:sonar /opt/sonarqube/ -R\ncp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup\ncat &lt;&lt;EOT&gt; /opt/sonarqube/conf/sonar.properties\nsonar.jdbc.username=sonar\nsonar.jdbc.password=admin123\nsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube\nsonar.web.host=0.0.0.0\nsonar.web.port=9000\nsonar.web.javaAdditionalOpts=-server\nsonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError\nsonar.log.level=INFO\nsonar.path.logs=logs\nEOT\n\ncat &lt;&lt;EOT&gt; /etc/systemd/system/sonarqube.service\n[Unit]\nDescription=SonarQube service\nAfter=syslog.target network.target\n\n[Service]\nType=forking\n\nExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start\nExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop\n\nUser=sonar\nGroup=sonar\nRestart=always\n\nLimitNOFILE=65536\nLimitNPROC=4096\n\n\n[Install]\nWantedBy=multi-user.target\nEOT\n\nsystemctl daemon-reload\nsystemctl enable sonarqube.service\n#systemctl start sonarqube.service\n#systemctl status -l sonarqube.service\napt-get install nginx -y\nrm -rf /etc/nginx/sites-enabled/default\nrm -rf /etc/nginx/sites-available/default\ncat &lt;&lt;EOT&gt; /etc/nginx/sites-available/sonarqube\nserver{\n    listen      80;\n    server_name sonarqube.groophy.in;\n\n    access_log  /var/log/nginx/sonar.access.log;\n    error_log   /var/log/nginx/sonar.error.log;\n\n    proxy_buffers 16 64k;\n    proxy_buffer_size 128k;\n\n    location / {\n        proxy_pass  http://127.0.0.1:9000;\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n        proxy_redirect off;\n\n        proxy_set_header    Host            \\$host;\n        proxy_set_header    X-Real-IP       \\$remote_addr;\n        proxy_set_header    X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header    X-Forwarded-Proto http;\n    }\n}\nEOT\nln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube\nsystemctl enable nginx.service\n#systemctl restart nginx.service\nsudo ufw allow 80,9000,9001/tcp\n\necho \"System reboot in 30 sec\"\nsleep 30\nreboot\n</code></pre> SonarQube-Analysis-Properties <pre><code>sonar.projectKey=vprofile\nsonar.projectName=vprofile-repo\nsonar.projectVersion=1.0\nsonar.sources=src/\nsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/\nsonar.junit.reportsPath=target/surefire-reports/\nsonar.jacoco.reportsPath=target/jacoco.exec\nsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml\n</code></pre>"}]}