{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Blog","text":"<p> <p>About</p> <p></p>"},{"location":"about/","title":"About","text":"<p>Hello, my name is SAITEJA IRRINKI, and I work as a Senior DevOps Engineer in Build &amp; Release. I'm experienced in Infrastructure Provisioning, Configuration Management, Version Control, Code Quality, Environment Administration, Defect Tracking, Release Management, Continuous Integration, and Continuous Delivery, Cloud Computing, Scripting for Automation, Static Web Development, Orchestration-Technologies, and Operating Systems- Linux and Windows.</p> <p> +91 9493322788</p> <p> saitejairrinki91@gmail.com</p> <p> https://www.instagram.com/saitejairrinki/ </p> <p> https://saitejairrinki.medium.com/</p>"},{"location":"awsdevops/","title":"DevOps","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization\u2019s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p> <p></p>"},{"location":"blog/","title":"My DevOps Blog","text":"<p>Welcome to my DevOps Blog. Here you will find a collection of articles and tutorials on various DevOps topics. </p> <p>Whether you are just getting started with DevOps or a pro, I hope you find something useful here. </p> <p>This blog is all about my thoughts, experiences, Challanges and lessons learned in the world of DevOps.</p> <p>I have been working in the IT industry and have been involved in a variety of roles including InfraStructure Provisioning, system administration and operations. In recent years I have become very interested in the DevOps movement and how it can help organisations to improve the way they deliver software.</p> <p>I hope you enjoy reading my blog and please feel free to leave any comments or questions</p> <p> saitejairrinki91@gmail.com</p> <p> +91 9493322788</p> <p>The majority of the content in this blog is created by me through research and conducting POC, while only a small number of items are sourced from the internet, such as definitions,icons,pictures and etc used for reference.</p> <p>Thanks for reading!</p>"},{"location":"fund/","title":"Fuel the Continuous Learning Journey!","text":"<p>\ud83d\ude80 Support Our DevOps Blog - Fueling Insights, One Post at a Time! \ud83d\udee0\ufe0f </p> <p>Hey there, fellow DevOps enthusiasts,</p> <p>I hope our DevOps blog has been a helpful resource on our journey to mastering all things DevOps. Your support can take this learning adventure even further!</p> <p>Why Support?</p> <p>Maintaining a blog takes dedication, time, and resources. As much as I'm fueled by my passion for DevOps, a little support goes a long way in enabling us to continue producing high-quality, well-researched, and practical content. Your contributions will help cover expenses like hosting, domain registration, and the tools necessary to experiment with the latest DevOps technologies. With your support, we can dedicate more time to writing, testing new concepts, and diving deeper into the ever-evolving world of DevOps.</p> <p>How Can You Help?</p> <p>Contribution: A small contribution can go a long way in keeping the blog alive and thriving. </p> <p>Spread the Word: Share the blog with friends and colleagues who share our passion.</p> <p>Your support means the world and keeps the DevOps knowledge flowing. Let's continue this journey together!</p> <p>Cheers,  </p> <p>SAITEJA IRRINKI </p>"},{"location":"job_roles/","title":"DevOps ENGINEER ROLES","text":""},{"location":"job_roles/#aws-tasks","title":"AWS Tasks:","text":"<p>Managing Ec2 Instances, EIP, Network Interfaces, Security Groups &amp; Key Pairs  Managing EBS Volumes, AMI &amp; Snapshots (Backup &amp; Restore, Migration, etc.) Setup &amp; Managing Elastic Load Balancers, ACM &amp; Autoscaling Groups Setting &amp; Managing Cloudwatch Alarms on metrics from Ec2, ELB &amp; RDS  Creating &amp; Managing RDS Instances, RDS Snapshots, Updating Parameters Groups  AWS CLI for any AWS Tasks</p>"},{"location":"job_roles/#cloud-migration-using-lift-shift-strategy-on-aws-services-using","title":"Cloud Migration using Lift &amp; Shift Strategy on AWS, Services using","text":"<ul> <li> <p>VPC, Ec2, S3, Application Load Balancer, Route53 </p> </li> <li> <p>IAM to give secure access to AWS account using MFA</p> </li> <li> <p>Tightly controlled Security Group for firewall rules of EC2</p> </li> <li> <p>EBS volume for storage on Ec2, Snapshot for Backups of EBS</p> </li> <li> <p>Autoscaling for Automatic scaling of Ec2 instance based on CPU usage</p> </li> <li> <p>Modernization on AWS Cloud, Services used</p> </li> <li> <p>Beanstalk for PAAS for Tomcat Web App</p> </li> <li> <p>RDS for MySQL Database</p> </li> <li> <p>Object storage S3 to store and retrieve files</p> </li> <li> <p>Route53 for Private &amp; Public Hosted zones/Records</p> </li> <li> <p>Amazon MQ for fully managed RabbitMQ</p> </li> <li> <p>ElastiCache for in-memory datastore in cloud</p> </li> <li> <p>Monitoring with CloudWatch, Grafana</p> </li> <li> <p>Notification using SNS</p> </li> </ul>"},{"location":"job_roles/#aws-cloud-automation-using","title":"AWS Cloud Automation using","text":"<ul> <li>Ansible</li> <li>CloudFormation (Stacks)</li> </ul>"},{"location":"job_roles/#aws-securities","title":"AWS Securities","text":"<ul> <li>Inspector &amp; Best practices</li> <li>IAM management</li> </ul>"},{"location":"job_roles/#azure-azure-devops","title":"Azure &amp; Azure DevOps","text":"<ul> <li>Automated the provisioning of Azure resources (App Registration, Blob, Back-up Center, Cosmos DB, Key Vault, Azure VMs) using PowerShell and Templates.</li> <li>Created a Build/Release pipeline with automated build and Continuous-Integration using Azure DevOps</li> </ul>"},{"location":"job_roles/#continuous-delivery-of-java-web-application","title":"Continuous Delivery of Java Web Application","text":"<ul> <li> <p>CICD Pipeline using Jenkins, Git, Maven, Nexus, S3 &amp; SonarQube</p> </li> <li> <p>Deploying Artifact to Beanstack</p> </li> <li> <p>Jenkins Pipeline As A Code for CICD</p> </li> <li> <p>Website Automation through Jenkins whenever there is a GIT Commit</p> </li> </ul>"},{"location":"job_roles/#configuration-management-using-ansible","title":"Configuration Management using Ansible","text":"<ul> <li>Ansible AdHoc commands to execute remote tasks</li> <li>Ansible playbook for Service/Server Deployments</li> <li>Ansible playbook to setup VPC &amp; Bastion Host on AWS</li> <li>Writing our own configuration file (ansible.cfg)</li> <li>Ansible Roles for modular &amp; reusable automation framework</li> </ul>"},{"location":"job_roles/#docker-containers","title":"Docker Containers","text":"<ul> <li>Building customized docker images using Dockerfile</li> <li>Docker-compose to define &amp; run MultiContainer Docker Application</li> </ul>"},{"location":"job_roles/#kubernetes","title":"Kubernetes","text":"<ul> <li>Creating Production grade K8s cluster using Kops &amp; Kubeadm</li> <li>Hosting Containerized Application on K8s cluster using Pod, Service, Replication Controller, Deployment, Secrets &amp; ConfigMap</li> </ul>"},{"location":"job_roles/#mkdocs","title":"MkDocs","text":"<ul> <li>Designing and Building a Static Website using MkDocs (The Current Website you're watching) </li> </ul>"},{"location":"profile_summary/","title":"PROFILESUMMARY","text":"<ul> <li>Experience in IT area comprising the configuration management, Deploy, CI/CD pipeline, AWS, and DevOps methodologies.</li> <li>Expertise in troubleshooting the problems generated while building and deploying.</li> <li>Working Experience on Git, GitHub, and Jenkins. Debugging issues if there is any failure in broken Jenkins builds and maintaining Jenkins build pipeline.</li> <li>Expertise Knowledge in Source Code Management (version control system) tools using GIT. </li> <li>Experienced in Branching, Merging, and Tagging concepts in Version Control tools like GIT.</li> <li>Proficient in developing Continuous Integration / Continuous Delivery pipelines.</li> <li>Experience with containerization tools like Docker and Kubernetes. </li> <li>Implemented Docker-based Continues Integration and Deployment framework.</li> <li>Strong experience in building tools and packaging the source code using Maven. </li> <li>Scheduled builds overnight to support development needs using Jenkins, Git, and Maven. </li> <li>Experience in integrating Unit Tests and Code Quality Analysis Tools like SonarQube. </li> <li>Experience in using Nexus Repository Manager and S3 Bucket for Maven builds.</li> <li>Experience in orchestration tools like Ansible and Kubernetes. </li> <li>Experience with services IAM, Compute Engine, Kubernetes Engine, Storage Services, S3 Bucket, and VPC Network.</li> <li>Experience with Amazon Web services Creating, configuring, and Managing EC2, Storage, IAM, S3, VPC, ELB, EFS, SNS, Route53, and some more services in AWS.</li> <li>Experience using Apache Tomcat &amp; Red Hat Server application servers for deployments.</li> <li>Working experience with operating systems like Linux and Windows.</li> <li>Performed continuous Build and Deployments to multiple DEV, QA, PRE-Prod, and PROD environments.</li> <li>Working Experience in Azure Cloud and Azure DevOps</li> <li>Ensuring security and compliance across pipelines and repositories using Azure DevOps provides security features like RBAC.</li> <li>Performed continuous Build and Deployments to multiple DEV, QA, Validation and PROD Environments</li> <li>Know about Google Cloud Platform.</li> <li>Have good Knowledge of Terraform.</li> <li>Have good Knowledge of Shell Scripting.</li> <li>Have Knowledge of Kubernetes Core Concepts.</li> <li>Experience with Styra Declarative Authorization Service (DAS).</li> <li>Known about Open Policy Agent (OPA).</li> </ul>"},{"location":"profile_summary/#academic-details","title":"ACADEMIC DETAILS","text":"<ul> <li>B.Tech - 2020</li> </ul>"},{"location":"profile_summary/#professional-experience","title":"PROFESSIONAL EXPERIENCE","text":"<ul> <li>Working as a DevOps Engineer from November 2020 to till-date.</li> </ul>"},{"location":"profile_summary/#strengths","title":"STRENGTHS","text":"<ul> <li>Flexibility and adaptability to work in any environment.</li> <li>Good troubleshooting skills.</li> <li>Willingness to accept any challenge irrespective of its complexity.</li> <li>Good team player with a positive attitude.</li> </ul>"},{"location":"project/","title":"Projects","text":""},{"location":"project/#project-1","title":"PROJECT 1","text":""},{"location":"project/#role-devops-engineer","title":"Role: DevOps Engineer","text":""},{"location":"project/#environment-git-github-maven-apache-tomcat-jenkins-linux-sonarqube-nexus-aws-ansible","title":"Environment: Git, GitHub, Maven, Apache Tomcat, Jenkins, Linux, SonarQube, Nexus, AWS, Ansible.","text":""},{"location":"project/#roles-and-responsibilities","title":"Roles and Responsibilities :","text":"<ul> <li>Configured Git with Jenkins and scheduled jobs using the POLL SCM option</li> <li>Installed and configured GIT and communicated with the repositories in GitHUB. Collaborate with different teams to deploy application code into Dev, QA, and Staging.</li> <li>Installing and updating the Jenkins plugins to achieve CI/CD.</li> <li>Responsible for installing Jenkins master and slave nodes.</li> <li>Created Jenkins CICD pipelines for continuous build &amp; deployment and integrated Junit and SonarQube plugins in Jenkins for automated testing and code quality check.</li> <li>Integrated SonarQube with Jenkins for continuous inspection of code quality and analysis with SonarQube scanner for Maven.</li> <li>Managed Sonatype Nexus repositories to download the artifacts (jar, war &amp; ear) during the build.</li> <li>Wrote playbook manifests for deploying, configuring, and managing components.</li> <li>Managing the working environments through configuration management tools ansible.</li> <li>Working with developers and Testers to test the source code and applications through Jenkins plugins.</li> <li>Installation of apache, tomcat, and troubleshooting web server issues.</li> <li>Administration and maintenance of servers using Red Hat Linux/CentOS-7,8.</li> <li>Installing and configuration of ansible server. * Implemented AWS solutions using EC2, S3, EBS, ELB, Route53, Auto scaling groups.</li> <li>Built servers using AWS, importing volumes, launching EC2, creating Security groups, Auto-scaling, Load balancers (ELBs) using Cloud formation templates &amp; AMIs using Infrastructure as a Service (IaaS Including EC2 and S3), focusing on high availability, fault tolerance, and auto-scaling.</li> <li>Configured ELB with different launch configurations using AMI and EC2 Autoscaling groups.</li> <li>Creating S3 buckets and S3 life cycle policies and bucket policies (Read/Write).</li> <li>Creating EBS Volumes and snapshots and attaching them to the EC2 instances.</li> </ul>"},{"location":"project/#project-2","title":"PROJECT 2","text":""},{"location":"project/#role-devops-engineer_1","title":"Role: DevOps Engineer","text":""},{"location":"project/#environment-git-github-maven-nexus-sonarqube-jenkins-docker-kubernetes-aws-linux","title":"Environment: Git, GitHub, Maven, Nexus, SonarQube, Jenkins, Docker, Kubernetes, AWS, Linux.","text":""},{"location":"project/#roles-and-responsibilities_1","title":"Roles and Responsibilities :","text":"<ul> <li>Involved in CI/CD process and integrated GIT, Nexus, SonarQube, and Maven artifacts build with and and and and Jenkins and creating Docker image and using the Docker image to deploy over Kubernetes. </li> <li>Building and deploying various microservices in EKS. </li> <li>Creating and maintaining namespaces, config maps, secrets, service, ingress, RBAC in Kubernetes. </li> <li>Implemented and maintained the Branching and build/ release strategies utilizing GIT. </li> <li>Experience with container-based deployments using Docker, working with Docker Images, Docker Hub and Docker-registries and Kubernetes. </li> <li>Building/Maintaining Docker container clusters managed by Kubernetes Linux, Bash, GIT, Docker. </li> <li>Implemented docker-maven-plugin in maven pom to build docker images for all microservices and later used Docker file to build the docker images from the java jar files. </li> <li>Utilized Kubernetes for the runtime environment of the CI/CD system to build, and test deploy. </li> <li>Experience in working on AWS and its services like AWS IAM, VPC, EC2, EKS, EBS, S3, ELB, Auto Scaling, Route 53, Cloud Front, Cloud Watch, Cloud Trail, and SNS. </li> <li>Experienced in Cloud automation using AWS Cloud Formation templates to create customized VPC, subnets, NAT, EC2 instances, ELB, and Security groups. </li> <li>Experienced in creating complex IAM policies, Roles, and user management for delegated acceaccess withAWS.</li> <li>Identify, troubleshoot and resolve issues related to the build and deploy process. </li> <li>Deploying Docker images in Kubernetes cluster using Yaml files and exposing the application to the internet using service object.</li> </ul>"},{"location":"project/#project-3","title":"PROJECT 3","text":""},{"location":"project/#role-devops-teamlead","title":"Role: DevOps - Teamlead","text":""},{"location":"project/#environment-azure-cloud-services-azure-devops-powershell-power-apps-windows-os","title":"Environment: Azure Cloud Services, Azure DevOps, PowerShell, Power Apps, Windows OS.","text":""},{"location":"project/#roles-and-responsibilities_2","title":"Roles and Responsibilities:","text":"<ul> <li>Worked with team to Develop Application Requirements.</li> <li>Created a Workflow &amp; Branching Strategy with the Requirements.</li> <li>Setup/managed Azure Repos and Branching Strategy.</li> <li>Established a Build/Release pipeline with automated build and Continuous-Integration.</li> <li>Automated the provisioning of Powerapps Environments (Dev, Staging, Pre-Prod, Prod) using Azure DevOps Tasks and Azure resources (App Registration, Blob, Back-up Center, Cosmos DB, Key Vault, Azure VMs) using PowerShell and Templates.</li> <li>Validated Azure Resources during provisioning &amp; pre-configuration by writing a PowerShell Script.</li> <li>Monitored Automated build &amp; Continuous-Integration process to drive Build/Release Failure Resolution.</li> <li>Automated and Implemented system backup and recovery procedures.</li> <li>Worked with Software Development and Testing-Team members to design and develop robust solutions to meet client requirements for functionality, scalability, and performance.</li> <li>Documented project design for reference and future use cases</li> <li>Involved in CI/CD process and integrated SonarQube with Azure DevOps for QualityAnalysis.</li> <li>Have hands-on experience in writing PowerShell when needed to automate jobs and tasks in the pipeline.</li> <li>Experience deploying infrastructure resources utilizing ARM templates.</li> <li>Maintained Source code Repositories</li> <li>Implemented high availability with Azure classic and Azure Resource Manager (ARM) deployment models.</li> <li>Experience delivering applications in Azure.</li> <li>Ability to Document deployment processes and transfer knowledge to other operational team members.</li> <li>Ensure all cloud deployments follow established workflows.</li> <li>Supports Development and production systems and deployments.</li> </ul>"},{"location":"resume/","title":"Resume","text":"<p>To View My Resume please click here  Resume</p>"},{"location":"technical_skills/","title":"TECHNICAL SKILLS","text":"Category Tools Technologies &amp; Softwares Operating Systems - Linux   Windows  Virtualization - Vagrant Cloud Platform - AWS    AZURE   GCP  Scripting Languages - Shell Scripting  PowerShell  Configuration Management Tool - Ansible  Infrastructure as Code - Terraform   Cloud Formation  Version Control Tool - GIT  Build Software - Maven Build &amp; Release/CICD - Azure DevOps  Jenkins  Containerization Tools - Docker   Kubernetes  Web/App Server - Apache Tomcat  Ticketing Tool - Freshdesk  Static Web Development - MkDocs Repository Manager - Nexus Repository Monitoring - CloudWatch  Grafana Code Quality Analysis - SonarQube/Cloud"},{"location":"azure/branchingstrategy/","title":"Branching strategies","text":"<p>Azure DevOps supports several branching strategies that teams can use to manage source code and collaborate on development. Here are some of the most common branching strategies:</p> <p>Mainline/Branching: In this strategy, all developers work on a single branch, usually called \"master\" or \"main\". Developers make changes directly to this branch, and all changes are integrated and tested continuously. This strategy is best for small teams or small projects where the development cycle is short.</p> <p>Feature Branching: In this strategy, each new feature is developed on a separate branch, which is later merged into the main branch when the feature is complete. This strategy allows multiple features to be developed simultaneously without impacting the main branch. It is a common strategy for large projects with multiple teams working on different features.</p> <p>Release Branching: In this strategy, a separate branch is created for each release. Development work is done on the main branch, and when it's time for a release, a new branch is created from the main branch. Any bug fixes or changes are made on this branch and then merged back into the main branch after the release is completed. This strategy is best for projects with frequent releases.</p> <p>Gitflow: This is a variation of the feature branching strategy. It involves two long-lived branches: \"master\" and \"develop\", and multiple short-lived branches for feature development and hotfixes. Features are developed on separate branches and then merged into the \"develop\" branch for integration testing. Once the feature is complete, it is merged into the \"master\" branch for release.</p> <p></p> <p>Note</p> <p>These are just a few of the branching strategies that can be used with Azure DevOps. The best strategy for a team depends on the project requirements, team size, and development cycle. It is essential to choose a strategy that aligns with the team's workflow and ensures smooth collaboration and code management.</p>"},{"location":"azure/comparison/","title":"Azure DevOps vs Jenkins","text":"<p>Azure DevOps and Jenkins are two popular tools for continuous integration and delivery (CI/CD) that help teams to automate the software development and delivery process. </p>"},{"location":"azure/comparison/#here-are-some-key-differences-between-azure-devops-and-jenkins","title":"Here are some key differences between Azure DevOps and Jenkins:","text":"<p>Hosted vs. Self-hosted: Azure DevOps is a cloud-based service, while Jenkins is a self-hosted solution that can be installed on-premises or on a virtual machine. This means that Azure DevOps offers the convenience of not having to worry about maintaining servers, while Jenkins offers more control over the environment.</p> <p>Integration with Microsoft Tools: Azure DevOps is tightly integrated with Microsoft's development tools, including Visual Studio and Azure. This integration allows for seamless collaboration and provides a more cohesive solution for teams that use Microsoft's tools. Jenkins, on the other hand, has a wider range of integrations with various tools and platforms.</p> <p>Ease of use: Azure DevOps provides a user-friendly interface that is easy to navigate and use, making it an excellent option for small to medium-sized teams. Jenkins, however, can be more complex and requires more technical knowledge to set up and configure.</p> <p>Security: Azure DevOps provides enterprise-grade security and compliance features, such as multi-factor authentication, encryption, and compliance with industry standards. Jenkins, on the other hand, requires additional plugins and configurations to provide the same level of security.</p> <p>Cost: Azure DevOps offers a range of pricing options, including a free tier for small teams. Jenkins, on the other hand, is open-source and free to use, but may require additional costs for hosting and maintenance.</p> <p>Conclusion</p> <p>Overall, Azure DevOps is a more integrated and user-friendly solution that is better suited for small to medium-sized teams or those using Microsoft tools. Jenkins, on the other hand, offers more flexibility and control for larger teams and organizations that require a more customized CI/CD solution.</p>"},{"location":"azure/installation/","title":"A Step-by-Step Guide to Installing Azure DevOps \ud83d\udce5","text":"<p>Are you ready to dive into the world of Azure DevOps and streamline your software development projects? Azure DevOps is a powerful set of tools and services designed to facilitate collaboration, automation, and continuous delivery in your development workflow. In this guide, we'll walk you through the installation process, ensuring you're set up for success right from the beginning. \ud83d\udee0\ufe0f</p>"},{"location":"azure/installation/#prerequisites","title":"Prerequisites \ud83d\udccb","text":"<p>Before we begin, ensure that you have the following prerequisites in place:</p> <ul> <li>Microsoft Account \ud83d\udc64: You'll need a Microsoft account to access Azure DevOps. If you don't have one, you can create it here.</li> </ul> <p>Now, let's get started with the installation.</p>"},{"location":"azure/installation/#1-azure-devops-account-setup","title":"1. Azure DevOps Account Setup \ud83c\udfd7\ufe0f","text":"Account Creation <p>Setting up your first project in Azure DevOps</p> <p>The first step is to set up your Azure DevOps account. Follow these steps:</p> <ul> <li>Visit Azure DevOps and sign in with your Microsoft account.</li> <li>Click on the \"New Project\" button to create your first project.</li> <li>Give your project a name and choose the appropriate visibility settings (public or private). You can also select a template based on your project type.</li> <li>Click \"Create\" to set up your project.</li> </ul>"},{"location":"azure/installation/#2-run-a-basic-hello-world-pipeline","title":"2. Run a Basic Hello World Pipeline \ud83d\ude80","text":"First Pipeline <p>creating and running your first pipeline in Azure DevOps</p> <p>With your project set up, it's time to create and run your first pipeline. A pipeline automates your software build, test, and deployment processes. Here's how to set up a basic \"Hello World\" pipeline:</p> <ul> <li>In your Azure DevOps project, navigate to the \"Pipelines\" section.</li> <li>Click on the \"New Pipeline\" button.</li> <li>Follow the guided steps to configure your pipeline. You'll define your source code repository (e.g., GitHub, Azure Repos), choose a template (e.g., Starter pipeline), and define the build and deployment steps.</li> <li>Save and run your pipeline.</li> </ul> <p>Enable/Disable Classic Pipelines</p> <p>If you're unable to locate the classic pipeline creation option, it suggests that classic pipeline creation has been turned off in either your organization or project settings. To activate this capability, please refer to the provided Document link</p>"},{"location":"azure/installation/#additional-tips","title":"Additional Tips \ud83d\udcda","text":"<ul> <li> <p>Integration \ud83d\udd04: Azure DevOps integrates seamlessly with popular development tools, version control systems, and cloud platforms. Explore the integrations that best suit your project's needs.</p> </li> <li> <p>Extensions \ud83e\udde9: Azure DevOps offers a marketplace of extensions that can enhance your workflow. Browse and install extensions that can automate tasks, add reporting capabilities, and more.</p> </li> <li> <p>Security \ud83d\udd10: Ensure that your Azure DevOps environment is secure by setting up proper access controls, authentication, and continuous security monitoring.</p> </li> <li> <p>Documentation \ud83d\udcd6: Azure DevOps provides extensive documentation and tutorials. Don't hesitate to consult the official documentation for in-depth information on specific features and best practices.</p> </li> </ul> <p>By following these steps and utilizing the resources available, you'll be well on your way to harnessing the power of Azure DevOps for efficient and collaborative software development. Good luck with your projects! \ud83c\udf1f</p>"},{"location":"azure/intro/","title":"Introduction","text":"<p>Azure DevOps is a comprehensive set of development tools that provides an integrated and collaborative environment for software development teams. It offers a suite of services that includes version control, agile project management, continuous integration and delivery, testing and release management capabilities.</p> <p>Azure DevOps was previously known as Visual Studio Team Services (VSTS) and provides cloud-hosted services that can be accessed from anywhere with an internet connection. It is a cloud-based solution that allows teams to collaborate on projects, track progress, manage code repositories, build and test applications, and deploy applications to production.</p>"},{"location":"azure/intro/#some-of-the-key-features-of-azure-devops-include","title":"Some of the key features of Azure DevOps include:","text":"<p>Azure Repos: This is a cloud-based version control system that allows teams to manage and share code securely across the organization. It supports both Git and Team Foundation Version Control (TFVC) repositories.</p> <p>Azure Boards: This is a project management tool that supports agile methodologies such as Scrum and Kanban. It allows teams to plan and track work, create backlogs, and manage sprints and iterations.</p> <p>Azure Pipelines: This is a continuous integration and delivery (CI/CD) tool that allows teams to build, test, and deploy applications to multiple platforms and environments, including cloud platforms like Azure and AWS.</p> <p>Azure Test Plans: This is a testing tool that enables teams to test applications across different platforms and devices. It includes features like exploratory testing, test case management, and automated testing.</p> <p>Azure Artifacts: This is a package management tool that allows teams to manage and share packages, such as code libraries and dependencies, across the organization.</p>"},{"location":"azure/intro/#azure-devops-basic-workflow-digaram","title":"Azure DevOps Basic Workflow Digaram","text":"<p>Conclusion</p> <p>Overall, Azure DevOps provides a comprehensive suite of tools that enables software development teams to collaborate effectively and deliver high-quality applications at a faster pace</p>"},{"location":"azure/playwright/","title":"Automation Testing using Playwright","text":"<p>Playwright is a popular open-source testing framework for web applications that allows developers to write end-to-end tests in a simple and concise manner. With Azure DevOps, you can automate your Playwright tests and integrate them into your CI/CD pipeline for continuous testing and deployment.</p> <p>Here are the steps to set up Playwright automation with Azure DevOps:</p> <ul> <li> <p>Create a new project in Azure DevOps: Navigate to the Azure DevOps portal and create a new project.</p> </li> <li> <p>Set up a build pipeline: In the project, create a new build pipeline and configure it to use the appropriate build agent. Then, add a task to install the necessary dependencies for running Playwright tests, such as Node.js and Playwright.</p> </li> </ul> <pre><code>npm init playwright@latest\n</code></pre> <ul> <li>Configure the Playwright test task: Add a new task to the build pipeline for running Playwright tests. This task should run the command to execute your Playwright tests, such as \"npx playwright test\" or \"npm test\". You can also configure this task to specify the browser and environment in which the tests should run.</li> </ul> <pre><code>npx playwright test\n\nnpm test\n</code></pre> <ul> <li> <p>Set up test reporting: To view the results of your Playwright tests in Azure DevOps, you can add a test reporting task to the build pipeline. This task should publish the test results to Azure DevOps so that you can view them in the test summary.</p> </li> <li> <p>Set up continuous testing: Once your build pipeline is set up, you can configure it to trigger automatically whenever changes are made to your code repository. This will enable continuous testing and allow you to catch any issues early in the development process.</p> </li> </ul> <p>By following these steps, you can set up Playwright automation with Azure DevOps and integrate it into your development workflow for continuous testing and deployment.</p> <p>Info</p> <p>You can automate some DevOps operations that lack automation or API support using PlayWright by recording and playing, however this is not a recommended procedure as it may fail if there are any changes to the UI or platform side.</p> Official Document <p>Please see the official document for further details.</p> <p>https://playwright.dev/docs/intro</p>"},{"location":"azure/serviceprinciple/","title":"How to Create Service Principles in\u00a0Azure","text":"<p>To use service principles while running Azure commands in Azure DevOps pipelines, you will need to first create a service principle and grant it the necessary permissions in Azure. Then, you can use the service principle's credentials to authenticate and authorize the Azure commands that you want to run in your Azure DevOps pipelines.</p> <p></p>"},{"location":"azure/serviceprinciple/#here-is-an-example-of-how-you-can-create-a-service-principle-and-use-it-in-an-azure-devops-pipeline","title":"Here is an example of how you can create a service principle and use it in an Azure DevOps pipeline:","text":"<ul> <li> <p>Sign in to the Azure portal and navigate to the Azure Active Directory page.</p> </li> <li> <p>Click on \"App registrations\" and then click on the \"New registration\" button.</p> </li> <li> <p>Give your service principle a name and select \"Accounts in this organizational directory only\" as the supported account type. Click on the \"Register\" button to create the service principle.</p> </li> <li> <p>Click on the service principle that you just created, and then click on the \"Certificates &amp; secrets\" tab.</p> </li> <li> <p>Click on the \"New client secret\" button, give the secret a description, and select an expiration time. Click on the \"Add\" button to create the secret.</p> </li> <li> <p>Copy the secret value, as you will need it later or Store it in a Key-Vault</p> </li> <li> <p>Navigate to the resource or resources that you want to grant the service principle access to.</p> </li> <li> <p>Click on the \"Access control (IAM)\" tab, and then click on the \"Add role assignment\" button.</p> </li> <li> <p>Select the role that you want to assign to the service principle, and then type in the name of the service principle in the \"Select\" field. Click on the \"Save\" button to assign the role to the service principle.</p> </li> <li> <p>To use the service principle in your Azure DevOps pipeline, you will need to pass the service principle's client ID and client secret as environment variables. Here is an example of how you can do this:</p> </li> <li> <p>In your Azure DevOps project, navigate to the \"Pipelines\" page and click on the pipeline that you want to edit.</p> </li> <li> <p>Click on the \"Variables\" tab, and then click on the \"Add\" button.</p> </li> <li> <p>Add two new variables with the names \"AZURE_CLIENT_ID\" and \"AZURE_CLIENT_SECRET\", and set the values to the service principle's client ID and client secret, respectively.</p> </li> <li> <p>In your pipeline tasks, use the service principle's client ID and client secret to authenticate and authorize the Azure commands that you want to run. For example, you can use the following command to authenticate using the service principle:</p> </li> </ul> <pre><code>az login\u200a-\u200aservice-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET\u200a-\u200atenant $TENANT_ID\n</code></pre> <p>Note</p> <ul> <li> <p>Replace TENANT_ID with the ID of your Azure Active Directory tenant.</p> </li> <li> <p>Don't forget to store AZURE_CLIENT_ID &amp; AZURE_CLIENT_SECRET in Key-Vault for Future Use.</p> </li> </ul>"},{"location":"azure/techchallenges/","title":"Azure DevOps Technical Challanges","text":"<p>Here are some common technical challenges that can arise when using Azure DevOps:</p> ChallengesTips to Overcome <p>Integration challenges: Integrating Azure DevOps with other tools in your tech stack can be challenging. This includes integrating with other Azure services, as well as non-Azure tools like GitHub or Jira.</p> <p>Pipeline complexity: Azure DevOps provides powerful tools for building and deploying pipelines, but creating complex pipelines can be challenging. For example, setting up continuous delivery with multiple environments can require configuring many moving parts.</p> <p>Security and compliance: Ensuring security and compliance across your pipelines and repositories can be challenging. Azure DevOps provides security features like RBAC, but implementing and enforcing security policies can require careful planning and configuration.</p> <p>Release management: Managing the release of software to multiple environments can be challenging. This includes configuring release gates and approvals, rolling back releases, and tracking deployments across environments.</p> <p>Performance and scalability: As your pipeline and repository size grows, you may encounter performance and scalability issues. This can include slow build times, long queue times, and issues with concurrent builds.</p> <p>Version control: Using Git for version control in Azure DevOps can be challenging for teams not familiar with Git. It requires a different workflow than centralized version control systems, and can be difficult to manage if multiple teams are working on the same codebase.</p> <p>Continuous testing: Setting up and maintaining continuous testing with Azure DevOps can be challenging. This includes configuring test plans, integrating with test automation tools, and ensuring that tests run reliably and accurately. </p> <p>Integration challenges: To overcome integration challenges, it's important to understand the capabilities of Azure DevOps and the tools you want to integrate with. Azure DevOps provides a range of APIs and extensions that can help with integration. It's also important to plan integration in advance and test it thoroughly before deployment.</p> <p>Pipeline complexity: To simplify complex pipelines, it's important to break them down into smaller, more manageable pieces. Use templates and variables to reduce duplication and improve consistency. It's also important to test pipelines regularly and optimize for speed and reliability.</p> <p>Security and compliance: To ensure security and compliance, it's important to understand the security features of Azure DevOps and how to configure them properly. You should also establish clear security policies and ensure that all users are aware of them. Regular audits and monitoring can also help ensure compliance.</p> <p>Release management: To manage releases effectively, it's important to establish clear release policies and automate as much of the release process as possible. Use release gates and approvals to control the release process and ensure that releases are tested thoroughly before deployment. Monitor releases closely and be prepared to roll back if necessary.</p> <p>Performance and scalability: To improve performance and scalability, it's important to optimize pipelines for speed and reduce queue times. This can involve using parallelism, caching, and distributed builds. It's also important to monitor performance regularly and scale resources as necessary.</p> <p>Version control: To overcome version control challenges, it's important to establish clear version control policies and train all users in the use of Git. Use branching and merging effectively to manage code changes, and ensure that code reviews are conducted regularly to maintain code quality.</p> <p>Continuous testing: To set up and maintain continuous testing, it's important to establish clear testing policies and automate as much of the testing process as possible. Use test plans and test suites to organize and manage tests, and integrate with test automation tools for better efficiency. Monitor test results closely and be prepared to adjust your testing strategy as necessary. </p>"},{"location":"azure/techchallenges/#skipping-ci-for-git-push","title":"Skipping CI for Git Push","text":"<p>Moreover, you may instruct Azure Pipelines to forego starting a pipeline that a push would typically start. To prevent Azure Pipelines from performing CI for this push, just put [skip ci] in the message or description of any of the commits that are a part of a push. Any of the following modifications are also acceptable.</p> <ul> <li> <p>[skip ci] or [ci skip] </p> </li> <li> <p><code>***NO_CI***</code></p> </li> </ul> <p>Example</p> <p>git commit -m \" Commit Message ***No_CI*** \"</p>"},{"location":"azure/techchallenges/#syncs-changes-back-to-the-same-branch-in-azure-devops","title":"syncs changes back to the same branch in Azure DevOps:","text":"<p>The script provided below is a PowerShell script that is used in Azure DevOps to sync changes back to the same branch in the code repository. This script assumes that you have already cloned the repository in the pipeline and are working in the same branch.</p> Pre-Requsites <p>Enable OAuth Token in Agent: The agent running the script should have an OAuth token enabled to access the code repository. This can be done by following the steps in the Azure DevOps documentation.</p> <p>Now let's take a closer look at the script and what each line does:</p> <p>git pull origin $(Build.SourceBranch):  This command pulls the latest changes from the source branch into the local repository.</p> <p>git checkout $(Build.SourceBranch):  This command switches to the source branch.</p> <p>git commit -m \"Syncing updates with the code repository NO_CI\":  This  command commits the changes made to the repository and adds a commit message. The NO_CI flag in the commit message disables the continuous integration (CI) trigger for subsequent commits. This means that if there are further changes made to the code repository, the pipeline won't trigger a build, preventing the pipeline from running in a loop.</p> <p>git push origin HEAD:$(Build.SourceBranch):  This command pushes the changes to the same branch in the remote repository.</p> <p>The next block of code is an if statement that checks if the previous command executed successfully:</p> <p><pre><code>if ($?) {Write-Host \"Syncing Changes Back to Repo\"}\nelse {Write-Host \"Pushing Changes Anyway\"\ngit push -f origin HEAD:$(Build.SourceBranch)}\n</code></pre> If the previous command was successful, it displays \"Syncing Changes Back to Repo\". If the command failed, it displays \"Pushing Changes Anyway\" and then forces a push to the same branch in the remote repository.</p> <p>Overall, this script automates the process of syncing changes back to the same branch in a code repository using Azure DevOps.</p> PowerShellShell <pre><code>git pull origin $(Build.SourceBranch)\ngit checkout $(Build.SourceBranch)\ngit merge %sourceBranch% -m \"Merge to $(Build.SourceBranch)\"\ngit config --global user.email \"$(Build.RequestedForEmail)\"\ngit config --global user.name \"$(Build.RequestedFor)\"\ngit add .\ngit status\ngit commit -m \"Syncing updates with the code repository ***NO_CI***\"\ngit push origin HEAD:$(Build.SourceBranch)\n\nif ($?) {\n    Write-Host \"Syncing Changes Back to Repo\"\n} \nelse {\n    Write-Host \"Pushing Changes Anyway\"\ngit push -f origin HEAD:$(Build.SourceBranch)\n}\n</code></pre> <pre><code>git pull origin $(Build.SourceBranch)\ngit checkout $(Build.SourceBranch)\ngit merge %sourceBranch% -m \"Merge to $(Build.SourceBranch)\"\ngit config --global user.email \"$(Build.RequestedForEmail)\"\ngit config --global user.name \"$(Build.RequestedFor)\"\ngit add .\ngit status\ngit commit -m \"Syncing updates with the code repository ***NO_CI***\"\ngit push origin HEAD:$(Build.SourceBranch)\n\nif [ $? -eq 0 ]\nthen\n  echo \"Syncing Changes Back to Repo\"\nelse\n  echo \"Pulling before pushing Changes Back to Repo\"\n  git pull origin $BUILD_SOURCEBRANCH\n  git push origin HEAD:$BUILD_SOURCEBRANCH\nfi\n</code></pre>"},{"location":"azure/techchallenges/#pre-merge-validation-from-variable-source-branches-to-fixed-target-branch","title":"Pre-Merge Validation from Variable Source Branches to Fixed Target Branch","text":"<p>When there is a pull request from a variable feature branch, such as <code>Feature-A</code>, <code>Feature-B</code>, or <code>Feature-C</code> to the <code>Development</code> branch, we must determine whether the code in the feature branch is buildable or not. When the feature branch is good, we must only merge it with the development branch. However, in Azure Classic pipelines, we are unable to automatically change the source branch name. The script below will get the variable feature branch name whenever there is a PR and\u00a0 it will run the Build Validation pipeline by changing the branch name.</p> Pre-Merge Validation from Feature-Dev Branch <pre><code>echo \"Login to Azure \"\n\naz login -u $(username) -p $(password)\n\n\necho \"Fetching Current Feature Branch \"\n\n$PR = az repos pr list --repository $(Repo) --target-branch $(targetbranch) --organization $(URL) --project $(project) | grep sourceRefName | awk '{print $3}'\n$Branchname = $PR.Split(\"/\")[2] | sed 's/\"\"/\\ /g' | sed 's/,/\\ /g' | sed -e 's/^[ \\t]*//' | Foreach {$_.TrimEnd()}\n\n\necho \"The Current PR Received from Branch: $Branchname \"\ngit switch $Branchname\n\necho \"Commands to Execute\"\n\nnpm i #Example Commands\nnpm run build #Example Commands\n</code></pre>"},{"location":"devops/ansible/","title":"Ansible \ud83e\udd16","text":"<p>Ansible is an open-source IT Configuration Management, Deployment &amp; Orchestration tool. It aims to provide large productivity gains to a wide variety of automation challenges. This tool is very simple to use yet powerful enough to automate complex multi-tier IT application environments.</p>"},{"location":"devops/ansible/#installing-ansible","title":"Installing Ansible \ud83d\udce5","text":"Ubuntu \ud83d\udc27CentOS \ud83d\udc27 <p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install software-properties-common -y\n</code></pre> <pre><code>sudo add-apt-repository --yes --update ppa:ansible/ansible\n</code></pre> <pre><code>sudo apt install ansible -y\n</code></pre> <pre><code>ansible --version\n</code></pre></p> <p><pre><code>sudo yum install epel-release -y\n</code></pre> <pre><code>sudo yum install ansible -y\n</code></pre> <pre><code>ansible --version\n</code></pre></p>"},{"location":"devops/ansible/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like Ec2 Instances, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., Ec2 Full Access, S3 Full Access). Note down the user's access key ID and secret access key. Reference Document Link</p> </li> </ol> <p>Warning</p> <p>Utilize Role-Based Authentication when working with Terraform on AWS Instances or Services, as it offers a higher level of security compared to using access keys.</p>"},{"location":"devops/ansible/#scp-commands","title":"SCP Commands \ud83d\udee1\ufe0f","text":"<p>To Copy files From Localmachine to Remote machine <pre><code>scp -i &lt;private-key-file-path&gt; &lt;files to copy&gt; &lt;user&gt;@&lt;IP&gt;:&lt;Remote-machine-path&gt;\n</code></pre></p> <p>Example</p> <p>scp -i Downloads/control.pem sample.txt ubuntu@54.165.128.104:/home/ubuntu/</p> <p>To Copy Files from Remotemachine to Local machine <pre><code>scp -i &lt;private-key-file-path&gt; &lt;user&gt;@&lt;IP&gt;:&lt;remote-files-path&gt; &lt;Localmachine-path&gt;\n</code></pre></p> <p>Example</p> <p>scp -i Downloads/control.pem ubuntu@54.165.128.104:/home/ubuntu/sample.txt Desktop/</p>"},{"location":"devops/ansible/#sample-inventory-file","title":"Sample Inventory File \ud83d\udccb","text":"Inventory inventory<pre><code>##Host Level\n\nwebserver01    ansible_host=&lt;Private IP&gt;\nwebserver02    ansible_host=&lt;Private IP&gt;\nwebserver03    ansible_host=&lt;Private IP&gt;      \ndbserver01     ansible_host=&lt;Private IP&gt;\ndbserver02     ansible_host=&lt;Private IP&gt;      ansible_user=ubuntu\n\n##Group Level \n\n[Group1]\nwebserverserver01\nwebserverserver02\nwebserverserver03\n\n[Group2]\ndbserver01\ndbserver02\n\n##Parent Level\n\n[dc_mumbai:children] \nwebservergrp\ndbsrvgrp\n\n##Variables\n\n[dc_mumbai:vars]\nansible_user=&lt;user&gt;\nansible_ssh_private_key_file=&lt;key-path&gt;\n</code></pre> <p>Info</p> <p>Host level has the highest priority, If you mention anything like username or Keyfile etc. It will take only, which are mentioned at the host level.</p>"},{"location":"devops/ansible/#ansible-commands","title":"Ansible Commands \ud83d\udee0\ufe0f","text":"<p>To test the connection of a particular Remote Machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping &lt;hostname&gt;\n</code></pre> To test the connection of a particular Group of Remote Machines <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping &lt;Groupname&gt;\n</code></pre> To test the connection of All Remote Machine <pre><code>ansible -i &lt;Inventoryfile path&gt; -m ping all\n</code></pre> To see details about the machine  <pre><code>ansible -i &lt;Inventoryfile path&gt; -m setup &lt;hostname&gt;\n</code></pre></p>"},{"location":"devops/ansible/#some-example-ad-hoc-commands","title":"Some Example Ad hoc Commands \ud83d\udce1","text":"Ad hoc Commands <p>Copy files to Remote machines name start with web Command Line<pre><code>ansible -i &lt;Inventoryfile path&gt; -m copy -a \"src=index.html dest=/var/www/html/index.html\" 'web*' --become\n</code></pre> Installing httpd in centos Remote machine  Command Line<pre><code>ansible -i &lt;Inventoryfile path&gt; -m yum -a \"name=httpd state=present\" websrvgrp --become\n</code></pre> Start &amp; Enable httpd in centos Remote machine Command Line<pre><code>ansible -i &lt;Inventoryfile path&gt; -m service -a \"name=httpd state=started enabled=yes\" websrvgrp --become\n</code></pre></p> <p>Info</p> <p>Ansible Playbooks should be with .yml or .yaml Extension for example vim sample.yml</p>"},{"location":"devops/ansible/#playbook-for-creating-files-directories","title":"Playbook For Creating Files &amp; Directories \ud83d\udcda","text":"Sample Ansible Playbooks <p>1st_playbook.yaml<pre><code>- name: Creating Files &amp; Directories\n  hosts: &lt;host&gt;\n  become: yes\n  tasks:\n     - name: Creating a Directory\n         file:\n           path: /tmp/welcome\n           state: directory\n\n     - name: Creating a File\n         file:\n           path: /tmp/sample.txt\n           state: touch\n</code></pre> To Execute the playbook <pre><code>ansible-playbook -i &lt;Inventory file path&gt; sample.yml\n</code></pre></p>"},{"location":"devops/ansible/#writing-playbook-for-installing-httpd-service-in-remote-machines-with-start-and-enable-and-copying-indexhtml-files-from-local-machine-to-the-remote-machine","title":"Writing Playbook For Installing Httpd service in remote machines with start and enable and copying index.html files from local machine to the remote machine \ud83c\udf10","text":"installation_service.yaml<pre><code>- name: Install httpd and start the service\n  hosts: all\n  tasks:\n     - name: Installing the Apache package\n       yum:\n         name: httpd\n         state: present\n\n\n     - name: Starting service\n       service:\n         name: httpd\n         state: started\n         enabled: yes\n\n     - name: Copy file with owner and permissions\n       copy:\n         src: ./index.html\n         dest: /var/www/html/index.html\n\n     - name: Restarting service\n       service:\n         name: httpd\n         state: restarted\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine","title":"Writing Playbook for Setting Up Website in Remote Machine \ud83c\udf10","text":"website.yaml<pre><code>- name: Setting up Website\n  hosts: websrv\n  gather_facts: False\n  become: True\n\n  tasks:\n    - name: Installing Packages in CentOS\n      yum:\n        name: \"{{item}}\"\n        state: present\n      when: ansible_distribution == \"CentOS\"\n      loop:\n        - httpd\n        - wget\n        - unzip\n\n    - name: Start &amp; Enable httpd\n      service:\n        name: httpd\n        state: started\n        enabled: yes   \n\n    - name: Downloading Source code\n      get_url:\n        url: https://www.tooplate.com/zip-templates/2114_pixie.zip\n        dest: /opt\n\n    - name: Unarchive a file that is already on the remote machine\n      unarchive:\n        src: /opt/2114_pixie.zip\n        dest: /opt\n        remote_src: yes\n\n    - name : Deploy Website\n      copy:\n        src: /opt/2114_pixie/\n        dest: /var/www/html/\n        remote_src: yes\n\n\n    - name: Restarting httpd service\n      service:\n        name: httpd\n        state: restarted\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-for-setting-up-website-in-remote-machine-with-conditions-handlers","title":"Writing Playbook for Setting Up Website in Remote Machine with Conditions &amp; Handlers.\ud83c\udf10","text":"loops_conditions_handlers.yaml<pre><code>- name: Writing playbook for loops and conditions\n  hosts: all\n  tasks:\n    - name: Install packages on centos\n      yum:\n        name: \"{{item}}\"\n        state: present\n      when: ansible_distribution == \"CentOS\"\n      loop:\n        - httpd\n        - wget\n        - unzip\n        - zip\n        - git           \n\n    - name: Install packages on Ubuntu \n      apt:\n        name: \"{{item}}\"\n        state: present\n        update_cache: yes\n      when: ansible_distribution == \"Ubuntu\"\n      loop:\n        - apache2\n        - wget\n        - unzip\n        - zip\n        - git\n\n\n    - name: Start &amp; enable service on CentOS\n      service:\n       name: httpd\n       state: started\n       enabled: yes\n      when: ansible_distribution == \"CentOS\"\n\n    - name: Start &amp; enable service on Ubuntu\n      service:\n       name: apache2\n       state: started\n       enabled: yes\n      when: ansible_distribution == \"Ubuntu\"\n\n\n    - name: Push index.html on centos\n      copy:\n        src: index.html\n        dest: /var/www/html/\n        backup: yes\n      when: ansible_distribution == \"CentOS\"\n      notify:\n        - Restart service on CentOS\n\n    - name: Push index.html on ubuntu\n      copy:\n        src: index.html\n        dest: /var/www/html/\n        backup: yes\n      when: ansible_distribution == \"Ubuntu\"\n      notify:\n        - Restart service on Ubuntu\n\n  handlers:\n    - name: Restart service on CentOS\n      service:\n        name: httpd\n        state: restarted\n        enabled: yes\n      when: ansible_distribution == \"CentOS\"\n\n    - name: Restart service on Ubuntu\n      service:\n        name: apache2\n        state: restarted\n        enabled: yes\n      when: ansible_distribution == \"Ubuntu\"\n</code></pre>"},{"location":"devops/ansible/#writing-playbook-to-create-vpc-in-aws-cloud-and-including-variables-from-different-file","title":"Writing Playbook to Create VPC in AWS Cloud and Including Variables from different File \u2601\ufe0f","text":"Requirements <ul> <li> <p>python &gt;= 3.6</p> </li> <li> <p>boto3 &gt;= 1.15.0</p> </li> <li> <p>botocore &gt;= 1.18.0</p> </li> </ul> <p><pre><code>apt install python3-pip\n</code></pre> <pre><code>pip install boto\n</code></pre> <pre><code>pip install boto3\n</code></pre> <pre><code>pip install botocore\n</code></pre> Creating File to store Variables <pre><code>vim vpc_setup.txt\n</code></pre> <pre><code>vpc_name: \"Vprofile-vpc\"\n\n#Vpc-range\nvpcrange: '172.21.0.0/16'\n\n#subnet range\npubip1: '172.21.1.0/24'\npubip2: '172.21.2.0/24'\npubip3: '172.21.3.0/24'\npvtip1: '172.21.4.0/24'\npvtip2: '172.21.5.0/24'\npvtip3: '172.21.6.0/24'\n\n#region\nregion: 'us-east-2'\n\n#zone names\nzone1: us-east-2a\nzone2: us-east-2b\nzone3: us-east-2c\n\n\nstate: present\n</code></pre></p> Playbook \ud83d\udcda aws_vpc.yaml<pre><code>- hosts: localhost\n  connection: local\n  gather_facts: False\n  tasks:\n    - name: Import vpc variables\n      include_vars: /path/vpc_setup\n\n    - name: create vpc\n      ec2_vpc_net:\n          name: \"{{vpc_name}}\"\n          cidr_block: \"{{vpcrange}}\"\n          region: \"{{region}}\"\n          dns_support: yes\n          dns_hostnames: yes\n          tenancy: default\n          state: \"{{state}}\"\n      register: vpcout\n\n\n    - name: Create a public subnet for zone1\n      ec2_vpc_subnet:\n          vpc_id: \"{{vpcout.vpc.id}}\"\n          region: \"{{region}}\"\n          az: \"{{zone1}}\"\n          state: \"{{state}}\"\n          cidr: \"{{pubip1}}\"\n          map_public: yes\n          tags:\n            Name: vprofile_pubsub1\n      register: pubsub1_out\n\n    - name: Create a public subnet for zone2\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone2}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pubip2}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pubsub2\n      register: pubsub2_out\n\n    - name: Create a public subnet for zone3\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone3}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pubip3}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pubsub3\n      register: pubsub3_out\n\n    - name: Create a private subnet for zone1\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone1}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pvtip1}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pvtsub1\n      register: pvtsub1_out\n\n    - name: Create a private subnet for zone2\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone2}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pvtip2}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pvtsub2\n      register: pvtsub2_out\n\n    - name: Create a private subnet for zone3\n      ec2_vpc_subnet:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         az: \"{{zone3}}\"\n         state: \"{{state}}\"\n         cidr: \"{{pvtip3}}\"\n         map_public: yes\n         tags:\n           Name: vprofile_pvtsub3\n      register: pvtsub3_out\n\n    - name: Internet gateway setup\n      ec2_vpc_igw:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         state: \"{{state}}\"\n         tags:\n           Name: vprofile_IGW\n      register: igw_out\n\n\n\n    - name: public subnet route table\n      ec2_vpc_route_table:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         tags:\n           Name: vprofile_Public\n         subnets:\n           - \"{{pubsub1_out.subnet.id}}\"\n           - \"{{pubsub2_out.subnet.id}}\"\n           - \"{{pubsub3_out.subnet.id}}\"\n         routes:\n           - dest: 0.0.0.0/0\n             gateway_id: \"{{ igw_out.gateway_id }}\"\n      register: public_route_table\n\n\n    - name: Create a new nat gateway and allocate a new EIP if a nat gateway does not yet exist in the subnet.\n      ec2_vpc_nat_gateway:\n         state: \"{{state}}\"\n         subnet_id: \"{{pubsub1_out.subnet.id}}\"\n         wait: true\n         region: \"{{region}}\"\n         if_exist_do_not_create: true\n      register: nat_out\n\n    - name: private subnet route table\n      ec2_vpc_route_table:\n         vpc_id: \"{{vpcout.vpc.id}}\"\n         region: \"{{region}}\"\n         tags:\n           Name: vprofile_Private\n         subnets:\n           - \"{{pvtsub1_out.subnet.id}}\"\n           - \"{{pvtsub2_out.subnet.id}}\"\n           - \"{{pvtsub3_out.subnet.id}}\"\n         routes:\n           - dest: 0.0.0.0/0\n             gateway_id: \"{{nat_out.nat_gateway_id}}\"\n      register: private_route_table\n\n\n    - debug:\n        var: \"{{item}}\"\n      loop:\n         - vpcout.vpc.id\n         - pubsub1_out.subnet.id\n         - pubsub2_out.subnet.id\n         - pubsub3_out.subnet.id\n         - pvtsub1_out.subnet.id\n         - pvtsub2_out.subnet.id\n         - pvtsub3_out.subnet.id\n         - igw_out.gateway_id\n         - public_route_table.route_table.id\n         - nat_out.nat_gateway_id\n         - private_route_table.route_table.id\n\n    - set_fact:\n        vpcid: \"{{vpcout.vpc.id}}\"\n        pubsublid: \"{{ pubsub1_out.subnet.id }}\"\n        pubsub2id: \"{{ pubsub2_out.subnet.id }}\"\n        pubsub3id: \"{{ pubsub3_out.subnet.id }}\"\n        privsublid: \"{{ pvtsub1_out.subnet.id }}\"\n        privsub2id: \"{{ pvtsub2_out.subnet.id }}\"\n        privsub3id: \"{{ pvtsub3_out.subnet.id }}\"\n        igwid: \"{{ igw_out.gateway_id }}\"\n        pubRTid: \"{{ public_route_table.route_table.id }}\"\n        NATGWid: \"{{ nat_out.nat_gateway_id }}\"\n        privRTid: \"{{ private_route_table.route_table.id }}\"\n        cacheable: yes\n\n    - name: creating file for vpc output\n      copy:\n        content: \"vpcid: {{vpcout.vpc.id}}\\n pubsublid: {{ pubsub1_out.subnet.id }}\\npubsub2id: {{ pubsub2_out.subnet.id }}\\npubsub3id: {{ pubsub3_out.subnet.id }}\\nprivsublid: {{ pvtsub1_out.subnet.id }}\\nprivsub2id: {{ pvtsub2_out.subnet.id }}\\nprivsub3id: {{ pvtsub3_out.subnet.id }}\\nigwid: {{ igw_out.gateway_id }}\\npubRTid: {{ public_route_table.route_table.id }}\\nNATGWid: {{ nat_out.nat_gateway_id }}\\nprivRTid: {{ private_route_table.route_table.id }}\"\n        dest: /home/ubuntu/Vprofile/vars/output_vars\n</code></pre>"},{"location":"devops/ansible/#launching-ec2-instance-in-aws-cloud","title":"Launching Ec2 Instance in AWS Cloud \ud83d\ude80","text":"Requirements <ul> <li> <p>python &gt;= 3.6</p> </li> <li> <p>boto3 &gt;= 1.15.0</p> </li> <li> <p>botocore &gt;= 1.18.0</p> </li> </ul> <p><pre><code>apt install python3-pip\n</code></pre> <pre><code>pip install boto\n</code></pre> <pre><code>pip install boto3\n</code></pre> <pre><code>pip install botocore\n</code></pre></p> Launching Instance Playbook aws_ec2.yaml<pre><code>- name: Launching Ec2 Instance\n  hosts: localhost\n  connection: local\n  tasks:\n     - name: Creating Key pair\n       amazon.aws.ec2_key:\n         name: samplekey\n         region: us-west-1\n       register: key\n\n     - debug:\n        var: key\n\n     - name: Storing private key into a file\n       copy:\n          content: \"{{key.key.private_key}}\"\n          dest: \"./sample.pem\"\n          mode: 0600\n       when: key.changed\n\n\n     - name: Creating Security Group\n       amazon.aws.ec2_group:\n         name: mysg\n         description: Allowing 22 and 80\n         vpc_id: vpc-0c8e70cf05b1342ac\n         region: us-west-1\n         rules:\n           - proto: tcp\n             from_port: 22\n             to_port: 22\n             cidr_ip: 0.0.0.0/0\n             rule_desc: allow all on port 80 &amp; 22\n       register: sg_out\n\n     - name: Launching bastion_host\n       ec2:\n          key_name: \"samplekey\"\n          region: us-west-1\n          instance_type: t2.micro\n          image: ami-0573b70afecda915d\n          wait: yes\n          wait_timeout: 300\n          instance_tags:\n            name: \"Ansible Instance\"\n            project: vprofile\n            owner: devops team\n          exact_count: 1\n          count_tag:\n            name: \"Ansible Instance\"\n            project: vprofile\n            owner: devops team\n          group_id: \"{{sg_out.group_id}}\"\n          vpc_subnet_id: subnet-0c4734c845e549cda\n       register: instance\n</code></pre>"},{"location":"devops/ansible/#ansible-configuration-file","title":"Ansible Configuration File \u2699\ufe0f","text":"<p>Note</p> <p>You should save the configuration file with name ansible.cfg</p> <pre><code>vim ansible.cfg\n</code></pre> Ansible Configuration File ansible.cfg<pre><code>[defaults]\nhost_key_checking=False\ninventory=&lt;Inventory File Path&gt;\ntimeout=20\nlog_path=/var/log/ansible_world.log\nremote_port=22\nremote_user=&lt;username&gt;\n\n[privilege_escalation]\nbecome=True\nbecome_method=sudo\nbecome_user=root\nbecome_ask_pass=False\n</code></pre>"},{"location":"devops/aws-cli/","title":"AWS","text":"<p>Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally.</p>"},{"location":"devops/aws-cli/#aws-cli-command-line-interface","title":"AWS CLI (Command Line Interface)","text":""},{"location":"devops/aws-cli/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like Ec2 Instances, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., Ec2 Full Access, S3 Full Access). Note down the user's access key ID and secret access key. Reference Document Link</p> </li> </ol> <p>Warning</p> <p>Utilize Role-Based Authentication when working with Terraform on AWS Instances or Services, as it offers a higher level of security compared to using access keys.</p>"},{"location":"devops/aws-cli/#installing-aws-cli","title":"Installing AWS-CLI","text":"<pre><code>sudo -i\n</code></pre> <p><pre><code>sudo apt update\n</code></pre> <pre><code>apt install awscli -y\n</code></pre> </p>"},{"location":"devops/aws-cli/#configure-aws-cli-with-iam-user-credentials-with-a-specific-region","title":"Configure AWS CLI with IAM user Credentials with a specific Region","text":"<p><pre><code>aws configure\n</code></pre>  Once it is done try some aws cli commands like aws s3 ls If u have any buckets in your s3 it will list</p>"},{"location":"devops/aws-cli/#ec2-elastic-compute-cloud","title":"EC2 \u2013 Elastic Compute Cloud","text":""},{"location":"devops/aws-cli/#create-a-key-pair","title":"Create a key pair","text":"<p><pre><code>aws ec2 create-key-pair --key-name &lt;keypair-Name&gt; --query 'KeyMaterial' --output text &gt; &lt;keypair-Name.pem&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-a-key-pair","title":"Delete a key pair","text":"<p>To delete a key pair, run the aws ec2 delete-key-pair command, substituting MyKeyPair with the name of the pair to delete. <pre><code>aws ec2 delete-key-pair --key-name &lt;keypair-Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-security-group-adding-inbound-rules","title":"Create a Security Group &amp; Adding Inbound rules","text":"<p><pre><code>aws ec2 create-security-group --group-name &lt;security grp Name&gt; --description \"&lt;Description&gt;\"\n</code></pre> <pre><code>curl https://checkip.amazonaws.com\n</code></pre> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;security group Id&gt; --protocol tcp --port &lt;port Number&gt; --cidr &lt;ip address&gt;\n</code></pre> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;security grp Id&gt;--protocol tcp --port 22-8000 --cidr 0.0.0.0/0 \n</code></pre>  To view the initial information for my-sg, run the aws ec2 describe-security-groups command. For an EC2-Classic security group, you can reference it by its name. <pre><code>aws ec2 describe-security-groups --group-names &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-security-group","title":"Delete your security group","text":"<p>The following command example deletes the EC2-Classic security group named. <pre><code>aws ec2 delete-security-group --group-name &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#launch-instance","title":"Launch Instance","text":"<p>You can use the following command to launch a t2.micro instance in EC2-Classic. Replace the italicized parameter values with your own. You can get the AMI IDs from documentation or console for your required Instance. <pre><code> aws ec2 run-instances --image-id &lt;ami-Id&gt; --count 1 --instance-type &lt;type&gt; --key-name &lt;keypair-Name&gt; --security-groups &lt;security grp Name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#add-a-tag-to-your-instance","title":"Add a tag to your Instance","text":"<p><pre><code>aws ec2 create-tags --resources &lt;Instance-Id&gt;--tags Key=Name,Value=&lt;value&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#terminate-your-instance","title":"Terminate your Instance","text":"<p>To delete an instance, you use the command aws ec2 terminate-instances to delete it. <pre><code>aws ec2 terminate-instances --instance-ids &lt;Instance-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-launch-template","title":"Create Launch Template","text":"<p><pre><code>aws ec2 create-launch-template --launch-template-name &lt;Name&gt;\":[{\"AssociatePublicIpAddress\":true,\"DeviceIndex\":0,\"Ipv6AddressCount\":1,\"SubnetId\":\"pe\":\"&lt;Instance type\",\"TagSpecifications\":[{\"ResourceType\":\"instance\",\" Tags\":[{\"Key\":\"Name\",\"Value\":\"&lt;value&gt;\"}]}]}'\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-launch-template","title":"Delete Launch Template","text":"<p><pre><code>aws ec2 delete-launch-template --launch-template-id &lt; template id&gt;  --region &lt;region&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#creating-auto-scaling-group","title":"Creating Auto-Scaling group","text":"<p><pre><code>aws autoscaling create-auto-scaling-group --auto-scaling-group-name &lt;Name&gt;  --launch-LaunchTemplateId=&lt;template \u2013 id &gt; --min-size 2 --max-size 5 --vpc-zone-identifier \"subnet1-id,subnet2-id,subnet3-id\"\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-auto-scaling-group","title":"Delete your Auto-Scaling Group","text":"<pre><code>aws autoscaling delete-auto-scaling-group --auto-scaling-group-name &lt; Auto -Scaling group Name &gt;\n</code></pre>"},{"location":"devops/aws-cli/#ebs-elastic-block-storage","title":"EBS \u2013 Elastic Block Storage","text":""},{"location":"devops/aws-cli/#create-ebs-volume","title":"Create EBS Volume","text":"<p>To create an empty General Purpose SSD (gp2) volume <pre><code>aws ec2 create-volume --volume-type &lt;volume type&gt; --size &lt;size in number&gt; --availability-zone &lt;zone&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-an-encrypted-volume","title":"To create an encrypted volume","text":"<p><pre><code>aws ec2 create-volume --volume-type &lt;volume type&gt; --size &lt;size in number&gt;  --encrypted --availability-zone &lt;zone&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-a-volume-with-tags","title":"To create a volume with tags","text":"<p><pre><code>aws ec2 create-tags --resources &lt;volume-id&gt; --tags Key=Name,Value=&lt;value&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-delete-a-volume","title":"To Delete a Volume","text":"<pre><code>aws ec2 delete-volume --volume-id &lt;volume Id&gt;\n</code></pre> <p>Output</p> <p>Output: None</p>"},{"location":"devops/aws-cli/#to-create-a-snapshot","title":"To create a snapshot","text":"<p>This example command creates a snapshot of the volume with a volume ID of  and a short description to identify the snapshot. <pre><code>aws ec2 create-snapshot --volume-id &lt;volume Id&gt; --description \"&lt;Description&gt;\"\n</code></pre>"},{"location":"devops/aws-cli/#to-create-a-snapshot-with-tags","title":"To create a snapshot with tags","text":"<p><pre><code>aws ec2 create-snapshot --volume-id &lt;volume Id&gt; --description 'Prod backup' --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=&lt;value&gt;},{Key=Database,Value=Mysql}]'\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-allocate-an-elastic-ip-address-for-ec2-classic","title":"To allocate an Elastic IP address for EC2-Classic","text":"<p>The following allocate-address example allocates an Elastic IP address to use with an instance in EC2-Classic. <pre><code>aws ec2 allocate-address\n</code></pre> </p>"},{"location":"devops/aws-cli/#elb-elastic-load-balancer","title":"ELB \u2013 Elastic Load Balancer","text":""},{"location":"devops/aws-cli/#create-load-balancer","title":"Create-load-balancer","text":""},{"location":"devops/aws-cli/#to-create-an-application-load-balancer","title":"To create an Application load balancer","text":"<p>The below commands to find subnet id &amp; Instance Id  <pre><code>aws ec2 describe-subnets\n</code></pre> <pre><code>aws ec2 describe-instances\n</code></pre> <pre><code>aws elbv2 create-load-balancer --name &lt;Load balancer Name&gt;--type &lt;type&gt; --subnets &lt;subnet-Id&gt; &lt;subnet-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-create-a-network-load-balancer","title":"To create a Network load balancer","text":"<p><pre><code>aws elbv2 create-load-balancer --name &lt;Load balancer Name&gt;--type &lt;type&gt; --subnets &lt;subnet-Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-register-instances-with-a-load-balancer","title":"To register instances with a load balancer","text":"<pre><code>aws elb register-instances-with-load-balancer --load-balancer-name &lt;Load balancer Name&gt; --instances &lt;Instance-Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#to-delete-a-specific-load-balancer","title":"To Delete a Specific Load balancer","text":"<p><pre><code>aws elbv2 delete-load-balancer --load-balancer-arn &lt;arn end point&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#rds-relational-database-service","title":"RDS - Relational Database Service","text":""},{"location":"devops/aws-cli/#create-db-instance","title":"Create-db-Instance","text":"<p><pre><code> aws rds create-db-instance --db-instance-identifier &lt;db - Name&gt; --db-instance-class &lt;db.type&gt; --engine &lt;Database Engine&gt;  --master-username &lt;username&gt; --master-user-password &lt;password&gt; --allocated-storage &lt;storage in numbers&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#to-delete-your-db-instance","title":"To delete your db-Instance","text":"<p><pre><code>aws rds delete-db-instance --db-instance-identifier &lt;db - Name&gt; --final-db-snapshot-identifier &lt;db - Name&gt;-final-snap\n</code></pre> </p>"},{"location":"devops/aws-cli/#s3-simple-storage-service","title":"S3 \u2013 Simple Storage Service","text":""},{"location":"devops/aws-cli/#list-buckets-objects","title":"List Buckets &amp; Objects","text":"<p>To list your buckets, folders, or objects, use the s3 ls command. Using the command without a target or options lists all buckets. <pre><code>aws s3 ls\n</code></pre> <pre><code>aws s3 ls s3://&lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-bucket","title":"Create a bucket","text":"<p>Use the s3 mb command to make a bucket. Bucket names must be globally unique (unique across all of Amazon S3) and should be DNS compliant. <pre><code>aws s3 mb s3:// &lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#copy-objects","title":"Copy objects","text":"<p>Use the s3 cp command to copy objects from a bucket or a local directory <pre><code>aws s3 cp &lt;file&gt; s3:// &lt;bucket name&gt;\n</code></pre> <pre><code>aws s3 cp s3://&lt;source bucket/file&gt; s3://&lt;destination-bucket&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#move-objects","title":"Move objects","text":"<p>Use the s3 mv command to move objects from a bucket or a local directory. <pre><code>aws s3 mv &lt;local file&gt; s3:// &lt;bucket name&gt; \n</code></pre> <pre><code>aws s3 mv s3:// &lt;source bucket/file&gt; s3://&lt;destination-bucket&gt;\n</code></pre></p>"},{"location":"devops/aws-cli/#sync-objects","title":"Sync Objects","text":"<p><pre><code>aws s3 sync . s3://&lt;bucket name&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-objects","title":"Delete Objects","text":"<p><pre><code>aws s3 rm s3://&lt;bucket name/file&gt; --recursive\n</code></pre> </p>"},{"location":"devops/aws-cli/#empty-bucket","title":"Empty Bucket","text":"<pre><code>aws s3 rm s3://&lt;bucket name&gt; --recursive\n</code></pre>"},{"location":"devops/aws-cli/#delete-bucket","title":"Delete Bucket","text":"<pre><code>aws s3 rb s3://&lt;bucket name&gt;\n</code></pre>"},{"location":"devops/aws-cli/#vpc-virtual-private-cloud","title":"VPC \u2013 Virtual Private Cloud","text":""},{"location":"devops/aws-cli/#to-create-a-vpc-and-subnets-using-the-aws-cli","title":"To create a VPC and subnets using the AWS CLI","text":""},{"location":"devops/aws-cli/#create-a-vpc-with-a-1000016-cidr-block-using-the-following-create-vpc-command","title":"Create a VPC with a 10.0.0.0/16 CIDR block using the following create-vpc command.","text":"<p><pre><code>aws ec2 create-vpc --cidr-block &lt;Ip address&gt; --query Vpc.VpcId --output text\n</code></pre> </p>"},{"location":"devops/aws-cli/#using-the-vpc-id-from-the-previous-step-create-a-subnet-with-a-1001024-cidr-block-using-the-following-create-subnet-command","title":"Using the VPC ID from the previous step, create a subnet with a 10.0.1.0/24 CIDR block using the following create-subnet command.","text":"<p><pre><code>aws ec2 create-subnet --vpc-id &lt;vpc - Id&gt;--cidr-block &lt;Ip address&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-second-subnet-in-your-vpc-with-a-1002024-cidr-block","title":"Create a second subnet in your VPC with a 10.0.2.0/24 CIDR block.","text":"<p><pre><code>aws ec2 create-subnet --vpc-id &lt;vpc - Id&gt;--cidr-block &lt;Ip address&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-an-internet-gateway-using-the-following-create-internet-gateway-command","title":"Create an internet gateway using the following create-internet-gateway command.","text":"<p><pre><code>aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text\n</code></pre> </p> <p></p>"},{"location":"devops/aws-cli/#using-the-id-from-the-previous-step-attach-the-internet-gateway-to-your-vpc-using-the-following-attach-internet-gateway-command","title":"Using the ID from the previous step, attach the internet gateway to your VPC using the following attach-internet-gateway command.","text":"<pre><code>aws ec2 attach-internet-gateway --vpc-id &lt;vpc - Id&gt;--internet-gateway-id &lt;IGW - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#create-a-custom-route-table-for-your-vpc-using-the-following-create-route-table-command","title":"Create a custom route table for your VPC using the following create-route-table command.","text":"<p><pre><code>aws ec2 create-route-table --vpc-id &lt;vpc - Id&gt;--query RouteTable.RouteTableId --output text\n</code></pre> </p>"},{"location":"devops/aws-cli/#create-a-route-in-the-route-table-that-points-all-traffic-00000-to-the-internet-gateway-using-the-following-create-route-command","title":"Create a route in the route table that points all traffic (0.0.0.0/0) to the internet gateway using the following create-route command.","text":"<p><pre><code>aws ec2 create-route --route-table-id &lt;route table - Id&gt;--destination-cidr-block 0.0.0.0/0 --gateway-id &lt;Igw - Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#you-can-describe-the-route-table-using-the-following-describe-route-tables-command","title":"You can describe the route table using the following describe-route-tables command.","text":"<p><pre><code>aws ec2 describe-route-tables --route-table-id &lt;route table - Id&gt;\n</code></pre> </p>"},{"location":"devops/aws-cli/#the-route-table-is-currently-not-associated-with-any-subnet-you-need-to-associate-it-with-a-subnet-in-your-vpc-so-that-traffic-from-that-subnet-is-routed-to-the-internet-gateway","title":"The route table is currently not associated with any subnet. You need to associate it with a subnet in your VPC so that traffic from that subnet is routed to the internet gateway.","text":"<p><pre><code>aws ec2 describe-subnets --filters \"Name=vpc-id,Values=&lt;vpc \u2013Id&gt;  --query \"Subnets[*].{ID:SubnetId,CIDR:CidrBlock}\"\n</code></pre> </p>"},{"location":"devops/aws-cli/#you-can-choose-which-subnet-to-associate-with-the-custom-route-table-for-example-subnet-0c312202b3f26703a-and-associate-it-using-the-associate-route-table-command-this-subnet-is-your-public-subnet","title":"You can choose which subnet to associate with the custom route table, for example, subnet-0c312202b3f26703a, and associate it using the associate-route-table command. This subnet is your public subnet.","text":"<pre><code>aws ec2 associate-route-table  --subnet-id &lt;subnet-Id&gt; --route-table-id &lt;route table - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#clean-up","title":"CLEAN UP","text":""},{"location":"devops/aws-cli/#delete-your-custom-route-table","title":"Delete your custom route table:","text":"<pre><code>aws ec2 delete-route-table --route-table-id &lt;route table - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-subnets","title":"Delete your subnets:","text":"<pre><code>aws ec2 delete-subnet --subnet-id &lt;subnet-Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#detach-your-internet-gateway-from-your-vpc","title":"Detach your internet gateway from your VPC:","text":"<pre><code>aws ec2 detach-internet-gateway --internet-gateway-id &lt;Igw -Id&gt; --vpc-id &lt;vpc- Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-internet-gateway","title":"Delete your internet gateway:","text":"<pre><code>aws ec2 delete-internet-gateway --internet-gateway-id &lt;Igw - Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#delete-your-vpc","title":"Delete your VPC:","text":"<pre><code>aws ec2 delete-vpc --vpc-id &lt;vpc- Id&gt;\n</code></pre>"},{"location":"devops/aws-cli/#cloud-watch","title":"Cloud Watch","text":""},{"location":"devops/aws-cli/#creating-alarm","title":"Creating Alarm","text":"<p><pre><code>aws cloudwatch put-metric-alarm --alarm-name &lt;Alarm name&gt; --alarm-description \"&lt;Description&gt;\" --metric-name &lt;Metric&gt; --namespace AWS/EC2 --statistic Average --period 300 --threshold &lt;70&gt; --comparison-operator &lt;GreaterThanThreshold&gt;  --dimensions \"Name=InstanceId,Value=&lt;Id&gt;\" --evaluation-periods 2 --alarm-actions &lt;SNS \u2013 arn &gt; --unit Percent \n</code></pre> </p>"},{"location":"devops/aws-cli/#delete-your-alarm","title":"Delete Your Alarm","text":"<pre><code>aws cloudwatch delete-alarms --alarm-names &lt;Alarm name&gt; \n</code></pre>"},{"location":"devops/aws-cli/#disable-your-alarm","title":"Disable your Alarm","text":"<pre><code>aws cloudwatch disable-alarm-actions --alarm-names &lt;Alarm name&gt;\n</code></pre>"},{"location":"devops/aws-cli/#enable-your-alarm","title":"Enable your Alarm","text":"<pre><code>aws cloudwatch enable-alarm-actions --alarm-names &lt;Alarm name&gt;\n</code></pre>"},{"location":"devops/awsdevops/","title":"DevOps","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increase an organization\u2019s ability to deliver applications and services at high velocity: Evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p>"},{"location":"devops/awsdevops/#learning-paths-with-practical-workshops","title":"Learning Paths with Practical Workshops","text":"<p>Decoding DevOps:This course is designed to provide a comprehensive understanding of the principles and practices of DevOps. It covers topics such as continuous integration and delivery, infrastructure as code, configuration management, and monitoring and logging. The course is suitable for software developers, system administrators, and IT professionals who want to learn about DevOps and how it can be implemented in their organizations. By the end of the course, you should have a good understanding of the DevOps culture, tools, and practices.</p> <p>Real Time DevOps Projects:This course is focused on providing hands-on experience with DevOps tools and practices through real-world projects. You'll work on projects such as setting up a CI/CD pipeline, automating infrastructure deployment, and monitoring and logging applications. The course is designed for learners who have some experience with DevOps and want to gain practical experience working on real-world projects. By the end of the course, you should have a portfolio of completed projects that you can showcase to potential employers. </p> <p></p>"},{"location":"devops/azure/","title":"Azure","text":"<p>Updating...</p>"},{"location":"devops/docker-setup/","title":"Docker \ud83d\udc33","text":"<p>Docker is a powerful containerization tool designed to simplify the process of creating, deploying, and running applications using containers. Containers package applications along with their dependencies, making it easy to deploy them consistently across different environments. Unlike traditional virtualization, containers share the host OS kernel, making them lightweight and efficient.</p>"},{"location":"devops/docker-setup/#installation-of-docker-engine-docker-compose-on-ubuntu-centos","title":"Installation of Docker Engine &amp; Docker Compose on Ubuntu &amp; CentOS \ud83d\udce5","text":"Docker Setup <p>To streamline the installation process, you can use the following script to install Docker Engine and Docker Compose on both Ubuntu and CentOS: <pre><code>vim docker_setup.sh\n</code></pre> <pre><code>#!/bin/bash\napt --help &gt;&gt;/dev/null\nif [ $? -eq 0 ]\n   then \necho \" INSTALLING DOCKER IN UBUNTU\"\necho\nsudo apt update\nsudo apt-get remove docker docker-engine docker.io containerd runc\nsudo apt-get update\nsudo apt-get -y install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \\\n  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io -y  \nsudo docker run hello-world\n    else\necho \" INSTALLING DOCKER IN CENTOS\"\necho\nsudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine\nsudo yum install -y yum-utils\nsudo yum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\nsudo yum install docker-ce docker-ce-cli containerd.io -y   \nsudo systemctl start docker\nsudo docker run hello-world\nfi\necho \" Installing Docker Compose\"\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre> Now, grant execute permission to the script: <pre><code>sudo chmod +x docker_setup.sh\n</code></pre> Finally, run the script to install Docker and Docker Compose: <pre><code>./docker_setup.sh\n</code></pre></p>"},{"location":"devops/docker-setup/#docker-commands","title":"Docker Commands \ud83d\ude80","text":"Docker Images \ud83d\uddbc\ufe0fContainers \ud83d\udce6Share \ud83d\udd04 <p>List All Images \ud83d\udccb</p> <p>To display all locally stored Docker images, use the following command:   <pre><code>docker image ls\n</code></pre></p> <p>Build an Image \ud83d\udee0\ufe0f</p> <p>To create an image from a Dockerfile in the current directory and assign a tag, utilize this command:  <pre><code>docker build -t &lt;imagename&gt;:&lt;tag&gt; \n</code></pre></p> <p>Delete an Image \ud83d\uddd1\ufe0f</p> <p>Remove an image from the local image store using the following command:  <pre><code>docker image rm &lt;imagename&gt;:&lt;tag&gt;\n</code></pre></p> <p>Run a container in an interactive mode:  <pre><code>docker run -it &lt;imagename&gt;:&lt;tag&gt;\n</code></pre></p> <p>Run a container from the Image nginx:latest, name it \"web,\" and expose port 5000 externally, mapped to port 80 inside the container in detached mode: \u25b6\ufe0f <pre><code>docker run --name web -d -p 5000:80 nginx:latest\n</code></pre></p> <p>Run a detached container in a previously created container network: \ud83c\udf10 <pre><code>docker network create &lt;mynetwork&gt;\n</code></pre> <pre><code>docker run --name web -d --net mynetwork -p 5000:80 nginx:latest\n</code></pre></p> <p>Follow the logs of a specific container: <pre><code>docker logs -f &lt;container name or container container-id&gt;\n</code></pre></p> <p>List only active containers \ud83d\udfe2 <pre><code>docker ps\n</code></pre></p> <p>List all containers \ud83d\udfe2\ud83d\udd34 <pre><code>docker ps -a\n</code></pre></p> <p>Stop a container \u23f9\ufe0f <pre><code>docker stop &lt;container name or container container-id&gt;\n</code></pre></p> <p>Stop a container (timeout = 1 second)  <pre><code>docker stop -t1\n</code></pre></p> <p>Remove a stopped container \ud83d\uddd1\ufe0f <pre><code>docker rm &lt;container name or container container-id&gt;\n</code></pre></p> <p>Force stop and remove a container <pre><code>docker rm -f &lt;container name or container container-id&gt;\n</code></pre></p> <p>Remove all containers  <pre><code>docker rm -f $(docker ps-aq)\n</code></pre></p> <p>Remove all stopped containers <pre><code>docker rm $(docker ps -q -f \u201cstatus=exited\u201d)\n</code></pre></p> <p>Execute a new process in an existing container: Execute and access bash inside a container <pre><code>docker exec -it &lt;container name or container-id&gt; bash\n</code></pre> To inspect the container <pre><code>docker inspect &lt;container name or container container-id&gt;\n</code></pre></p> <p>To Establish Connections from Local to Remote. log in with your Dockerhub Credentials. <pre><code>docker login\n</code></pre> Pull an image from a registry</p> <pre><code>docker pull &lt;imagename&gt;:&lt;tag&gt;\n</code></pre> <p>Retag a local image with a new image name and tag <pre><code>docker tag myimage:1.0 myrepo/myimage:2.0\n</code></pre></p> <p>Push an image to a registry. <pre><code>docker push myrepo/myimage:2.0\n</code></pre></p>"},{"location":"devops/docker-setup/#dockerfile","title":"Dockerfile \ud83d\udccb","text":"Sample DockerfileUsing VariablesUsing Existing zip/tar File <p>Sample Dockerfile for Deploying a Static website.</p> <p><pre><code>vim Dockerfile\n</code></pre> <pre><code># Use the CentOS 7 base image\nFROM centos:7\n\n# Set metadata labels\nLABEL \"Author\"=\"saiteja Irrinki\"\nLABEL \"Project\"=\"Wave\"\n\n# Install necessary packages - Apache, wget, and unzip\nRUN yum install httpd wget unzip -y\n\n# Download the website template\nRUN wget https://www.tooplate.com/zip-templates/2121_wave_cafe.zip\n\n# Unzip the downloaded template\nRUN unzip 2121_wave_cafe.zip\n\n# Copy the contents of the unzipped template to the web server directory\nRUN cp -r 2121_wave_cafe/* /var/www/html/\n\n# Start the Apache web server in the foreground\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]\n\n# Expose port 80 for web traffic\nEXPOSE 80\n\n# Set the working directory to the web server directory\nWORKDIR /var/www/html\n\n# Create a volume for Apache's log files\nVOLUME /var/log/httpd\n</code></pre></p> <p>Using Variables in Dockerfile for Deploying a Static website.</p> <p><pre><code>vim Dockerfile\n</code></pre> <pre><code># Define variables\nARG AUTHOR=\"saiteja Irrinki\"\nARG PROJECT=\"Highway\"\nARG DOWNLOAD_URL=\"https://templatemo.com/tm-zip-files-2020/templatemo_520_highway.zip\"\nARG FILE=\"templatemo_520_highway\"\n# Use the variables\nFROM centos:7\nLABEL \"Author\"=\"$AUTHOR\"\nLABEL \"Project\"=\"$PROJECT\"\nRUN yum install httpd wget unzip -y\n\n# Use --no-check-certificate flag to bypass SSL certificate checks\nRUN wget --no-check-certificate \"$DOWNLOAD_URL\"\n\nRUN unzip \"$FILE\".zip\nRUN cp -r \"$FILE\"/* /var/www/html/\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]\nEXPOSE 80\nWORKDIR /var/www/html\nVOLUME /var/log/httpd\n\n# Set the image name and optionally a tag\nLABEL \"name\"=\"$PROJECT\"\n</code></pre></p> <p><pre><code>vim Dockerfile\n</code></pre> Adding Existing zip/tar files in Dockerfile</p> <p>Docker expects the wave.tar.gz file to exist in the same directory where you are executing the docker build command. Docker will then copy the contents of that file into the /var/www/html/ directory within the Docker image being built. </p> <pre><code># Use the latest Ubuntu base image\nFROM ubuntu:latest\n\n# Set metadata labels for author and project\nLABEL \"Author\"=\"Saiteja Irrinki\"\nLABEL \"Project\"=\"Wave\"\n\n# Set environment variable to make the installation non-interactive\nENV DEBIAN_FRONTEND=noninteractive\n\n# Update package list and install Apache2 and Git\nRUN apt update &amp;&amp; apt install apache2 git -y\n\n# Add the contents of 'wave.tar.gz' to the Apache web server\n# You can use either 'COPY' or 'ADD'. Here, we're using 'ADD'.\nADD wave.tar.gz /var/www/html/\n\n# Start the Apache2 web server in the foreground\nCMD [\"/usr/sbin/apache2ctl\", \"-D\", \"FOREGROUND\"]\n\n# Expose port 80 for web traffic\nEXPOSE 80\n\n# Set the working directory to the Apache web server's document root\nWORKDIR /var/www/html\n\n# Create a volume for Apache2 log files\nVOLUME /var/log/apache2\n</code></pre> <p>Build the image from the Docker file \ud83d\uddbc\ufe0f</p> <p>Replace saitejairrinki/wavecafe:v1 with your preferred image name and tag.     <pre><code> docker build -t saitejairrinki/wavecafe:v1 . \n</code></pre></p> <p>Run Container From the Image \ud83d\udce6 <pre><code>docker run --name wavecafe -d -p 9699:80 saitejairrinki/wavecafe:v1\n</code></pre> Access it from your browser, ensuring that you have allowed port 9699 (or your chosen port) in your security group if you are using a cloud VM. \ud83c\udf10 <pre><code>Public-IPaddress:9699\n</code></pre></p> <p>Docker Image</p> <p>You can pull my image and start a container from it without needing to create a Dockerfile.</p> <pre><code>docker pull saitejairrinki/wavecafe:v1\n</code></pre> <p><pre><code>docker run --name wavecafe -d -p 9999:80 saitejairrinki/wavecafe:v1\n</code></pre> Now Access From the Browser, Make sure you have to allow the port number in my case 9999 in your security group if you are using cloud VM. <pre><code>Public-IPaddress:9999\n</code></pre></p>"},{"location":"devops/docker-setup/#docker-compose","title":"Docker Compose \ud83d\udcdf","text":"Single Docker File \ud83d\udcc4Multi Docker File \ud83d\udcd1Multiple Dockerfiles in Same Directory \ud83d\udcc2 <p>Creating Docker Compose for a single local Docker File</p> <pre><code>version: \"3\"\nservices:\n    Wavecafe:\n        build:\n            context: /Dockerfile_path/\n        ports:\n                - \"5555:80\"\n        container_name: wavecafe\n</code></pre> <p>Creating Docker Compose for a Multi local Docker File</p> <pre><code>version: '3'\n\nservices:\n  website1:\n    build:\n      context: /home   # Path to the first Dockerfile directory\n    ports:\n      - \"8081:80\"      # Map port 8081 on the host to port 80 in the container\n\n  website2:\n    build:\n      context: /tmp    # Path to the second Dockerfile directory\n    ports:\n      - \"8082:80\"      # Map port 8082 on the host to port 80 in the container\n</code></pre> <p>If you have multiple Dockerfiles in the same build context directory, you can use the dockerfile option to specify a different Dockerfile name.</p> <pre><code>version: '3'\n\nservices:\n  website1:\n    build:\n      context: ./website1\n      dockerfile: Dockerfile1\n    ports:\n      - \"8080:80\"  # Map host port 8080 to container port 80\n\n  website2:\n    build:\n      context: ./website2\n      dockerfile: Dockerfile2\n    ports:\n      - \"8081:80\"  # Map host port 8081 to container port 80\n</code></pre> <p>Creating Docker Compose for DockerHub Images <pre><code>version: '3'\nservices:\n  website:\n    image: saitejairrinki/wavecafe:v1\n    ports:\n      - \"8085:80\"\n</code></pre></p> <p>Start all containers defined in the docker-compose.yml file <pre><code>docker-compose up\n</code></pre></p> <p>Start containers in the background (detached mode) <pre><code>docker-compose up -d\n</code></pre></p> <p>Stop and remove all containers defined in the docker-compose.yml file <pre><code>docker-compose down\n</code></pre> Stop, remove containers, and remove volumes <pre><code>docker-compose down -v\n</code></pre></p> <p>List containers and their current status <pre><code>docker-compose ps\n</code></pre></p> <p>View logs of all containers defined in the docker-compose.yml file <pre><code>docker-compose logs\n</code></pre> Follow logs in real-time <pre><code>docker-compose logs -f\n</code></pre></p> <p>Build or rebuild Docker images for all services <pre><code>docker-compose build\n</code></pre></p> <p>Pull the latest images for all services <pre><code>docker-compose pull\n</code></pre></p> <p>Restart all containers <pre><code>docker-compose restart\n</code></pre></p> <p>Execute a command in a running container <pre><code>docker-compose exec &lt;service-name&gt; &lt;command&gt;\n</code></pre></p> <p>Stop all containers without removing them <pre><code>docker-compose stop\n</code></pre></p> <p>Start stopped containers <pre><code>docker-compose start\n</code></pre></p> <p>Stop and remove containers, networks, volumes, and associated Docker images <pre><code>docker-compose down --rmi all\n</code></pre></p> <p>Remove containers for services not defined in the docker-compose.yml file <pre><code>docker-compose down --remove-orphans\n</code></pre></p> Docker Volume \ud83d\udcbd <p>Creating a Separate Directory to Store Container data</p> <p><pre><code>mkdir mountbind\n</code></pre> Now link your Directory while running the container <pre><code>docker run --name db01 -e MYSQL_ROOT_PASSWORD=secret123 -p 3300:3306 -v /root/mountbind:/var/lib/mysql -d mysql:5.7\n</code></pre> Now do ls to the Directory there you can see the containers data <pre><code>ls mountbind\n</code></pre> Creating docker Volume, use the below command to see all the available options of docker volume <pre><code>docker volume --help\n</code></pre> Creating a new docker volume with name datadb <pre><code>docker volume create datadb\n</code></pre> Command removes all Docker volumes that are present on your system. <pre><code>docker volume rm $(docker volume ls -q)\n</code></pre> Now run your container with that volume <pre><code>docker run --name db02 -e MYSQL_ROOT_PASSWORD=secret123 -p 3301:3306 -v datadb:/var/lib/mysql -d mysql:5.7\n</code></pre> Now check  <pre><code>ls /var/lib/docker/volumes/datadb/_data/\n</code></pre> Now for testing Create any file with any name of your choice, here I'm creating a file with the name Milkyway <pre><code>touch /var/lib/docker/volumes/datadb/_data/milkyway\n</code></pre> Now log in into the container and Verify your file. <pre><code>docker exec -it db02 /bin/bash\n</code></pre> <pre><code>ls /var/lib/mysql/\n</code></pre> Now exit from the container <pre><code>exit\n</code></pre> If you want to access the MySQL database with a MySQL client then follow the below steps</p> <p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install mysql-client\n</code></pre> Now fetch the container IP by doing Docker Inspect <pre><code>docker inspect db02 |grep -i ipaddress\n</code></pre> Now Connect with that IP <pre><code>mysql -h 172.17.0.4 -u root -psecret123\n</code></pre></p>"},{"location":"devops/git/","title":"Git and GitHub: Empowering Collaborative Software Development","text":"<p>In the fast-paced world of software development, effective version control and collaboration are paramount. Enter Git and GitHub, the dynamic duo that has revolutionized how teams manage, track, and collaborate on code. In this blog, we'll explore the fundamentals of Git, the power of GitHub, and how they work together to empower collaborative software development.</p>"},{"location":"devops/git/#understanding-git","title":"Understanding Git","text":"<p>Git, created by Linus Torvalds in 2005, is a distributed version control system. It allows developers to track changes in their codebase, collaborate seamlessly, and easily manage different versions of their software.</p>"},{"location":"devops/git/#key-concepts","title":"Key Concepts","text":"<p>Repository (Repo): A Git repository is like a project's folder, containing all the files, history, and configurations related to the project. It can exist locally on your machine or remotely on a server.</p> <p>Commit : A commit is a snapshot of your code at a specific point in time. It represents a set of changes that you want to save.</p> <p>Branch: Branches are parallel lines of development. They allow you to work on different features or bug fixes independently.</p> <p>Merge: Merging combines the changes from one branch into another. It's often used to integrate completed features or fixes back into the main codebase.</p> <p>Pull Request (PR): A pull request is a request to merge changes from one branch into another. It's a common practice for code review and collaboration in open source and team projects.</p>"},{"location":"devops/git/#basic-git-workflow","title":"Basic Git Workflow","text":"<p>Here's a simplified Git workflow:</p> <ol> <li> <p>Initialize a Repository: Start a Git repository in your project folder with <code>git init</code>.</p> </li> <li> <p>Stage Changes: Use <code>git add</code> to stage changes you want to commit.</p> </li> <li> <p>Commit Changes: Commit your staged changes with <code>git commit -m \"Your commit message\"</code>.</p> </li> <li> <p>Create Branches: Create branches with <code>git branch branch-name</code>.</p> </li> <li> <p>Switch Branches: Move between branches with <code>git checkout branch-name</code>.</p> </li> <li> <p>Merge Branches: Merge branches with <code>git merge branch-name</code>.</p> </li> <li> <p>Collaborate: Push your changes to a remote repository and collaborate with others.</p> </li> </ol>"},{"location":"devops/git/#git-command-reference","title":"Git Command Reference","text":"<p>While we've covered the basics of Git here, you can dive deeper by referring to the Atlassian Git Cheatsheet. This resource provides a comprehensive list of Git commands and their usage.</p>"},{"location":"devops/git/#securely-adding-ssh-keys-to-github","title":"Securely Adding SSH Keys to GitHub \ud83d\udee1\ufe0f","text":""},{"location":"devops/git/#adding-ssh-keys-to-your-github-account","title":"Adding SSH Keys to Your GitHub Account\ud83c\udf10","text":"<ul> <li> <p>Description: GitHub allows SSH key authentication for secure repository access. Adding your public key to your GitHub account grants you secure access to repositories.</p> </li> <li> <p>Steps:</p> </li> <li>Generate an SSH key pair using <code>ssh-keygen</code> if you haven't already.</li> <li>Copy your public key to the clipboard:      <pre><code>cat ~/.ssh/id_rsa.pub | xclip -selection clipboard   # On Linux\n</code></pre></li> <li>Log in to your GitHub account.</li> <li>Go to Settings &gt; SSH and GPG keys.</li> <li>Click New SSH key.</li> <li>Paste your public key into the Key field and give it a meaningful title.</li> <li> <p>Click Add SSH key.</p> </li> <li> <p>Use Case: Adding SSH keys to GitHub simplifies secure access to your repositories without the need for a password.</p> </li> </ul>"},{"location":"devops/git/#continuous-integration-and-continuous-deployment-cicd-with-github-actions","title":"Continuous Integration and Continuous Deployment (CI/CD) with GitHub Actions","text":"<p>GitHub Actions is a powerful automation and CI/CD platform integrated with GitHub repositories. It allows you to build, test, and deploy your code directly from your GitHub repository.</p>"},{"location":"devops/git/#components-of-github-actions","title":"Components of GitHub Actions:","text":"<ul> <li>Workflows \ud83d\udd04</li> <li>Jobs \ud83d\udee0\ufe0f</li> <li>Runners \ud83c\udfc3</li> <li>Actions \ud83e\udd16</li> <li>Artifacts \ud83d\udce6</li> </ul>"},{"location":"devops/git/#real-time-use-case","title":"Real-Time Use Case:","text":"<p>GitHub Actions is best suited for developers and teams using GitHub for version control. It seamlessly integrates with your repositories and is an excellent choice for projects hosted on GitHub.</p>"},{"location":"devops/git/#mostly-used-for","title":"Mostly Used For:","text":"<ul> <li>Continuous Integration (CI)</li> <li>Continuous Deployment (CD)</li> <li>Automated Testing</li> <li>Workflow Automation</li> </ul>"},{"location":"devops/git/#understanding-github-actions-workflows","title":"Understanding GitHub Actions Workflows","text":"<p>Workflows in GitHub Actions are defined YAML files that specify the automation process for your project. A workflow can include one or more jobs, each consisting of a series of steps and actions to execute.</p>"},{"location":"devops/git/#github-actions-runner-types","title":"GitHub Actions Runner Types","text":"<p>GitHub Actions allows you to use different types of runners to execute your workflows:</p> <ul> <li>GitHub-hosted runners are provided by GitHub and offer a variety of pre-configured environments.</li> <li>Self-hosted runners are runners you can set up and maintain in your own infrastructure, giving you more control over the execution environment.</li> </ul>"},{"location":"devops/git/#installation-process","title":"Installation Process:","text":"<ol> <li>Access your GitHub repository.</li> <li>Navigate to the \"Actions\" tab.</li> <li>Create a new workflow or choose from existing templates.</li> <li>Define your workflow steps, including build and deployment tasks.</li> <li>Save your workflow configuration in a YAML file.</li> <li>Trigger your workflow manually or automatically based on events like code pushes or pull requests.</li> </ol>"},{"location":"devops/git/#creating-a-basic-github-actions-workflow","title":"Creating a Basic GitHub Actions Workflow","text":""},{"location":"devops/git/#yaml-method","title":"YAML Method:","text":"<pre><code>name: CI/CD with GitHub Actions\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout Repository\n      uses: actions/checkout@v2\n\n    - name: Print Hello World\n      run: echo 'Hello, World!'\n</code></pre> <p>To create a basic GitHub Actions workflow using YAML configuration:</p> <ol> <li>Access your GitHub repository.</li> <li>Navigate to the \"Actions\" tab.</li> <li>Click on \"Set up a workflow yourself\" to create a new YAML file.</li> <li>Copy and paste the provided YAML configuration.</li> <li>Save the file as <code>.github/workflows/main.yml</code>.</li> <li>Trigger your workflow manually or by pushing code changes.</li> </ol> <p>This workflow will execute and print \"Hello, World!\" when triggered by a code push.</p>"},{"location":"devops/git/#more-workflows","title":"More Workflows","text":"GitHub Auto-Push Workflow <p>This GitHub Actions workflow is designed to automate the process of pushing changes to a GitHub repository's main branch</p> <pre><code>name: Push Changes to GitHub\n\n# Define the trigger for this workflow: it runs when there are pushes to the specified branch (main).\non:\n  push:\n    branches:\n      - main  # Replace with the branch you want to trigger this on\n\njobs:\n  build:\n    # Specify that this job runs on a self-hosted runner.\n    runs-on: self-hosted\n\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        # Disable persisting credentials to use a personal access token (PAC) instead of the GITHUB_TOKEN.\n        persist-credentials: false\n\n        # Set the fetch depth to 0 to ensure the full history is fetched, avoiding errors when pushing refs.\n        fetch-depth: 0\n\n    - name: Create local changes\n      run: |\n        # Execute the 'free -m' command and redirect the output to a file named 'memory.txt'.\n        free -m &gt; memory.txt\n\n    - name: Commit files\n      run: |\n        # Add all changes to the Git staging area.\n        git add .\n\n        # Configure the user email and name for this local Git session.\n        git config --local user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n        git config --local user.name \"github-actions[bot]\"\n\n        # Commit all changes with the message \"Add changes [skip ci]\".\n        git commit -a -m \"Add changes [skip ci]\"\n\n    - name: Push changes\n      uses: ad-m/github-push-action@master\n      with:\n        # Provide the GitHub token (PAC) stored in GitHub secrets for authentication.\n        github_token: ${{ secrets.PAC }}\n\n        # Specify the target branch for pushing the changes (main).\n        branch: main\n</code></pre>"},{"location":"devops/git/#automated-github-website-deployment","title":"Automated GitHub Website Deployment","text":"<p>This script automates the process of setting up a website by downloading and extracting files from a URL, adding them to a Git repository, committing the changes, and pushing them to a remote Git repository. It also includes package installation and cleanup steps in a Linux Ubuntu OS.</p> <pre><code>#!/bin/bash\n# Website setup script\n\n# Define variables for URLs, file names, and repository information.\nURL=https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nFILE=2118_chilling_cafe\nGIT_URL=git@github.com:username/username.github.io.git\nGIT_REPO=username.github.io #use your github username \nPKG=apt\nEMAIL=admin123@gmail.com\nUSER=admin\n\n# Install necessary packages (e.g., git, wget, unzip) using the package manager specified in PKG.\nsudo $PKG install git wget unzip -y\n\n# Create a directory structure for Git and navigate to it.\nmkdir -p ~/git\ncd ~/git\n\n# Clone the specified Git repository.\ngit clone $GIT_URL\n\n# Remove the contents of the Git repository directory.\nrm -rf $GIT_REPO/*\n\n# Download a file from the specified URL and unzip it.\nwget $URL\nunzip $FILE.zip\n\n# Copy the unzipped files to the Git repository directory.\ncp -r $FILE/* $GIT_REPO/\n\n# Navigate to the Git repository directory.\ncd $GIT_REPO\n\n# Configure Git user email and name globally.\ngit config --global user.email \"$EMAIL\"\ngit config --global user.name \"$USER\"\n\n# Add all changes, commit with the current date, and push to the remote repository.\ngit add .\ngit commit -m \"$(date)\"\ngit push\n\n# Remove the temporary git directory.\nrm -rf ~/git\n</code></pre>"},{"location":"devops/jenkins/","title":"Jenkins","text":"<p>Jenkins is a continuous integration tool. It can fetch the code from the version control system, build the code, test it and notify the developer. Jenkins can do continuous delivery also. Jenkins has so many plugins, by using these plugins we can do any task in Jenkins. Jenkins is an open sources project. It is a java based web application server so we need to set up first java on the machine to run the jenkins server.</p>"},{"location":"devops/jenkins/#jenkins-ci-cd-automation","title":"Jenkins CI CD Automation","text":""},{"location":"devops/jenkins/#jenkins-setup","title":"Jenkins Setup","text":"UbuntuCentOS <pre><code> #!/bin/bash\nsudo apt update\nsudo apt install openjdk-11-jdk -y\ncurl -fsSL https://pkg.jenkins.io/debian/jenkins.io-2023.key | sudo tee \\\n  /usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install jenkins -y\n</code></pre> <pre><code>#!/bin/bash\nsudo wget -O /etc/yum.repos.d/jenkins.repo \\\n    https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key\nsudo yum upgrade -y\n# Add required dependencies for the jenkins package\nsudo yum install fontconfig java-11-openjdk -y\nsudo yum install jenkins -y\n</code></pre>"},{"location":"devops/jenkins/#cicd-setup","title":"CICD Setup","text":"<p>You can find my Jenkins CICD pipeline jobs which are saved in an HTML format, please download and extract to see the jobs </p> <p>To View My Jenkins CICD Jobs Click here</p>"},{"location":"devops/shellscripting/","title":"Shell Scripting","text":""},{"location":"devops/shellscripting/#if-condition","title":"If Condition","text":"If ConditionIf Else ConditionElif ConditionExample <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nelse\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n&lt;Command&gt;\n\nif [ &lt;condition&gt; ]\n\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nelif [ &lt;condition&gt; ]\nthen\n\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nelse\n    &lt;Commands to Execute&gt;\n    &lt;print Messages with echo&gt;\n\nfi\n</code></pre> <pre><code>#!/bin/bash\n\napt --help &amp;&gt;&gt; /dev/null\n\nif [ $? -eq 0 ]\n\nthen\n     echo \" This is Ubuntu Operating System\"\nelse\n     echo \" This is CentOS Operating System\"\nfi\n</code></pre> Reference Link <p>How to program with Bash</p> <p>https://opensource.com/article/19/10/programming-bash-logical-operators-shell-expansions</p>"},{"location":"devops/shellscripting/#for-loop","title":"For Loop","text":"For LoopExample <pre><code>#!/bin/bash\n\nfor &lt;variable&gt; in &lt;list&gt;\ndo\n   &lt;command&gt;\ndone\n</code></pre> <pre><code>#For loop example for Creating Files with name alpha beta gamma\n\n#!/bin/bash\n\nfor FILE in alpha beta gamma \ndo\n   echo \"Creating file $FILE in system\"\n\n   sudo touch $FILE\n\ndone\n</code></pre>"},{"location":"devops/shellscripting/#while-loop","title":"While Loop","text":"While LoopWhile Loop Example <pre><code>#!/bin/bash\nwhile [ &lt;condition&gt; ]\ndo\n   &lt;command&gt;\ndone\n</code></pre> <pre><code>#!/bin/bash\n#Here variable \"a\" is speed\n\na=0\necho \"Starting the Engine\"\n\nwhile [ $a -le 100 ]\ndo\n    sleep 1\n    echo \"Current Speed $a\"\n\n    a=$(($a+10))\n\ndone\n</code></pre>"},{"location":"devops/shellscripting/#functions","title":"Functions","text":"Empty String  <p>Certainly! Here's an example of how you can create a Bash script with a main function to call the <code>check_empty_string</code> function:</p> <pre><code>#!/bin/bash\n\ncheck_empty_string() {\n    if [ -z \"$1\" ]; then\n        echo \"Error: Input string is empty!\"\n    else\n        echo \"Input string is not empty.\"\n    fi\n}\n\nmain() {\n    input=\"example\"\n    check_empty_string \"$input\"\n}\n\n# Call the main function\nmain\n</code></pre> <p>In this script, the <code>main</code> function is defined to call the <code>check_empty_string</code> function with the <code>input</code> variable. You can replace <code>\"example\"</code> with any input string you want to test. When you run the script, it will execute the <code>main</code> function, which in turn calls the <code>check_empty_string</code> function.</p> <p>Sure, here are a few basic reusable Bash functions along with their use cases and examples:</p> greetis_evenfile_infoSumreverse <p>Use Case: A function to greet a person.</p> <pre><code>greet() {\n    echo \"Hello, $1!\"\n}\n\n# Example usage\ngreet \"Alice\"\ngreet \"Bob\"\n</code></pre> <p>Use Case: A function to check if a number is even.</p> <pre><code>is_even() {\n    if (( $1 % 2 == 0 )); then\n        echo \"Even\"\n    else\n        echo \"Odd\"\n    fi\n}\n\n# Example usage\nis_even 4  # Output: Even\nis_even 7  # Output: Odd\n</code></pre> <p>Use Case: A function to display information about a file.</p> <pre><code>file_info() {\n    echo \"File: $1\"\n    echo \"Size: $(du -h \"$1\" | cut -f1)\"\n    echo \"Owner: $(stat -c %U \"$1\")\"\n}\n\n# Example usage\nfile_info \"/path/to/somefile.txt\"\n</code></pre> <p>Use Case: A function to calculate the sum of two numbers.</p> <pre><code>sum() {\n    echo \"$(($1 + $2))\"\n}\n\n# Example usage\nresult=$(sum 5 3)\necho \"Sum: $result\"  # Output: Sum: 8\n</code></pre> <p>Use Case: A function to reverse a string.</p> <pre><code>reverse() {\n    echo \"$1\" | rev\n}\n\n# Example usage\nreversed=$(reverse \"hello\")\necho \"Reversed: $reversed\"  # Output: Reversed: olleh\n</code></pre>"},{"location":"devops/shellscripting/#shell-script-for-setting-up-website","title":"Shell Script For Setting Up Website","text":"UbuntuCentosUsing VariablesUsing If Else Condition <pre><code>#!/bin/bash\nsudo apt update\nsudo apt install wget net-tools unzip figlet apache2 -y\nsudo systemctl start apache2\nsudo systemctl enable apache2\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip  \nsudo unzip -o 2118_chilling_cafe.zip\nsudo rm -rf /var/www/html/*\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\ncd ..\nsudo rm -rf webfiles\nsudo systemctl restart apache2\nfiglet done\n</code></pre> <pre><code>#!/bin/bash\nsudo yum install wget net-tools unzip httpd -y\nsudo systemctl start httpd\nsudo systemctl enable httpd\nmkdir -p webfiles\ncd webfiles\nsudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\nsudo unzip -o 2118_chilling_cafe.zip\nsudo rm -rf /var/www/html/*\nsudo cp -r 2118_chilling_cafe/* /var/www/html/\ncd ..\nsudo rm -rf webfiles\nsudo systemctl restart httpd\n</code></pre> <pre><code>#!/bin/bash\n#Website setup\n#Adding variables :-)\nURL=https://templatemo.com/tm-zip-files-2020/templatemo_520_highway.zip\nSRV=httpd\nPKG=yum\nFILE=templatemo_520_highway\necho \"  Installing the Services &amp; Extractors\"\necho\nsudo $PKG install $SRV wget unzip -y &amp;&gt;&gt; /dev/null\necho \"Start &amp; Enabling the Services\"\necho\nsudo systemctl start $SRV\nsudo systemctl enable $SRV\necho \"Downloading the zip file from tooplate.com\"\necho\nmkdir -p webfiles\ncd webfiles\necho\nsudo wget $URL &amp;&gt;&gt; /dev/null\necho \"extracting the files \"\necho\nsudo unzip -o $FILE.zip &amp;&gt;&gt; /dev/null\necho \"copying the extracted file into html\"\necho\necho \"Cleaning files in html Directory\"\nsudo rm -rf /var/www/html/*\necho\nsudo cp -r $FILE/* /var/www/html &amp;&gt;&gt; /dev/null\necho \"Restarting the Services\"\nsudo systemctl restart $SRV\ncd ..\nsudo rm -rf webfiles\nsudo systemctl status $SRV | grep Active\ndate\n</code></pre> <pre><code>#!/bin/bash\napt --help &amp;&gt;&gt; /dev/null\nif [ $? -eq 0 ]\nthen \n   sudo apt update\n   sudo apt install wget figlet net-tools unzip apache2 -y\n   sudo systemctl start apache2\n   sudo systemctl enable apache2\n   mkdir -p webfiles\n   cd webfiles\n   sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip\n   sudo unzip -o 2118_chilling_cafe.zip\n   sudo rm -rf /var/www/html/*\n   sudo cp -r 2118_chilling_cafe/* /var/www/html/\n   cd ..\n   sudo rm -rf webfiles\n   sudo systemctl restart apache2\n   sudo systemctl status apache2 | grep Active\n   figlet done\nelse\n    sudo yum install wget net-tools unzip httpd -y\n    sudo systemctl start httpd\n    sudo systemctl enable httpd\n    sudo mkdir -p webfiles\n    cd webfiles\n    sudo wget https://www.tooplate.com/zip-templates/2118_chilling_cafe.zip # (1)!\n    sudo unzip -o 2118_chilling_cafe.zip\n    sudo rm -rf /var/www/html/*\n    sudo cp -r 2118_chilling_cafe/* /var/www/html/\n    cd ..    \n    sudo rm -rf webfiles\n    sudo systemctl restart httpd\n    sudo systemctl status httpd | grep Active\nfi\n</code></pre> <ol> <li>You can use more templates from tooplate.com</li> </ol>"},{"location":"devops/terraform/","title":"Terraform","text":"<p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can help with multi-cloud by having one workflow for all clouds. The infrastructure Terraform manages can be hosted on public clouds like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, or on-prem in private clouds such as VMWare vSphere.</p>"},{"location":"devops/terraform/#terraform-installation","title":"Terraform Installation \ud83d\udce5","text":"Ubuntu \ud83d\udc27CentOS \ud83d\udc27 <pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\n</code></pre> <pre><code>sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\n</code></pre> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install terraform\n</code></pre> <pre><code>sudo yum install -y yum-utils\n</code></pre> <pre><code>sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\n</code></pre> <pre><code>sudo yum -y install terraform\n</code></pre>"},{"location":"devops/terraform/#reference","title":"Reference \ud83d\udccc","text":"Terraform Registry <p>https://registry.terraform.io/</p> Terraform Commands Cheat sheet Reference Link <p>https://acloudguru.com/blog/engineering/the-ultimate-terraform-cheatsheet</p>"},{"location":"devops/terraform/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like Ec2 Instances, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., Ec2 Full Access, S3 Full Access). Note down the user's access key ID and secret access key. \ud83d\udccc Reference Document Link</p> </li> </ol> <p>Warning</p> <p>Utilize Role-Based Authentication when working with Terraform on AWS Instances or Services, as it offers a higher level of security compared to using access keys.</p>"},{"location":"devops/terraform/#terraform-sample-scripts","title":"Terraform Sample Scripts","text":"<p>Provisioning an AWS EC2 Instance </p> Utilizing an existing key and security groupProvisioning with New Key and Security Group main.tf<pre><code>provider \"aws\" {\n  region = \"us-east-2\"\n}\n\nresource \"aws_instance\" \"Instance\" {\n  ami                    = \"ami-0fb653ca2d3203ac1\"\n  instance_type          = \"t2.micro\"\n  key_name               = \"terraform\"\n  vpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n  tags = {\n    Name = \"IAAC\"\n    Team = \"DevOps\"\n  }\n}\n</code></pre> main.tf<pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_key_pair\" \"example_keypair\" {\n  key_name   = \"satrun\"\n  public_key = tls_private_key.example_keypair.public_key_openssh\n}\n\nresource \"tls_private_key\" \"example_keypair\" {\n  algorithm = \"RSA\"\n}\n\nresource \"local_file\" \"private_key_file\" {\n  content  = tls_private_key.example_keypair.private_key_pem\n  filename = \"satrun.pem\"\n}\n\nresource \"aws_security_group\" \"example_security_group\" {\n  name        = \"satrun-security-group\"\n  description = \"Example Security Group\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"YOUR PUBLIC IP/32\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"YOUR PUBLIC IP/32\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"example_instance\" {\n  ami                = \"ami-09e67e426f25ce0d7\" # Ubuntu 20.04 LTS in us-east-1 (N. Virginia) region\n  instance_type      = \"t2.micro\"\n  key_name           = aws_key_pair.example_keypair.key_name\n  vpc_security_group_ids = [aws_security_group.example_security_group.id]\n\n  tags = {\n    Name = \"satrun-instance\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#terraform-vars","title":"Terraform Vars","text":"<p>Using Variables</p> <p>Create a File with name vars.tf </p> <p><pre><code>vim vars.tf\n</code></pre> vars.tf<pre><code>variable \"REGION\" {\n  default = \"us-east-2\"\n}\n\nvariable \"ZONE1\" {\n  default = \"us-east-2a\"\n}\n\nvariable \"AMIS\" {\n  type = map(any)\n  default = {\n    us-east-2 = \"ami-0fb653ca2d3203ac1\"\n    us-east-1 = \"ami-0e1d30f2c40c4c701\"\n  }\n}\n</code></pre></p>"},{"location":"devops/terraform/#terraform-provider","title":"Terraform Provider","text":"<p>Example for AWS <pre><code>vim provider.tf\n</code></pre></p> provider.tf<pre><code>provider \"aws\" {\n  region = var.REGION\n}\n</code></pre> <p>Launching Instance with Vars File</p> <pre><code>vim main.tf\n</code></pre> main.tf<pre><code>resource \"aws_instance\" \"Instance\" {\n  ami                    = var.AMIS[var.REGION]\n  instance_type          = \"t2.micro\"\n  key_name               = \"terraform\"\n  vpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n  tags = {\n    Name = \"IAAC\"\n    Team = \"DevOps\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#terraform-provisioning","title":"Terraform Provisioning","text":"<p>Launching AWS Resources with Terraform Provisioning</p> main.tf<pre><code>resource \"aws_key_pair\" \"testing007\" {\n  key_name   = \"testing007\"\n  public_key = file(\"testing007.pub\")\n }\n\nresource \"aws_instance\" \"Instance\" {\n  ami                    = var.AMIS[var.REGION]\n  instance_type          = \"t2.micro\"\n  key_name               = aws_key_pair.testing007.key_name\n  vpc_security_group_ids = [\"sg-0138c7796472ac9a9\"]\n  tags = {\n    Name = \"IAAC\"\n    Team = \"DevOps\"\n }\n\n  provisioner \"file\" {\n    source      = \"./web.sh\"\n    destination = \"/tmp/web.sh\"\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"chmod u+x /tmp/web.sh\",\n      \"sudo /tmp/web.sh\"\n    ]\n  }\n\n  connection {\n    user        = var.USER\n    private_key = file(\"testing007\")\n    host        = self.public_ip\n  }\n }\n\noutput \"PublicIP\" {\n  value = aws_instance.Instance.public_ip\n}\n</code></pre> <p>Variables file - vars.tf </p> vars.tf<pre><code>variable \"REGION\" {\n  default = \"us-east-2\"\n}\n\nvariable \"ZONE1\" {\n  default = \"us-east-2a\"\n}\n\nvariable \"USER\" {\n  default = \"ubuntu\"\n}\n\nvariable \"AMIS\" {\n  type = map(any)\n  default = {\n    us-east-2 = \"ami-0fb653ca2d3203ac1\"\n    us-east-1 = \"ami-0e1d30f2c40c4c701\"\n  }\n\n}\n</code></pre>"},{"location":"devops/terraform/#to-store-state-remotely-in-s3-bucket","title":"To Store State Remotely in S3 Bucket","text":"<p>Create an S3 Bucket</p> main.tf<pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"terraform-state-009\"\n    key    = \"terraform/remote\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/#running-terraform-script-using-pipeline","title":"Running Terraform Script using Pipeline","text":"Info <p>Enable Authentications such as service connections, OAuth Tokens and etc of the required platform with your pipeline</p> main.tf<pre><code>Terraform with Pipeline\n\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_key_pair\" \"example_keypair\" {\n  key_name   = \"mars\"\n  public_key = tls_private_key.example_keypair.public_key_openssh\n}\n\nresource \"tls_private_key\" \"example_keypair\" {\n  algorithm = \"RSA\"\n}\n\nresource \"local_file\" \"private_key_file\" {\n  content  = tls_private_key.example_keypair.private_key_pem\n  filename = \"mars.pem\"\n}\n\nresource \"aws_security_group\" \"example_security_group\" {\n  name        = \"mars-security-group\"\n  description = \"Example Security Group\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"183.82.115.199/32\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"183.82.115.199/32\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"example_instance\" {\n  ami                = \"ami-09e67e426f25ce0d7\" # Ubuntu 20.04 LTS in us-east-1 (N. Virginia) region\n  instance_type      = \"t2.micro\"\n  key_name           = aws_key_pair.example_keypair.key_name\n  vpc_security_group_ids = [aws_security_group.example_security_group.id]\n\n  tags = {\n    Name = \"mars-instance\"\n  }\n}\n\nterraform {\n\n  backend \"s3\" {\n\n    bucket = \"terraform-azuredevops-saitejairrinki-org\"\n\n    key    = \"terraform/remote\"\n\n    region = \"us-east-1\"\n\n  }\n\n}\n</code></pre>"},{"location":"devops/vagrant/","title":"Vagrant \ud83d\udee0\ufe0f","text":"<p>Vagrant is an open-source tool that helps us to automate the creation and management of Virtual Machines. In a nutshell, we can specify the configuration of a virtual machine in a simple configuration file, and Vagrant creates the same Virtual machine using just one simple command. It provides command-line interfaces to automate such tasks.</p> Requirements <p>Virtualbox \ud83d\udce6</p> <p>Vagrant \ud83d\udce6</p>"},{"location":"devops/vagrant/#installing-virtualbox-in-the-host-machine","title":"Installing Virtualbox in the Host Machine","text":"Windows \ud83e\ude9fUbuntu Desktop \ud83d\udc27 <p>Download link <pre><code>https://download.virtualbox.org/virtualbox/6.1.30/VirtualBox-6.1.30-148432-Win.exe\n</code></pre></p> <p>Through Command line  <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install virtualbox\n</code></pre></p> Note <p>To Run Virtual Machine You should need to Disable the Secure-Boot in BIOS Options of your Machine \ud83d\udeab</p> <p>To Verify that your Virtual Box is Working fine or not in Linux, Please hit the below command. <pre><code>sudo systemctl status virtualbox.service\n</code></pre></p>"},{"location":"devops/vagrant/#installing-vagrant-in-the-host-machine","title":"Installing Vagrant in the Host Machine","text":"WindowsUbuntu Desktop <p>Download link <pre><code>https://releases.hashicorp.com/vagrant/2.3.4/vagrant_2.3.4_windows_i686.msi\n</code></pre></p> <p>Through Command line  <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install vagrant\n</code></pre></p> Note <p>You Should Need to Restart Your Machine After Installation of Vagrant </p>"},{"location":"devops/vagrant/#creating-a-common-directory-for-all-vms","title":"Creating a Common Directory for all VMs","text":"<p>Create a Directory with name vagrantvms \ud83d\udcc2 <pre><code>mkdir vagrantvms\n</code></pre> Change Directory to vagrantvms <pre><code>cd vagrantvms\n</code></pre></p>"},{"location":"devops/vagrant/#to-bring-up-vm","title":"To Bring up VM \ud83d\ude80","text":"Ubuntu \ud83d\udc27CentOS \ud83d\udc27 <p>Create a Directory name ubuntu <pre><code>mkdir ubuntu \n</code></pre> Change Directory to ubuntu <pre><code>cd ubuntu\n</code></pre> Initialize Ubuntu VM <pre><code>vagrant init ubuntu/bionic64 \n</code></pre> Now Bring up your Ubuntu VM  <pre><code>vagrant up\n</code></pre> Once your VM is up then login to your vm  <pre><code>vagrant ssh\n</code></pre> Now you can see the prompt of the ubuntu machine, If you wanna exit type exit</p> <p>Create a Directory name centos <pre><code>mkdir centos \n</code></pre> Change Directory to centos <pre><code>cd centos\n</code></pre> Initialize centos VM <pre><code>vagrant init geerlingguy/centos7 \n</code></pre> Now Bring up your centos VM  <pre><code>vagrant up\n</code></pre> Once your VM is up then login to your VM  <pre><code>vagrant ssh\n</code></pre> Now you can see the prompt of the centos machine, If you wanna exit type  \ud83d\udc49 exit</p> To Init Specific Vagrant box <p>https://app.vagrantup.com/boxes/search</p> <p>=== \"Multi VMS Vagrantfile\" \ud83d\udce6</p> <pre><code>Vagrant.configure(\"2\") do |config|\n  config.hostmanager.enabled = true \n  config.hostmanager.manage_host = true\n\n### Nginx VM ###\n  config.vm.define \"web01\" do |web01|\n    web01.vm.box = \"ubuntu/bionic64\"\n    web01.vm.hostname = \"web01\"\n    web01.vm.network \"private_network\", ip: \"192.168.33.11\"\n    web01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n  end\n\n### tomcat vm ###\n   config.vm.define \"app01\" do |app01|\n    app01.vm.box = \"geerlingguy/centos7\"\n    app01.vm.hostname = \"app01\"\n    app01.vm.network \"private_network\", ip: \"192.168.33.12\"\n    app01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n    app01.vm.provider \"virtualbox\" do |vb|\n     vb.memory = \"1024\"\n     end\n    app01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n#     yum update -y\n     yum install epel-release -y\n   SHELL\n   end\n\n### RabbitMQ vm  ####\n  config.vm.define \"rmq01\" do |rmq01|\n    rmq01.vm.box = \"geerlingguy/centos7\"\n    rmq01.vm.hostname = \"rmq01\"\n    rmq01.vm.network \"private_network\", ip: \"192.168.33.16\"\n    rmq01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n    rmq01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n   # yum update -y\n    #yum install epel-release -y\n    # yum install wget -y\n    SHELL\n  end ; f\n  ., v/\n### Memcache vm  #### \n  config.vm.define \"mc01\" do |mc01|\n    mc01.vm.box = \"geerlingguy/centos7\"\n    mc01.vm.hostname = \"mc01\"\n    mc01.vm.network \"private_network\", ip: \"192.168.33.14\"\n    mc01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n    mc01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n     yum install epel-release -y\n    SHELL\n  end\n\n### DB vm  ####\n  config.vm.define \"db01\" do |db01|\n    db01.vm.box = \"geerlingguy/centos7\"\n    db01.vm.hostname = \"db01\"\n    db01.vm.network \"private_network\", ip: \"192.168.33.15\"\n    db01.vm.synced_folder \"../vprofile-code\", \"/vprofile-vm-data\"\n    db01.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n#   yum update -y\n    yum install epel-release -y\n    SHELL\n  end\n\n\nend\n</code></pre>"},{"location":"devops/k8s/architecture/","title":"Kubernetes (K8s) Architecture and Its Components \ud83d\ude80","text":""},{"location":"devops/k8s/architecture/#what-is-kubernetes-and-why-do-we-need-it","title":"What is Kubernetes, and Why Do We Need It? \ud83e\udd14","text":"<p>Kubernetes, often abbreviated as K8s (K-eight-s), is a powerful open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). But why do we need Kubernetes in the first place?</p> <p>Containerization, popularized by technologies like Docker, allows developers to package applications and their dependencies into portable units called containers. Containers make it easy to ensure consistency across different environments and simplify the deployment process. However, managing containers at scale can quickly become complex and challenging. This is where Kubernetes comes to the rescue.</p> <p>Kubernetes provides a robust and flexible framework for orchestrating containerized applications, allowing you to:</p> <ul> <li>Scale: Automatically scale your applications up or down based on demand.</li> <li>Deploy: Roll out new versions of your application without downtime.</li> <li>Manage: Efficiently manage and distribute workloads across clusters of machines.</li> <li>Maintain: Handle failovers and ensure high availability.</li> <li>Control: Define and enforce resource usage policies.</li> <li>Automate: Automate many aspects of application management.</li> </ul>"},{"location":"devops/k8s/architecture/#kubernetes-architecture","title":"Kubernetes Architecture \ud83c\udfdb\ufe0f","text":"<p>To understand Kubernetes fully, it's essential to grasp its architecture. Kubernetes consists of several major components and concepts working together seamlessly to create a powerful container orchestration system.</p> <p></p> <p>Let's dive into these major components and explore some example commands for managing them:</p>"},{"location":"devops/k8s/architecture/#k8s-cluster","title":"K8s Cluster \ud83c\udf10","text":"<p>A Kubernetes cluster is the foundation of your container orchestration environment. It's a set of machines, physical or virtual, that run your containerized applications. The cluster is divided into two main components: the control plane and the nodes.</p>"},{"location":"devops/k8s/architecture/#control-plane","title":"Control Plane \ud83d\udd79\ufe0f","text":"<p>The control plane is the brain of the Kubernetes cluster. It manages and controls all the nodes and containers within the cluster. It consists of several key components:</p>"},{"location":"devops/k8s/architecture/#kube-api-server","title":"Kube API Server \ud83c\udf10","text":"<p>The Kubernetes API server is the entry point for all administrative tasks. It exposes the Kubernetes API, which administrators, developers, and other services interact with to manage and control the cluster.</p> <pre><code>kubectl get pods --namespace=kube-system\n</code></pre>"},{"location":"devops/k8s/architecture/#etcd-db","title":"Etcd (DB) \ud83d\uddc4\ufe0f","text":"<p>Etcd is a distributed key-value store used to store all cluster data. It serves as Kubernetes' backing store for all cluster configuration data, ensuring consistency and high availability.</p> <pre><code>kubectl get etcd --namespace=kube-system\n</code></pre>"},{"location":"devops/k8s/architecture/#kube-scheduler","title":"Kube Scheduler \ud83d\udcc5","text":"<p>The scheduler is responsible for distributing work (containers) across multiple nodes in the cluster. It considers factors like resource requirements and constraints to make intelligent decisions.</p> <pre><code>kubectl get pods --namespace=kube-system -o wide\n</code></pre>"},{"location":"devops/k8s/architecture/#kube-controller-manager","title":"Kube Controller Manager \ud83d\udd74\ufe0f","text":"<p>The controller manager runs controller processes that regulate the state of the system. These controllers ensure that the cluster moves towards the desired state and handles events like node failures or application scaling.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/k8s/architecture/#node-components","title":"Node Components \ud83d\udda5\ufe0f","text":"<p>Nodes are the worker machines that run containerized applications. Each node in the cluster has the following components:</p>"},{"location":"devops/k8s/architecture/#kubelet","title":"Kubelet \ud83e\uddd1\u200d\u2708\ufe0f","text":"<p>The kubelet is an agent that runs on each node. It ensures that containers are running in a Pod, a basic deployable unit in Kubernetes. Kubelet communicates with the control plane to manage containers on the node.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/k8s/architecture/#kube-proxy","title":"Kube Proxy \ud83d\udd00","text":"<p>Kube Proxy is responsible for network connectivity. It maintains network rules on nodes and performs network proxying for services in the cluster.</p> <pre><code>kubectl get services\n</code></pre>"},{"location":"devops/k8s/architecture/#container-runtime","title":"Container Runtime \ud83d\udc33","text":"<p>The container runtime is the software responsible for running containers. Kubernetes supports various container runtimes, including Docker, containerd, and CRI-O.</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"devops/k8s/architecture/#coredns","title":"CoreDNS \u2601\ufe0f","text":"<p>CoreDNS is a lightweight, flexible, and extensible DNS server that Kubernetes uses for service discovery within the cluster.</p> <pre><code>kubectl get svc --namespace=kube-system\n</code></pre>"},{"location":"devops/k8s/architecture/#kubectl","title":"Kubectl \ud83d\udee0\ufe0f","text":"<p>Kubectl is the command-line tool used to interact with Kubernetes clusters. It allows you to create, inspect, update, and delete resources in your cluster.</p> <pre><code>kubectl version\n</code></pre>"},{"location":"devops/k8s/architecture/#kubernetes-objects-resources","title":"Kubernetes Objects (Resources) \ud83d\udce6","text":"<p>Kubernetes uses a declarative configuration model. You describe the desired state of your applications and infrastructure using Kubernetes objects or resources. Here are some of the essential types of Kubernetes objects along with example commands for managing them:</p>"},{"location":"devops/k8s/architecture/#workloads","title":"Workloads \ud83d\udcbc","text":"<p>Workloads represent your applications and their components. Common workload objects include:</p> <ul> <li>Deployment: Manages the deployment and scaling of Pods.</li> </ul> <pre><code>kubectl create deployment my-deployment --image=my-image:v1\n</code></pre> <ul> <li>ReplicaSet: Ensures a specified number of replicas of a Pod are running.</li> </ul> <pre><code>kubectl scale deployment my-deployment --replicas=3\n</code></pre> <ul> <li>StatefulSet: Manages stateful applications with unique network identities.</li> </ul> <pre><code>kubectl get statefulsets\n</code></pre> <ul> <li>Pod: The smallest deployable unit in Kubernetes.</li> </ul> <pre><code>kubectl get pods\n</code></pre> <ul> <li>DaemonSet: Ensures that all or some nodes run a copy of a Pod.</li> </ul> <pre><code>kubectl get daemonset\n</code></pre> <ul> <li>ScheduledJob: Runs Jobs periodically.</li> </ul> <pre><code>kubectl get scheduledjobs\n</code></pre> <ul> <li>Job: Runs a specified task to completion.</li> </ul> <pre><code>kubectl get jobs\n</code></pre> <ul> <li>CronJob: Runs Jobs on a schedule.</li> </ul> <pre><code>kubectl get cronjobs\n</code></pre>"},{"location":"devops/k8s/architecture/#services","title":"Services \ud83c\udf10","text":"<p>Services define networking rules to access your application. They include:</p> <ul> <li>Service: Exposes a set of Pods as a network service.</li> </ul> <pre><code>kubectl expose deployment my-deployment --port=80 --target-port=8080 --type=LoadBalancer\n</code></pre> <ul> <li>Service Types: ClusterIP, NodePort, LoadBalancer.</li> </ul> <pre><code>kubectl get services\n</code></pre> <ul> <li>IngressController and Ingress: Manages external access to the services.</li> </ul> <pre><code>kubectl get ingresses\n</code></pre> <ul> <li>Labels and Selectors: Labels are used to select and organize objects.</li> </ul> <pre><code>kubectl get pods -l app=my-app\n</code></pre>"},{"location":"devops/k8s/architecture/#configuration","title":"Configuration \u2699\ufe0f","text":"<p>Configuration objects help manage application configuration data:</p> <ul> <li>Secrets: Store sensitive information, like passwords or API keys.</li> </ul> <pre><code>kubectl create secret generic my-secret --from-literal=username=myuser --from-literal=password=mypassword\n</code></pre> <ul> <li>ConfigMaps: Store configuration data as key-value pairs.</li> </ul> <p>```shell   kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value</p> <p>2   ```</p> <ul> <li>KubeConfig: Configure access to Kubernetes clusters.</li> </ul> <pre><code>kubectl config set-context my-context --cluster=my-cluster --user=my-user\n</code></pre>"},{"location":"devops/k8s/architecture/#storage","title":"Storage \ud83d\uddc4\ufe0f","text":"<p>Storage objects manage data storage:</p> <ul> <li>Volumes and PVC: Provide storage for Pods.</li> </ul> <pre><code>kubectl get pv\n</code></pre> <ul> <li>Storage Classes: Define different classes of storage.</li> </ul> <pre><code>kubectl get storageclass\n</code></pre>"},{"location":"devops/k8s/architecture/#security","title":"Security \ud83d\udd10","text":"<p>Security objects help secure your cluster:</p> <ul> <li>RBAC (Role-Based Access Control): Control access to resources based on roles.</li> </ul> <pre><code>kubectl create clusterrole my-role --verb=get,list --resource=pods\n</code></pre> <ul> <li>Service Accounts: Manage permissions for Pods.</li> </ul> <pre><code>kubectl create serviceaccount my-serviceaccount\n</code></pre>"},{"location":"devops/k8s/architecture/#set-resource-limits","title":"Set Resource Limits \u2696\ufe0f","text":"<p>You can set resource limits on Pods to manage resource usage effectively.</p> <pre><code>kubectl set resources deployment/my-deployment --limits=cpu=500m,memory=256Mi\n</code></pre>"},{"location":"devops/k8s/architecture/#health-checks","title":"Health Checks \u2764\ufe0f","text":"<p>Kubernetes supports health checks to ensure that your applications are running correctly.</p> <pre><code>kubectl get pods --field-selector=status.phase=Running\n</code></pre>"},{"location":"devops/k8s/eks/","title":"Spinning Up a Kubernetes Cluster on AWS EKS","text":"<p>The process of creating a Kubernetes cluster on Amazon Web Services (AWS) using Amazon Elastic Kubernetes Service (EKS). Kubernetes is an open-source container orchestration platform that helps manage containerized applications and services. Amazon EKS simplifies the setup and management of Kubernetes clusters, allowing you to focus on deploying and managing your applications.</p>"},{"location":"devops/k8s/eks/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>AWS Account: You need an active AWS account with necessary permissions to create resources like EKS clusters, IAM roles, VPC, etc.</p> </li> <li> <p>AWS CLI: Install and configure the AWS Command Line Interface (CLI) on your local machine. You can download it from the AWS CLI Documentation.</p> </li> <li> <p>kubectl: Install <code>kubectl</code>, the Kubernetes command-line tool, to interact with your cluster. You can follow the installation instructions in the Kubernetes Documentation.</p> </li> <li> <p>IAM User: Create an AWS IAM user with programmatic access and necessary permissions (e.g., AmazonEKSClusterPolicy, AmazonEKSServicePolicy). Note down the user's access key ID and secret access key. Document Link</p> </li> <li> <p>Key Pair (Optional): If you plan to SSH into your worker nodes, create an EC2 key pair or use an existing one. This can be done through the AWS Management Console.</p> </li> </ol>"},{"location":"devops/k8s/eks/#steps-to-create-an-eks-cluster","title":"Steps to Create an EKS Cluster","text":""},{"location":"devops/k8s/eks/#step-1-configure-the-aws-cli-with-the-iam-users-access-key-id-and-secret-access-key","title":"Step 1: Configure the AWS CLI with the IAM user's access key ID and secret access key:","text":"<pre><code>aws configure\n</code></pre>"},{"location":"devops/k8s/eks/#step-2-create-an-eks-cluster","title":"Step 2: Create an EKS Cluster","text":"<p>Of course, here are all the commands following the format you've provided:</p>"},{"location":"devops/k8s/eks/#command-1-create-an-eks-cluster","title":"Command 1: Create an EKS Cluster","text":"<pre><code>eksctl create cluster --name &lt;cluster-name&gt; --version &lt;k8s-version&gt; --region &lt;region&gt; --nodegroup-name &lt;nodegroup-name&gt; --node-type &lt;node-type&gt; --nodes &lt;node-count&gt; --nodes-min &lt;min-nodes&gt; --nodes-max &lt;max-nodes&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl create cluster --name kube-app --version 1.22 --region eu-central-1 --nodegroup-name kube-app --node-type t2.micro --nodes 1 --nodes-min 1 --nodes-max 3\n</code></pre>"},{"location":"devops/k8s/eks/#command-2-get-information-about-nodes","title":"Command 2: Get Information About Nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"devops/k8s/eks/#command-3-update-node-group-maximum-nodes-and-minimum-nodes","title":"Command 3: Update Node Group Maximum Nodes and Minimum Nodes","text":"<pre><code>eksctl update nodegroup --cluster=&lt;cluster-name&gt; --region=&lt;region&gt; --name=&lt;nodegroup-name&gt; --nodes-min=&lt;min-nodes&gt; --nodes-max=&lt;max-nodes&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl update nodegroup --cluster kube-app --region eu-central-1 --name kube-app --nodes-min=1 --nodes-max=2\n</code></pre>"},{"location":"devops/k8s/eks/#command-4-scale-node-group","title":"Command 4: Scale Node Group","text":"<pre><code>eksctl scale nodegroup --cluster &lt;cluster-name&gt; --region &lt;region&gt; --name &lt;nodegroup-name&gt; --nodes &lt;desired-nodes&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl scale nodegroup --cluster kube-app --region eu-central-1 --name kube-app --nodes 2\n</code></pre>"},{"location":"devops/k8s/eks/#command-5-get-node-group-details","title":"Command 5: Get Node Group Details","text":"<pre><code>eksctl get nodegroup --region &lt;region&gt; --name &lt;nodegroup-name&gt; --cluster &lt;cluster-name&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl get nodegroup --region eu-central-1 --name kube-app --cluster kube-app\n</code></pre>"},{"location":"devops/k8s/eks/#command-6-delete-eks-cluster","title":"Command 6: Delete EKS Cluster","text":"<pre><code>eksctl delete cluster --name &lt;cluster-name&gt; --region &lt;region&gt;\n</code></pre> <p>Example:</p> <pre><code>eksctl delete cluster --name kube-app --region eu-central-1\n</code></pre>"},{"location":"devops/k8s/exercise01/","title":"Hands-on Exercises with Kubernetes \ud83d\udcbb\ud83d\ude80","text":"<p>Kubernetes is the go-to platform for container orchestration, and mastering it is essential for modern DevOps and cloud-native development. One fundamental aspect of Kubernetes is managing resources using either imperative or declarative methods. In this hands-on blog post, we'll dive into both approaches with practical examples and real-time use cases.</p>"},{"location":"devops/k8s/exercise01/#kubernetes-syntax","title":"Kubernetes syntax \ud83e\udde9","text":"<p>Let's dive into the basic Kubernetes syntax for four key fields: <code>apiVersion</code>, <code>kind</code>, <code>metadata</code>, and <code>spec</code>:</p> <ol> <li> <p>apiVersion:</p> <p>Purpose: The <code>apiVersion</code> field specifies the version of the Kubernetes API that the resource definition uses. It ensures compatibility between the resource and the Kubernetes cluster.</p> <p>Format: Typically, the <code>apiVersion</code> field consists of two parts separated by a forward slash (\"/\"). The first part is the API group, and the second part is the API version within that group.</p> <p>Examples: Common API versions include <code>v1</code> (core API for basic resources), <code>apps/v1</code> (for application-related resources), and <code>networking.k8s.io/v1</code> (for network policies).</p> <p>Usage: It tells the Kubernetes cluster how to interpret the resource definition. It's essential to specify the correct <code>apiVersion</code> to create resources successfully.</p> </li> <li> <p>kind:</p> <p>Purpose: The <code>kind</code> field defines the type of Kubernetes resource you are creating or managing. It determines the behavior and attributes of the resource.</p> <p>Examples: Common resource kinds include <code>Pod</code>, <code>Service</code>, <code>Deployment</code>, <code>ConfigMap</code>, <code>Secret</code>, <code>Namespace</code>, and more.</p> <p>Usage: The <code>kind</code> field ensures that Kubernetes understands the type of resource you intend to create or modify.</p> </li> <li> <p>metadata:</p> <p>Purpose: The <code>metadata</code> field provides information about the resource, such as its name, labels, and annotations.</p> <p>Format: It is a YAML dictionary that includes various metadata fields like <code>name</code>, <code>labels</code>, <code>annotations</code>, and more.</p> <p>Examples: Setting <code>name</code> specifies the name of the resource, while <code>labels</code> allow you to add key-value pairs for categorization and selection.</p> <p>Usage: Metadata helps in identifying, organizing, and managing resources within the Kubernetes cluster.</p> </li> <li> <p>spec:</p> <p>Purpose: The <code>spec</code> field describes the desired state of the resource. It specifies the configuration settings, properties, and behaviors that the resource should have.</p> <p>Format: It depends on the resource type and its attributes. For example, in a Pod, the <code>spec</code> field may include details about containers, volumes, and networking.</p> <p>Usage: The <code>spec</code> field is where you define how you want the resource to function within the Kubernetes cluster. It represents the resource's desired state.</p> </li> </ol> <p>Here's an example of how these fields come together in a Kubernetes YAML file for creating a Pod:</p> <pre><code># Kubernetes YAML file for a basic Pod\n\n#The 'apiVersion' field specifies the Kubernetes API version. \napiVersion: v1 # (1)!\n\n# The 'kind' field defines the type of Kubernetes resource.\nkind: Pod # (2)!\n\n# Metadata section provides information about the resource.\nmetadata:\n  # 'name' specifies the name of the Pod.\n  name: my-pod\n\n  # 'labels' are key-value pairs used to organize and select resources.\n  # Here, we label the Pod as 'app: my-app.'\n  labels:\n    app: my-app \n\n# The 'spec' section describes the desired state of the resource.\nspec:\n  # 'containers' is a list of containers to run within the Pod.\n  containers:\n    - name: my-container\n      # 'image' specifies the Docker image to use for this container.\n      # In this case, we use the 'nginx:latest' image.\n      image: nginx:latest\n\n      # 'ports' section defines the ports to be exposed by the container.\n      ports:\n        - \n          # 'containerPort' specifies the port number (80) on which the container listens.\n          containerPort: 80\n</code></pre> <ol> <li> <p>Common API versions include:</p> <ul> <li> <p>v1: Core resources like Pods, Services, ConfigMaps, and more.</p> </li> <li> <p>apps/v1: Resources like Deployments, StatefulSets, and DaemonSets.</p> </li> <li> <p>extensions/v1beta1: Older version for resources like Ingress.</p> </li> <li> <p>networking.k8s.io/v1: Resources like NetworkPolicies.</p> </li> </ul> </li> <li> <p>In this case, we are creating a Pod, but other common resource types include:</p> <ul> <li> <p>Deployment</p> </li> <li> <p>Service</p> </li> <li> <p>ConfigMap</p> </li> <li> <p>Secret</p> </li> <li> <p>PersistentVolume</p> </li> <li> <p>PersistentVolumeClaim</p> </li> <li> <p>StatefulSet</p> </li> <li> <p>Namespace</p> </li> </ul> </li> </ol> <p>In this example, we have:</p> <ul> <li><code>apiVersion: v1</code> indicating the core API version.</li> <li><code>kind: Pod</code> specifying that we are defining a Pod resource.</li> <li><code>metadata</code> providing metadata like the Pod's name and labels.</li> <li><code>spec</code> describing the desired state of the Pod, including container details and port configuration.</li> </ul> <p>Understanding and correctly using these basic Kubernetes syntax elements is crucial for defining and managing Kubernetes resources effectively.</p>"},{"location":"devops/k8s/exercise01/#imperative-declarative-methods","title":"Imperative \ud83c\udd9a Declarative Methods","text":""},{"location":"devops/k8s/exercise01/#imperative-method","title":"Imperative Method:","text":"<p>Example</p> <p>Imperative commands directly manipulate Kubernetes resources in real-time. They are ideal for quick, one-off tasks. Let's explore some imperative examples:</p> <p>1. Create a Namespace: Command Line<pre><code>kubectl create namespace my-namespace \n</code></pre></p> <p>2. Run a Pod: Command Line<pre><code>kubectl run my-pod --image=nginx --port=80\n</code></pre></p> <p>3. Scale a Deployment: Command Line<pre><code>kubectl scale deployment my-deployment --replicas=3 # (1)\n</code></pre></p> <ol> <li>\ud83d\udd90\ufe0f Modify the quantity of replicas here</li> </ol>"},{"location":"devops/k8s/exercise01/#real-time-use-case","title":"Real-time Use Case:","text":"<p>Suppose you have a sudden spike in traffic to your website, and you need to quickly scale up your application. Imperative commands allow you to react swiftly without worrying about YAML configuration files.</p>"},{"location":"devops/k8s/exercise01/#declarative-method","title":"Declarative Method:","text":"<p>Example</p> <p>Declarative Kubernetes management relies on YAML files to define the desired state of resources. Here are some declarative examples:</p> <p>1. Create a Namespace namespace.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n</code></pre></p> <p>2. Run a Pod pod.yaml<pre><code>apiVersion: v1  # Kubernetes API version \ud83c\udf10\nkind: Pod  # Type of resource: a Pod \ud83d\ude80\nmetadata:\n  name: my-pod  # Name of the Pod \ud83c\udff7\ufe0f\nspec:\n  containers:\n    - name: nginx-container  # Container name \ud83d\udc33\n      image: nginx  # Docker image used \ud83d\udce6\n      ports:\n        - containerPort: 80  # Container's port configuration \ud83d\udd0c\n</code></pre></p> <p>3. Scale a Deployment deployment.yaml<pre><code>apiVersion: apps/v1  # Kubernetes API version for Deployments \ud83c\udf10\nkind: Deployment  # Type of resource: a Deployment \ud83d\ude80\nmetadata:\n  name: my-deployment  # Name of the Deployment \ud83c\udff7\ufe0f\nspec:\n  replicas: 3  # Number of desired replicas of the app \ud83d\udd04\n  selector:\n    matchLabels:\n      app: my-app  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  template:\n    metadata:\n      labels:\n        app: my-app  # Labels for Pods created by this Deployment \ud83c\udff7\ufe0f\n    spec:\n      containers:\n        - name: my-container  # Container name \ud83d\udc33\n          image: my-image  # Docker image used for the container \ud83d\udce6\n</code></pre></p>"},{"location":"devops/k8s/exercise01/#real-time-use-case_1","title":"Real-time Use Case:","text":"<p>Imagine you are setting up a Continuous Integration/Continuous Deployment (CI/CD) pipeline. Declarative manifests are the preferred choice as they clearly define the desired state of your resources, making it easier to track changes and collaborate with your team.</p>"},{"location":"devops/k8s/exercise01/#converting-imperative-kubernetes-commands-to-declarative-yaml-files","title":"Converting Imperative Kubernetes Commands to Declarative YAML Files","text":"<p>Command Line<pre><code>kubectl create deployment my-nginx-pod --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n</code></pre> This command converts an imperative action (creating a pod) into a declarative file using <code>kubectl</code>.</p>"},{"location":"devops/k8s/exercise01/#sample-pod-file-imperative-vs-declarative","title":"Sample Pod File - Imperative vs. Declarative:","text":"<p>Let's create a sample Pod file to run an Nginx container using both imperative and declarative methods:</p> <p>**Imperative ** Command Line<pre><code>kubectl run my-nginx-pod --image=nginx --port=80 \n</code></pre></p> <p>Declarative pod.yaml<pre><code>apiVersion: v1  # Kubernetes API version \ud83c\udf10\nkind: Pod  # Type of resource: a Pod \ud83d\ude80\nmetadata:\n  name: my-nginx-pod  # Name of the Pod \ud83c\udff7\ufe0f\nspec:\n  containers:\n    - name: nginx-container  # Container name \ud83d\udc33\n      image: nginx  # Docker image used \ud83d\udce6\n      ports:\n        - containerPort: 80  # Port configuration for the container \ud83d\udd0c\n</code></pre></p>"},{"location":"devops/k8s/exercise01/#sample-deployment-and-service-files-imperative-vs-declarative","title":"Sample Deployment and Service Files - Imperative vs. Declarative:","text":"<p>Now, let's create deployment and service files for deploying Nginx with a NodePort service using both methods:</p> <p>Imperative (command-line): <pre><code>kubectl create deployment nginx-deployment --image=nginx\nkubectl expose deployment nginx-deployment --port=80 --type=NodePort\n</code></pre></p> <p>Declarative deployment.yaml<pre><code>apiVersion: apps/v1  # Kubernetes API version for Deployments \ud83c\udf10\nkind: Deployment  # Type of resource: a Deployment \ud83d\ude80\nmetadata:\n  name: nginx-deployment  # Name of the Deployment \ud83c\udff7\ufe0f\nspec:\n  replicas: 3  # Number of desired replicas of the app \ud83d\udd04\n  selector:\n    matchLabels:\n      app: nginx  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  template:\n    metadata:\n      labels:\n        app: nginx  # Labels for Pods created by this Deployment \ud83c\udff7\ufe0f\n    spec:\n      containers:\n        - name: nginx-container  # Container name \ud83d\udc33\n          image: nginx  # Docker image used for the container \ud83d\udce6\n</code></pre></p> <p>Declarative service.yaml<pre><code>apiVersion: v1  # Kubernetes API version for Services \ud83c\udf10\nkind: Service  # Type of resource: a Service \ud83d\ude80\nmetadata:\n  name: nginx-service  # Name of the Service \ud83c\udff7\ufe0f\nspec:\n  selector:\n    app: nginx  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  ports:\n    - protocol: TCP  # Protocol used for the port \ud83d\udd0c\n      port: 80  # Port exposed by the Service \ud83c\udf10\n      targetPort: 80  # Target port on Pods \ud83c\udfaf\n  type: NodePort  # Type of Service: NodePort for external access \ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfe2\n</code></pre></p>"},{"location":"devops/k8s/exercise01/#deploying-grafana-with-loadbalancer","title":"Deploying Grafana with LoadBalancer:","text":"<p>To further demonstrate declarative YAML, let's create a deployment and service file to deploy Grafana with a NodePort service. You can customize the Grafana deployment based on your requirements.</p> deployment.yaml<pre><code>apiVersion: apps/v1  # Kubernetes API version for Deployments \ud83c\udf10\nkind: Deployment  # Type of resource: a Deployment \ud83d\ude80\nmetadata:\n  name: grafana-deployment  # Name of the Deployment \ud83c\udff7\ufe0f\nspec:\n  replicas: 1  # Number of desired replicas of the app \ud83d\udd04\n  selector:\n    matchLabels:\n      app: grafana  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  template:\n    metadata:\n      labels:\n        app: grafana  # Labels for Pods created by this Deployment \ud83c\udff7\ufe0f\n    spec:\n      containers:\n        - name: grafana-container  # Container name \ud83d\udc33\n          image: grafana/grafana:latest  # Docker image used for the container \ud83d\udce6\n          ports:\n            - containerPort: 3000  # Port configuration for the container \ud83d\udd0c\n</code></pre> service.yaml<pre><code>apiVersion: v1  # Kubernetes API version for Services \ud83c\udf10\nkind: Service  # Type of resource: a Service \ud83d\ude80\nmetadata:\n  name: grafana-service  # Name of the Service \ud83c\udff7\ufe0f\nspec:\n  selector:\n    app: grafana  # Label selector for matching Pods \ud83c\udff7\ufe0f\n  ports:\n    - protocol: TCP  # Protocol used for the port \ud83d\udd0c\n      port: 80  # Port exposed by the Service \ud83c\udf10\n      targetPort: 3000  # Target port on Pods \ud83c\udfaf\n  type: LoadBalancer  # Type of Service: LoadBalancer link for external access \ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfe2\n</code></pre> <p>In this example, we're deploying Grafana using declarative YAML files, allowing for a well-defined and repeatable process.</p>"},{"location":"devops/k8s/exercise01/#website-deployment","title":"Website Deployment","text":"deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wavecafe  # \ud83d\ude80 Name of the Deployment\nspec:\n  replicas: 1  # \ud83d\udeb6 Number of desired replicas\n  selector:\n    matchLabels:\n      app: cafe  # \ud83c\udff7\ufe0f Selector to match pods with the label \"app: cafe\"\n  template:\n    metadata:\n      labels:\n        app: cafe  # \ud83c\udff7\ufe0f Labels applied to pods created by this template\n    spec:\n      containers:\n        - name: my-app-container  # \ud83d\udce6 Name of the container\n          image: saitejairrinki/wavecafe:v1  # \ud83d\udc33 Docker image to use\n          ports:\n            - name: cafe-port  # \ud83c\udf10 Name of the port\n              containerPort: 80  # \ud83d\udeaa Port that the container listens on\n</code></pre> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: wave-cafe  # \u2615 Name of the Service\nspec:\n  selector:\n    app: cafe  # \ud83c\udff7\ufe0f Select pods with the label \"app: cafe\"\n  ports:\n    - protocol: TCP  # \ud83c\udf10 Protocol for the port\n      port: 80  # \ud83d\udeaa Port on the Service\n      targetPort: cafe-port  # \ud83d\ude80 Port on the pods to forward traffic to\n  type: NodePort  # \ud83c\udfe2 Expose the Service as a NodePort\n</code></pre>"},{"location":"devops/k8s/intro/","title":"Kubernetes","text":"<p>Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.</p> <p>In the digital realm, Kubernetes serves as a versatile \ud83c\udf10 platform. It's your trusted partner for managing containerized workloads and services, ensuring they run smoothly across diverse environments. It offers the power of declarative configuration and automation, simplifying the complex.</p>"},{"location":"devops/k8s/intro/#lets-explore-the-top-10-reasons-why-kubernetes-has-captured-the-hearts-of-developers-and-organizations-worldwide","title":"Let's explore the top 10 reasons why Kubernetes has captured the hearts of developers and organizations worldwide \ud83d\udea2","text":"<p>\ud83c\udf0d Largest Open Source Project: Kubernetes holds the title of being the largest open-source project globally. Its expansive reach means you can trust its capabilities.</p> <p>\ud83e\udd17 Great Community Support: The Kubernetes community is a treasure trove of knowledge and support. It's like having an army of experts at your disposal.</p> <p>\ud83d\udce6 Robust Container Deployment: Kubernetes excels at container management, ensuring your applications run smoothly, just like a well-oiled machine.</p> <p>\ud83d\udcc2 Effective Persistent Storage: Storage management is a breeze with Kubernetes, allowing your data to stay safe and accessible.</p> <p>\u2601\ufe0f Multi-Cloud Support (Hybrid Cloud): Whether your applications reside in the cloud, on-premises, or both, Kubernetes seamlessly bridges the gap.</p> <p>\ud83d\udc93 Container Health Monitoring: It keeps a vigilant eye on your containers, ensuring they're always in top-notch health.</p> <p>\ud83d\udda5\ufe0f Compute Resource Management: Kubernetes optimizes resource allocation, ensuring efficient use of computing power.</p> <p>\u2696\ufe0f Auto-Scaling Feature Support: When traffic surges, Kubernetes automatically scales your applications, maintaining responsiveness.</p> <p>\ud83c\udfed Real-world Use Cases: Kubernetes shines in real-world scenarios, from e-commerce to finance, demonstrating its versatility.</p> <p>\ud83c\udf10 High Availability by Cluster Federation: Ensuring your applications are always available, Kubernetes employs cluster federation.</p>"},{"location":"devops/k8s/intro/#types-few-types-of-kubernetes-clusters-for-various-situations","title":"Types Few types of Kubernetes clusters for various situations:","text":"<p>\ud83d\udc68\u200d\ud83d\udcbb Single-node Cluster (Minikube): Great for trying things out on your own computer. It's like a mini playground for Kubernetes.</p> <p>\ud83c\udf10 Multi-node Cluster (Kubeadm, KOPS): Perfect for running your apps in production. It can handle multiple containers and keeps everything running smoothly.</p> <p>\u2601\ufe0f Managed Kubernetes Services (EKS, GKE, AKS): If you want Kubernetes without the hassle of managing the servers, cloud providers like Amazon, Google, and Azure offer managed services.</p> <p>\ud83d\udee0\ufe0f Development and Staging Clusters: Think of these as testing areas. Developers use them to make sure everything works before it goes live.</p> <p>\ud83c\udfe2 On-Premises Clusters: For businesses that need to keep everything in their own data centers for security or compliance reasons.</p> <p>\u2601\ufe0f\ud83c\udfe2 Hybrid Clusters: If you want to use both your own servers and the cloud, hybrid clusters connect them seamlessly.</p> <p>So, Kubernetes is like your trusty orchestra conductor, making sure all your containerized applications play in harmony, no matter where they are. \ud83d\ude80\ud83c\udfb5</p>"},{"location":"devops/k8s/k3s/","title":"Creating Kubernetes Cluster with k3s","text":"<p>K3s is a lightweight Kubernetes distribution that Rancher Labs, which is a fully certified Kubernetes offering by CNCF. In K3s, we see that the memory footprint or binary which contains the components to run a cluster is small. It means that K3s are small in size.</p>"},{"location":"devops/k8s/k3s/#setup","title":"Setup","text":"<pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre> <p><pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <pre><code>sudo chmod 644 $KUBECONFIG\n</code></pre></p>"},{"location":"devops/k8s/k8sdashboard/","title":"Deploy the latest Kubernetes dashboard","text":"<p>Once you\u2019ve set up your Kubernetes cluster or if you already had one running, we can get started.</p> <p>The first thing to know about the web UI is that it can only be accessed using the localhost address on the machine it runs on. This means we need to have an SSH tunnel to the server. For most OS, you can create an SSH tunnel using this command. Replace the  and  with the relevant details to your Kubernetes cluster. <pre><code>ssh -L localhost:8001:127.0.0.1:8001 &lt;user&gt;@&lt;master_public_IP&gt;\n</code></pre> After you\u2019ve logged in, you can deploy the dashboard itself with the following single command. <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre> If your cluster is working correctly, you should see an output confirming the creation of a bunch of Kubernetes components like in the example below. Output <pre><code>namespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre> <p>Afterward, you should have two new pods running on your cluster.</p> <pre><code>kubectl get pods -A\n</code></pre> Output <pre><code>...\nkubernetes-dashboard   dashboard-metrics-scraper-78ujd94gf7-ff6h8   1/1     Running   0          30m\nkubernetes-dashboard   kubernetes-dashboard-g6hujkirf7-df65g        1/1     Running   0          30m\n</code></pre> <p>You can then continue ahead with creating the required user accounts.</p>"},{"location":"devops/k8s/k8sdashboard/#creating-admin-user","title":"Creating Admin user","text":"<p>The Kubernetes dashboard supports a few ways to manage access control. In this example, we\u2019ll be creating an admin user account with full privileges to modify the cluster and use tokens.</p> <p>Start by making a new directory for the dashboard configuration files. <pre><code>mkdir ~/dashboard &amp;&amp; cd ~/dashboard\n</code></pre> Create the following configuration and save it as a dashboard-admin.yaml file. Note that indentation matters in the YAML files which should use two spaces in a regular text editor.</p> <pre><code>vim dashboard-admin.yaml\n</code></pre> <p><pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n</code></pre> Once set, save the file and exit the editor.</p> <p>Then deploy the admin user role with the next command.</p> <p><pre><code>kubectl apply -f dashboard-admin.yaml\n</code></pre> You should see a service account and a cluster role binding created.</p> Output <pre><code>serviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\n</code></pre> <p>Using this method doesn\u2019t require setting up or memorising passwords, instead, accessing the dashboard will require a token.</p> <p>Get the admin token using the command below. <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre> You\u2019ll then see an output of a long string of seemingly random characters like in the example below.</p> Output <p>eyJhbGciOiJSUzI1NiIsImtpZCI6Ilk2eEd2QjJMVkhIRWNfN2xTMlA5N2RNVlR5N0o1REFET0dp dkRmel90aWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlc y5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1Y mVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuL XEyZGJzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZ SI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb 3VudC51aWQiOiI1ODI5OTUxMS1hN2ZlLTQzZTQtODk3MC0yMjllOTM1YmExNDkiLCJzdWIiOiJze XN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.GcUs MMx4GnSV1hxQv01zX1nxXMZdKO7tU2OCu0TbJpPhJ9NhEidttOw5ENRosx7EqiffD3zdLDptS22F gnDqRDW8OIpVZH2oQbR153EyP_l7ct9_kQVv1vFCL3fAmdrUwY5p1-YMC41OUYORy1JPo5wkpXrW OytnsfWUbZBF475Wd3Gq3WdBHMTY4w3FarlJsvk76WgalnCtec4AVsEGxM0hS0LgQ-cGug7iGbmf cY7odZDaz5lmxAflpE5S4m-AwsTvT42ENh_bq8PS7FsMd8mK9nELyQu_a-yocYUggju_m-BxLjgc 2cLh5WzVbTH_ztW7COlKWvSVg7yhjikrew</p> <p>The token is created each time the dashboard is deployed and is required to log into the dashboard. Note that the token will change if the dashboard is stopped and redeployed.</p>"},{"location":"devops/k8s/k8sdashboard/#creating-read-only-user","title":"Creating Read-Only user","text":"<p>If you wish to provide access to your Kubernetes dashboard, for example, for demonstrative purposes, you can create a read-only view for the cluster.</p> <p>Similarly to the admin account, save the following configuration in dashboard-read-only.yaml <pre><code>vim dashboard-read-only.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: read-only-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n  name: read-only-clusterrole\n  namespace: default\nrules:\n- apiGroups:\n  - \"\"\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources: [\"*\"]\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-only-binding\nroleRef:\n  kind: ClusterRole\n  name: read-only-clusterrole\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: read-only-user\n  namespace: kubernetes-dashboard\n</code></pre></p> <p>Once set, save the file and exit the editor.</p> <p>Then deploy the read-only user account with the command below.</p> <p><pre><code>kubectl apply -f dashboard-read-only.yaml\n</code></pre> To allow users to log in via the read-only account, you\u2019ll need to provide a token that can be fetched using the next command. <pre><code>kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n</code></pre> The toke will be a long series of characters and unique to the dashboard currently running.</p>"},{"location":"devops/k8s/k8sdashboard/#accessing-the-dashboard","title":"Accessing the dashboard","text":"<p>We\u2019ve now deployed the dashboard and created user accounts for it. Next, we can get started managing the Kubernetes cluster itself.</p> <p>However, before we can log in to the dashboard, it needs to be made available by creating a proxy service on the localhost. Run the next command on your Kubernetes cluster. <pre><code>kubectl proxy\n</code></pre> This will start the server at 127.0.0.1:8001 as shown by the output.</p> Output <p>Starting to serve on 127.0.0.1:8001</p> <p>Now, assuming that we have already established an SSH tunnel binding to the localhost port 8001 at both ends, open a browser to the link below.</p> <p>Link</p> <p>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</p> <p>If everything is running correctly, you should see the dashboard login window.</p> <p></p> <p>Select the token authentication method and copy your admin token into the field below. Then click the Sign in button.</p> <p>You will then be greeted by the overview of your Kubernetes cluster.</p> <p></p> <p>While signed in as an admin, you can deploy new pods and services quickly and easily by clicking the plus icon at the top right corner of the dashboard.</p> <p></p> <p>Then either copy in any configuration file you wish, select the file directly from your machine, or create a new configuration from a form.</p>"},{"location":"devops/k8s/kops/","title":"Creating Kubernetes Cluster with KOPS","text":"<p>Install AWS CLI  <pre><code>apt update &amp;&amp; apt install awscli -y\n</code></pre> Configure AWS CLI with IAM user Credentials with a specific Region  <pre><code>aws configure\n</code></pre></p> <p>Note</p> <p>If you are using AWS Instance better to use IAM Role than Creating User with Access-key</p> <p>Check Whether AWS CLI Commands Working or not  <pre><code>aws s3 ls\n</code></pre> Generate SSH Keys <pre><code>ssh-keygen\n</code></pre></p>"},{"location":"devops/k8s/kops/#install-kubectl-binary-with-curl-on-linux","title":"Install kubectl binary with curl on Linux","text":"<p>Download the latest release with the command: <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n</code></pre> Install kubectl <pre><code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> <pre><code>kubectl version --client\n</code></pre></p>"},{"location":"devops/k8s/kops/#installing-kubernetes-with-kops","title":"Installing Kubernetes with kops","text":"<p>Download the latest release with the command:</p> <p><pre><code>curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64\n</code></pre> <pre><code>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64\n</code></pre></p> <p>Make the kops binary executable <pre><code>chmod +x kops-linux-amd64\n</code></pre> Move the kops binary into your PATH. <pre><code>sudo mv kops-linux-amd64 /usr/local/bin/kops\n</code></pre> <pre><code>kops\n</code></pre></p>"},{"location":"devops/k8s/kops/#creating-k8s-cluster-with-kops","title":"Creating K8s Cluster with KOPS","text":""},{"location":"devops/k8s/kops/#kops-commands-to-setup-k8s-cluster-","title":"Kops commands to setup k8s cluster:-","text":"<p>Creating S3 Bucket for Kubernetes Cluster <pre><code>aws s3 mb s3:// &lt;bucket name&gt;\n</code></pre> <pre><code>kops create cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --zones=ap-south-1a,ap-south-1b --node-count=2 --node-size=t3.medium --master-size=t3.medium --dns-zone=saiteja.irrinki.xyz --node-volume-size=8 --master-volume-size=8\n</code></pre> It will create a configuration of kops <pre><code>kops update cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --yes --admin\n</code></pre> It will create kops data in the S3 bucket. It starts creating a cluster &amp; it takes 10 mins <pre><code>kops validate cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt;\n</code></pre> It shows ur cluster is ready</p> <p>To Delete Cluster</p> <pre><code>kops delete cluster --name=saiteja.irrinki.xyz --state=s3://&lt;s3 bucket&gt; --yes \n</code></pre>"},{"location":"devops/k8s/kubeadm/","title":"Creating Kubernetes Cluster with kubeadm","text":"For this activity, deploy minimum 2 AWS instances with the Security groups <p>All traffic is enabled between the instances </p> Node Name Instance Details Resources Master Node -t2.medium 4GB Ram , 2 CPU Worker Node -t2.micro 1GB Ram , 1 CPU"},{"location":"devops/k8s/kubeadm/#preparing-the-master-and-worker-nodes-for-kubeadm","title":"Preparing the Master and Worker nodes for kubeadm","text":"<p>Execute the below Commands on both Master &amp; Worker Nodes</p> <p>Change the hostname for nodes as master &amp; worker</p>"},{"location":"devops/k8s/kubeadm/#installing-docker","title":"Installing Docker:","text":"<p>Add the Docker repository key and Docker repository. <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\nadd-apt-repository \\\n\"deb https://download.docker.com/linux/$(. /etc/os-release; echo \"$ID\") \\\n$(lsb_release -cs) \\\nstable\"\n</code></pre> Update the list of packages and install the docker</p> <p><pre><code>apt-get update\n</code></pre> <pre><code>apt-get install -y docker-ce\n</code></pre> <pre><code>echo '{\"exec-opts\": [\"native.cgroupdriver=systemd\"]}' | sudo tee /etc/docker/daemon.json\n</code></pre> <pre><code>systemctl daemon-reload\n</code></pre> <pre><code>systemctl restart docker\n</code></pre> <pre><code>systemctl enable docker\n</code></pre></p>"},{"location":"devops/k8s/kubeadm/#installing-kubeadm-kublet-and-kubectl","title":"Installing kubeadm, kublet, and kubectl:","text":"<p>Add the Google repository key and Google repository</p> <p><pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n</code></pre> <pre><code>vim /etc/apt/sources.list.d/kubernetes.list\n</code></pre> <pre><code>deb http://apt.kubernetes.io kubernetes-xenial main\n</code></pre> Update the list of packages. And install kubelet, kubeadm, and kubectl.</p> <p><pre><code>apt-get update \n</code></pre> <pre><code>apt-get install kubelet kubeadm kubectl -y\n</code></pre> Disable the swap.</p> <pre><code>swapoff -a\n</code></pre> <p>Setup Master Node to Connect with Worker Nodes</p> <p><pre><code>sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> <pre><code> echo NODENAME=$(hostname -s)\n</code></pre> <pre><code>kubeadm init --apiserver-advertise-address=172.31.85.246  --apiserver-cert-extra-sans=172.31.85.246  --pod-network-cidr=10.0.0.0/16 \n</code></pre> Now Create a Directory Name .kube in the master node home directory <pre><code>mkdir .kube\n</code></pre> Copy the Default conf file to the .kube directory <pre><code>cp /etc/kubernetes/admin.conf .kube/config\n</code></pre> Replace the Ip address with your Instance Private Ip Address and set pod Network </p> <p>After Executing kubeadm init command you will get one command as output that need to execute on worker nodes</p> <p><pre><code>kubeadm join 172.31.85.246:6443 --token n6v08z.53b057pfwnj8mq10         --discovery-token-ca-cert-hash sha256:9e8a38ddfc46c41ecb3317db9fc2145f70bddbdd2c6b8091fbdbab18e1dbcb19 \n</code></pre> Configuring the Calico in Master Node <pre><code>kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre> Now check Kubectl commands <pre><code>kubectl get node\n</code></pre> Now test the cluster </p> <p><pre><code>kubectl run pod --image=nginx\n</code></pre> <pre><code>kubectl get pod\n</code></pre> <pre><code>kubectl get pod -o wide\n</code></pre></p>"},{"location":"miscellaneous/activedir/","title":"MISCELLANEOUS","text":"<p>To Setup Active Directory Domain Service in Windows Server Click here</p>"},{"location":"miscellaneous/bluegreen_deployment/","title":"Blue-Green Deployment","text":"<p>Blue-Green Deployment is a deployment strategy that involves maintaining two identical environments: a \"blue\" environment representing the current production version and a \"green\" environment representing the new version or update. The deployment process follows these steps:</p> <p></p> <ol> <li> <p>Initially, the blue environment is live and serving user traffic.</p> </li> <li> <p>The new version or update is deployed and tested in the green environment, which is isolated from the live production environment.</p> </li> <li> <p>The green environment undergoes thorough testing and validation to ensure that the new version functions as expected and meets quality standards.</p> </li> <li> <p>Once the green environment is deemed stable and ready, a traffic switch is performed, redirecting user traffic from the blue environment to the green environment.</p> </li> <li> <p>Now, the green environment becomes the live production environment, serving user traffic, while the blue environment is kept as a backup.</p> </li> <li> <p>If any issues arise in the green environment, a rollback can be quickly performed by switching the traffic back to the blue environment.</p> </li> </ol>"},{"location":"miscellaneous/bluegreen_deployment/#the-key-benefits-of-blue-green-deployment-are","title":"The key benefits of blue-green deployment are:","text":"<ol> <li> <p>Reduced downtime and risk: By maintaining separate environments, the switch between blue and green can be done seamlessly, minimizing downtime and reducing the risk of potential issues affecting users.</p> </li> <li> <p>Quick rollback: If any problems are encountered in the green environment, the switch can be reverted instantly by directing traffic back to the blue environment.</p> </li> <li> <p>Controlled release: Blue-green deployment allows for thorough testing and validation of the new version in a separate environment, ensuring that any issues are identified before impacting users.</p> </li> <li> <p>Easy rollbacks: In case of issues, the blue environment is readily available as a fallback option, providing a safety net for the production environment.</p> </li> </ol> <p>Conclusion</p> <p>Overall, blue-green deployment provides a controlled and low-risk approach to deploying updates, promoting stability, availability, and fast recovery in case of failures.</p>"},{"location":"miscellaneous/canary/","title":"Canary Deployment","text":"<p>Canary deployment is a deployment strategy that involves gradually rolling out a new version or update to a subset of users or servers, allowing for testing and monitoring before fully deploying to the entire infrastructure. In canary mode, the deployment process follows these steps:</p> <p></p> <ol> <li> <p>A small portion of the user traffic or a subset of servers, typically referred to as the \"canary group,\" is selected to receive the new version or update.</p> </li> <li> <p>The new version is deployed to the canary group while the rest of the infrastructure, including the major infrastructure and all other components, continues to run the current stable version.</p> </li> <li> <p>The canary group starts receiving a fraction of the user traffic, and their behavior and performance are closely monitored and analyzed.</p> </li> <li> <p>If the new version performs well without any issues or negative impact on the canary group, the deployment is gradually expanded to include a larger portion of the infrastructure, including the major infrastructure and ultimately all components.</p> </li> <li> <p>Monitoring and metrics are continuously analyzed during the rollout to identify any anomalies, errors, or performance degradation across the entire infrastructure.</p> </li> <li> <p>If issues are detected, the rollout can be quickly halted or rolled back, preventing widespread impact and minimizing disruption to the major infrastructure and all other components.</p> </li> <li> <p>Once the new version has been successfully rolled out to the entire infrastructure, encompassing the major infrastructure and all components, and is proven to be stable, the canary deployment is completed.</p> </li> </ol>"},{"location":"miscellaneous/canary/#the-key-benefits-of-canary-deployments-including-major-infrastructure-and-all-infrastructure-are","title":"The key benefits of canary deployments, including major infrastructure and all infrastructure, are:","text":"<ol> <li> <p>Risk mitigation: By initially deploying the new version to a small subset of users or servers, any issues or bugs can be detected early, reducing the risk of a widespread negative impact on the major infrastructure and all components.</p> </li> <li> <p>Controlled rollout: Canary deployments allow for controlled and gradual rollouts, providing an opportunity to monitor the performance, stability, and user experience of the new version across the major infrastructure and all other components before a full deployment.</p> </li> <li> <p>Continuous monitoring: During the canary phase, metrics and monitoring systems help identify any abnormalities, errors, or performance issues across the major infrastructure and all components, enabling timely actions such as rollback or further optimization.</p> </li> <li> <p>Faster feedback loop: Canary deployments facilitate faster feedback and validation of the new version, allowing teams to make necessary adjustments or improvements based on real-world usage and feedback across the major infrastructure and all components.</p> </li> </ol> <p>Conclusion</p> <p>Overall, canary deployments provide a controlled and incremental approach to deploying updates across the major infrastructure and all components, enabling teams to validate the new version in a controlled environment and ensure a smooth transition to the updated system.</p>"},{"location":"miscellaneous/ldap/","title":"Install and Configure OpenLDAP Server on Ubuntu","text":""},{"location":"miscellaneous/ldap/#set-hostname-for-the-ubuntu-server","title":"Set hostname for the Ubuntu server","text":"<p><pre><code>sudo hostnamectl set-hostname ldap.k8sengineers.com\n</code></pre> Add the IP and FQDN to file /etc/hosts <pre><code>vim /etc/hosts\n</code></pre></p> <pre><code>192.168.0.151 ldap.k8sengineers.com \n</code></pre> <p>Note</p> <p>Replace ldap.k8sengineers.com with your correct hostname/valid domain name.</p>"},{"location":"miscellaneous/ldap/#install-openldap-server-on-ubuntu","title":"Install OpenLDAP Server on Ubuntu","text":"<p><pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt -y install slapd ldap-utils\n</code></pre> During the installation, you\u2019ll be prompted to set LDAP admin password, provide your desired password, then press  <p></p> <p>Confirm the password and continue the installation by selecting  <p>You can confirm that your installation was successful using the commandslapcat to output SLAPD database contents.</p> <p><pre><code>sudo slapcat\n</code></pre> <pre><code>dn: dc=k8sengineers,dc=com\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: k8sengineers.com\ndc: k8sengineers\nstructuralObjectClass: organization\nentryUUID: 0139eef2-f01c-103b-8cf7-cb31a244bcdd\ncreatorsName: cn=admin,dc=k8sengineers,dc=com\ncreateTimestamp: 20211213045057Z\nentryCSN: 20211213045057.125264Z#000000#000#000000\nmodifiersName: cn=admin,dc=k8sengineers,dc=com\nmodifyTimestamp: 20211213045057Z\n\ndn: cn=admin,dc=k8sengineers,dc=com\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9aXpJOXpJT2tTUUNIUzkrVzJpUVQ5L1M4WVRzZjMvUU4=\nstructuralObjectClass: organizationalRole\nentryUUID: 013a5ebe-f01c-103b-8cf8-cb31a244bcdd\ncreatorsName: cn=admin,dc=k8sengineers,dc=com\ncreateTimestamp: 20211213045057Z\nentryCSN: 20211213045057.128175Z#000000#000#000000\nmodifiersName: cn=admin,dc=k8sengineers,dc=com\nmodifyTimestamp: 20211213045057Z  \n</code></pre></p>"},{"location":"miscellaneous/ldap/#add-base-dn-for-users-and-groups","title":"Add base dn for Users and Groups","text":"<p>The next step is adding a base DN for users and groups. Create a file named basedn.ldif with the below contents:</p> <p><pre><code>vim basedn.ldif\n</code></pre> <pre><code>dn: ou=people,dc=k8sengineers,dc=com\nobjectClass: organizationalUnit\nou: people\n\ndn: ou=groups,dc=k8sengineers,dc=com\nobjectClass: organizationalUnit\nou: groups\n</code></pre> Replace k8sengineers and com with your correct domain components.</p> <p>Now add the file by running the command:</p> <p><pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f basedn.ldif\n</code></pre> Enter LDAP Password:</p> Output <pre><code>adding new entry \"ou=people,dc=k8sengineers,dc=com\"\nadding new entry \"ou=groups,dc=k8sengineers,dc=com\"\n</code></pre>"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups","title":"Add User Accounts and Groups","text":"<p>Generate a password for the user account to add.</p> <p><pre><code>sudo slappasswd\n</code></pre> Set the Password</p> Output <pre><code>{SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx\n</code></pre> <p>Create a ldif file for adding users.</p> <p><pre><code>vim ldapusers.ldif\n</code></pre> <pre><code>dn: uid=admin,ou=people,dc=k8sengineers,dc=com\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: shadowAccount\ncn: admin\nsn: Wiz\nuserPassword: {SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx\nloginShell: /bin/bash\nuidNumber: 2000\ngidNumber: 2000\nhomeDirectory: /home/admin\n</code></pre></p> <ul> <li>Replace admin with the username to add</li> <li>dc=k8sengineers,dc=com with your correct domain values.</li> <li>cn &amp; sn with your Username Values</li> <li>{SSHA}ls2kqhrLiMuKg9w3JcMq4y1Cefi0amAx with your hashed password</li> </ul> <p>When done with the edit, add an account by running.</p> <pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f ldapusers.ldif \n</code></pre> Output <pre><code>adding new entry \"uid=admin,ou=people,dc=k8sengineers,dc=com\"\n</code></pre> <p>Do the same for the group. Create a ldif file:</p> <p><pre><code>vim ldapgroups.ldif\n</code></pre> <pre><code>dn: cn=admin,ou=groups,dc=k8sengineers,dc=com\nobjectClass: posixGroup\ncn: admin\ngidNumber: 2000\nmemberUid: admin\n</code></pre> Add group:</p> <pre><code>ldapadd -x -D cn=admin,dc=k8sengineers,dc=com -W -f ldapgroups.ldif\n</code></pre> Output <pre><code>adding new entry \"cn=admin,ou=groups,dc=k8sengineers,dc=com\"\n</code></pre> <p>You can combine the two into single file.</p>"},{"location":"miscellaneous/ldap/#install-ldap-account-manager-on-ubuntu","title":"Install LDAP Account Manager on Ubuntu","text":"<p>Install Apache Web server &amp; PHP <pre><code>sudo apt -y install apache2 php php-cgi libapache2-mod-php php-mbstring php-common php-pear\n</code></pre></p> <ul> <li> <p>For Ubuntu 22.04:sudo a2enconf php8.0-cgi</p> </li> <li> <p>For Ubuntu 20.04:sudo a2enconf php7.4-cgi</p> </li> <li> <p>For Ubuntu 18.04: sudo a2enconf php7.2-cgi</p> </li> </ul> <p>Here I'm using Ubuntu 20.04:</p> <p><pre><code>sudo a2enconf php7.4-cgi\n</code></pre> <pre><code>sudo systemctl reload apache2\n</code></pre> Install LDAP Account Manager <pre><code>sudo apt -y install ldap-account-manager\n</code></pre> Configure LDAP Account Manager <pre><code>http://&lt; IP address &gt;/lam\n</code></pre> </p> <p>The LDAP Account Manager Login form will be shown. We need to set our LDAP server profile by clicking on[LAM configuration] at the upper right corner. </p> <p>Then click on, Edit server profiles</p> <p>This will ask you for the LAM Profile name Password</p> <p>Note</p> <p>The default password is lam</p> <p> </p> <p>The first thing to change is Profile Password, this is at the end of the General Settings page.</p> <p></p> <p>Next is to set the LDAP Server address and Tree suffix. Mine looks like below, you need to use your Domain components as set in the server hostname.</p> <p></p> <p>Set Dashboard login by specifying the admin user account and domain components under the \u201cSecurity settings\u201d section</p> <p></p> <p>Switch to the \u201cAccount types\u201d page and set Active account types LDAP suffix and List attributes.</p> <p></p>"},{"location":"miscellaneous/ldap/#add-user-accounts-and-groups-with-ldap-account-manager","title":"Add user accounts and groups with LDAP Account Manager","text":"<p>Log in with the account admin to the LAM dashboard to start managing user accounts and groups.</p> <p></p> <p>Add User Group</p> <p>Give the group a name, optional group ID, and description. </p> <p>Add User Accounts Once you have the groups for user accounts to be added, click on Users &gt; New user to add a new user account to your LDAP server. You have three sections for user management:</p> <ul> <li>Personal \u2013 This contains the user\u2019s personal information like the first name, last name, email, phone, department, address e.t.c</li> </ul> <p></p>"},{"location":"miscellaneous/ldap/#configure-your-ubuntu-220420041804-as-ldap-client","title":"Configure your Ubuntu 22.04|20.04|18.04 as LDAP Client","text":""},{"location":"miscellaneous/ldap/#the-last-step-is-to-configure-the-systems-in-your-network-to-authenticate-against-the-ldap-server-weve-just-configured","title":"The last step is to configure the systems in your network to authenticate against the LDAP server we\u2019ve just configured:","text":"<p>Add LDAP server address to /etc/hosts file if you don\u2019t have an active DNS server in your network. <pre><code>sudo vim /etc/hosts\n</code></pre> <pre><code>192.168.18.50 ldap.k8sengineers.com\n</code></pre> Install LDAP client utilities on your Ubuntu system:</p> <p><pre><code>sudo apt -y install libnss-ldap libpam-ldap ldap-utils\n</code></pre> Begin configuring the settings to look like below * Set LDAP URI- This can be IP address or hostname </p> <ul> <li> <p>Set a Distinguished name for the search base </p> </li> <li> <p>Select LDAP version 3</p> </li> <li>Select Yes for Make local root Database admin</li> <li>Answer No for Does the LDAP database require login?</li> <li>Set LDAP account for root, something like cn=admin,cd=k8sengineers,cn=com</li> <li>Provide LDAP root account Password</li> </ul> <p>After the installation, edit /etc/nsswitch.conf and add ldap authentication to passwd and group lines. <pre><code>vim /etc/nsswitch.conf\n</code></pre> <pre><code>passwd: compat systemd ldap\ngroup: compat systemd ldap\nshadow: compat\n</code></pre></p> <p>Modify the file /etc/pam.d/common-password Remove use_authtok on line 26 to look like below. <pre><code>vim /etc/pam.d/common-password\n</code></pre> <pre><code>password [success=1 user_unknown=ignore default=die] pam_ldap.so try_first_pass\n</code></pre></p> <p>Enable creation of home directory on the first login by adding the following line to the end of file /etc/pam.d/common-session <pre><code>session optional pam_mkhomedir.so skel=/etc/skel umask=077\n</code></pre> See the below screenshot: </p> <p>Test by switching to a user account on LDAP</p> <pre><code>sudo su - &lt;username&gt;\n</code></pre>"},{"location":"miscellaneous/nexus/","title":"Nexus Installation Setup","text":"Instance Details Resources CentOS - t2.medium 4GB Ram , 2 CPU Nexus Script <pre><code>#!/bin/bash\nyum install java-1.8.0-openjdk.x86_64 wget -y   \nmkdir -p /opt/nexus/   \nmkdir -p /tmp/nexus/                           \ncd /tmp/nexus\nNEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\"\nwget $NEXUSURL -O nexus.tar.gz\nEXTOUT=`tar xzvf nexus.tar.gz`\nNEXUSDIR=`echo $EXTOUT | cut -d '/' -f1`\nrm -rf /tmp/nexus/nexus.tar.gz\nrsync -avzh /tmp/nexus/ /opt/nexus/\nuseradd nexus\nchown -R nexus.nexus /opt/nexus \ncat &lt;&lt;EOT&gt;&gt; /etc/systemd/system/nexus.service\n[Unit]                                                                          \nDescription=nexus service                                                       \nAfter=network.target                                                            \n\n[Service]                                                                       \nType=forking                                                                    \nLimitNOFILE=65536                                                               \nExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start                                  \nExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop                                    \nUser=nexus                                                                      \nRestart=on-abort                                                                \n\n[Install]                                                                       \nWantedBy=multi-user.target                                                      \n\nEOT\n\necho 'run_as_user=\"nexus\"' &gt; /opt/nexus/$NEXUSDIR/bin/nexus.rc\nsystemctl daemon-reload\nsystemctl start nexus\nsystemctl enable nexus\n</code></pre>"},{"location":"miscellaneous/sonarqube/","title":"SonarQube Installation Script","text":"Instance Details Resources Ubuntu - t2.medium 4GB Ram , 2 CPU SonarQube Script <pre><code>#!/bin/bash\ncp /etc/sysctl.conf /root/sysctl.conf_backup\ncat &lt;&lt;EOT&gt; /etc/sysctl.conf\nvm.max_map_count=262144\nfs.file-max=65536\nulimit -n 65536\nulimit -u 4096\nEOT\ncp /etc/security/limits.conf /root/sec_limit.conf_backup\ncat &lt;&lt;EOT&gt; /etc/security/limits.conf\nsonarqube   -   nofile   65536\nsonarqube   -   nproc    409\nEOT\n\nsudo apt-get update -y\nsudo apt-get install openjdk-11-jdk -y\nsudo update-alternatives --config java\njava -version\n\nsudo apt update\nwget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -\n\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'\nsudo apt install postgresql postgresql-contrib -y\n#sudo -u postgres psql -c \"SELECT version();\"\nsudo systemctl enable postgresql.service\nsudo systemctl start  postgresql.service\nsudo echo \"postgres:admin123\" | chpasswd\nrunuser -l postgres -c \"createuser sonar\"\nsudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\"\nsudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\"\nsudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\"\nsystemctl restart  postgresql\n#systemctl status -l   postgresql\nnetstat -tulpena | grep postgres\nsudo mkdir -p /sonarqube/\ncd /sonarqube/\nsudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip\nsudo apt-get install zip -y\nsudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/\nsudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube\nsudo groupadd sonar\nsudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar\nsudo chown sonar:sonar /opt/sonarqube/ -R\ncp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup\ncat &lt;&lt;EOT&gt; /opt/sonarqube/conf/sonar.properties\nsonar.jdbc.username=sonar\nsonar.jdbc.password=admin123\nsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube\nsonar.web.host=0.0.0.0\nsonar.web.port=9000\nsonar.web.javaAdditionalOpts=-server\nsonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError\nsonar.log.level=INFO\nsonar.path.logs=logs\nEOT\n\ncat &lt;&lt;EOT&gt; /etc/systemd/system/sonarqube.service\n[Unit]\nDescription=SonarQube service\nAfter=syslog.target network.target\n\n[Service]\nType=forking\n\nExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start\nExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop\n\nUser=sonar\nGroup=sonar\nRestart=always\n\nLimitNOFILE=65536\nLimitNPROC=4096\n\n\n[Install]\nWantedBy=multi-user.target\nEOT\n\nsystemctl daemon-reload\nsystemctl enable sonarqube.service\n#systemctl start sonarqube.service\n#systemctl status -l sonarqube.service\napt-get install nginx -y\nrm -rf /etc/nginx/sites-enabled/default\nrm -rf /etc/nginx/sites-available/default\ncat &lt;&lt;EOT&gt; /etc/nginx/sites-available/sonarqube\nserver{\n    listen      80;\n    server_name sonarqube.groophy.in;\n\n    access_log  /var/log/nginx/sonar.access.log;\n    error_log   /var/log/nginx/sonar.error.log;\n\n    proxy_buffers 16 64k;\n    proxy_buffer_size 128k;\n\n    location / {\n        proxy_pass  http://127.0.0.1:9000;\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\n        proxy_redirect off;\n\n        proxy_set_header    Host            \\$host;\n        proxy_set_header    X-Real-IP       \\$remote_addr;\n        proxy_set_header    X-Forwarded-For \\$proxy_add_x_forwarded_for;\n        proxy_set_header    X-Forwarded-Proto http;\n    }\n}\nEOT\nln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube\nsystemctl enable nginx.service\n#systemctl restart nginx.service\nsudo ufw allow 80,9000,9001/tcp\n\necho \"System reboot in 30 sec\"\nsleep 30\nreboot\n</code></pre> SonarQube-Analysis-Properties <pre><code>sonar.projectKey=vprofile\nsonar.projectName=vprofile-repo\nsonar.projectVersion=1.0\nsonar.sources=src/\nsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/\nsonar.junit.reportsPath=target/surefire-reports/\nsonar.jacoco.reportsPath=target/jacoco.exec\nsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml\n</code></pre>"}]}